{"format": "torch", "nodes": [{"name": "model", "id": 140328437645760, "class_name": "BartModel(\n  (shared): Embedding(50265, 1024, padding_idx=1)\n  (encoder): BartEncoder(\n    (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n    (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)\n    (layers): ModuleList(\n      (0): EncoderLayer(\n        (self_attn): Attention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n      (1): EncoderLayer(\n        (self_attn): Attention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n      (2): EncoderLayer(\n        (self_attn): Attention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n      (3): EncoderLayer(\n        (self_attn): Attention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n      (4): EncoderLayer(\n        (self_attn): Attention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n      (5): EncoderLayer(\n        (self_attn): Attention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n      (6): EncoderLayer(\n        (self_attn): Attention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n      (7): EncoderLayer(\n        (self_attn): Attention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n      (8): EncoderLayer(\n        (self_attn): Attention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n      (9): EncoderLayer(\n        (self_attn): Attention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n      (10): EncoderLayer(\n        (self_attn): Attention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n      (11): EncoderLayer(\n        (self_attn): Attention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n    )\n    (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n  )\n  (decoder): BartDecoder(\n    (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n    (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)\n    (layers): ModuleList(\n      (0): DecoderLayer(\n        (self_attn): Attention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (encoder_attn): Attention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n      (1): DecoderLayer(\n        (self_attn): Attention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (encoder_attn): Attention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n      (2): DecoderLayer(\n        (self_attn): Attention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (encoder_attn): Attention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n      (3): DecoderLayer(\n        (self_attn): Attention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (encoder_attn): Attention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n      (4): DecoderLayer(\n        (self_attn): Attention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (encoder_attn): Attention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n      (5): DecoderLayer(\n        (self_attn): Attention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (encoder_attn): Attention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n      (6): DecoderLayer(\n        (self_attn): Attention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (encoder_attn): Attention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n      (7): DecoderLayer(\n        (self_attn): Attention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (encoder_attn): Attention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n      (8): DecoderLayer(\n        (self_attn): Attention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (encoder_attn): Attention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n      (9): DecoderLayer(\n        (self_attn): Attention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (encoder_attn): Attention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n      (10): DecoderLayer(\n        (self_attn): Attention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (encoder_attn): Attention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n      (11): DecoderLayer(\n        (self_attn): Attention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (encoder_attn): Attention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n    )\n    (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n  )\n)", "parameters": [["shared.weight", [50265, 1024]], ["encoder.embed_positions.weight", [1026, 1024]], ["encoder.layers.0.self_attn.k_proj.weight", [1024, 1024]], ["encoder.layers.0.self_attn.k_proj.bias", [1024]], ["encoder.layers.0.self_attn.v_proj.weight", [1024, 1024]], ["encoder.layers.0.self_attn.v_proj.bias", [1024]], ["encoder.layers.0.self_attn.q_proj.weight", [1024, 1024]], ["encoder.layers.0.self_attn.q_proj.bias", [1024]], ["encoder.layers.0.self_attn.out_proj.weight", [1024, 1024]], ["encoder.layers.0.self_attn.out_proj.bias", [1024]], ["encoder.layers.0.self_attn_layer_norm.weight", [1024]], ["encoder.layers.0.self_attn_layer_norm.bias", [1024]], ["encoder.layers.0.fc1.weight", [4096, 1024]], ["encoder.layers.0.fc1.bias", [4096]], ["encoder.layers.0.fc2.weight", [1024, 4096]], ["encoder.layers.0.fc2.bias", [1024]], ["encoder.layers.0.final_layer_norm.weight", [1024]], ["encoder.layers.0.final_layer_norm.bias", [1024]], ["encoder.layers.1.self_attn.k_proj.weight", [1024, 1024]], ["encoder.layers.1.self_attn.k_proj.bias", [1024]], ["encoder.layers.1.self_attn.v_proj.weight", [1024, 1024]], ["encoder.layers.1.self_attn.v_proj.bias", [1024]], ["encoder.layers.1.self_attn.q_proj.weight", [1024, 1024]], ["encoder.layers.1.self_attn.q_proj.bias", [1024]], ["encoder.layers.1.self_attn.out_proj.weight", [1024, 1024]], ["encoder.layers.1.self_attn.out_proj.bias", [1024]], ["encoder.layers.1.self_attn_layer_norm.weight", [1024]], ["encoder.layers.1.self_attn_layer_norm.bias", [1024]], ["encoder.layers.1.fc1.weight", [4096, 1024]], ["encoder.layers.1.fc1.bias", [4096]], ["encoder.layers.1.fc2.weight", [1024, 4096]], ["encoder.layers.1.fc2.bias", [1024]], ["encoder.layers.1.final_layer_norm.weight", [1024]], ["encoder.layers.1.final_layer_norm.bias", [1024]], ["encoder.layers.2.self_attn.k_proj.weight", [1024, 1024]], ["encoder.layers.2.self_attn.k_proj.bias", [1024]], ["encoder.layers.2.self_attn.v_proj.weight", [1024, 1024]], ["encoder.layers.2.self_attn.v_proj.bias", [1024]], ["encoder.layers.2.self_attn.q_proj.weight", [1024, 1024]], ["encoder.layers.2.self_attn.q_proj.bias", [1024]], ["encoder.layers.2.self_attn.out_proj.weight", [1024, 1024]], ["encoder.layers.2.self_attn.out_proj.bias", [1024]], ["encoder.layers.2.self_attn_layer_norm.weight", [1024]], ["encoder.layers.2.self_attn_layer_norm.bias", [1024]], ["encoder.layers.2.fc1.weight", [4096, 1024]], ["encoder.layers.2.fc1.bias", [4096]], ["encoder.layers.2.fc2.weight", [1024, 4096]], ["encoder.layers.2.fc2.bias", [1024]], ["encoder.layers.2.final_layer_norm.weight", [1024]], ["encoder.layers.2.final_layer_norm.bias", [1024]], ["encoder.layers.3.self_attn.k_proj.weight", [1024, 1024]], ["encoder.layers.3.self_attn.k_proj.bias", [1024]], ["encoder.layers.3.self_attn.v_proj.weight", [1024, 1024]], ["encoder.layers.3.self_attn.v_proj.bias", [1024]], ["encoder.layers.3.self_attn.q_proj.weight", [1024, 1024]], ["encoder.layers.3.self_attn.q_proj.bias", [1024]], ["encoder.layers.3.self_attn.out_proj.weight", [1024, 1024]], ["encoder.layers.3.self_attn.out_proj.bias", [1024]], ["encoder.layers.3.self_attn_layer_norm.weight", [1024]], ["encoder.layers.3.self_attn_layer_norm.bias", [1024]], ["encoder.layers.3.fc1.weight", [4096, 1024]], ["encoder.layers.3.fc1.bias", [4096]], ["encoder.layers.3.fc2.weight", [1024, 4096]], ["encoder.layers.3.fc2.bias", [1024]], ["encoder.layers.3.final_layer_norm.weight", [1024]], ["encoder.layers.3.final_layer_norm.bias", [1024]], ["encoder.layers.4.self_attn.k_proj.weight", [1024, 1024]], ["encoder.layers.4.self_attn.k_proj.bias", [1024]], ["encoder.layers.4.self_attn.v_proj.weight", [1024, 1024]], ["encoder.layers.4.self_attn.v_proj.bias", [1024]], ["encoder.layers.4.self_attn.q_proj.weight", [1024, 1024]], ["encoder.layers.4.self_attn.q_proj.bias", [1024]], ["encoder.layers.4.self_attn.out_proj.weight", [1024, 1024]], ["encoder.layers.4.self_attn.out_proj.bias", [1024]], ["encoder.layers.4.self_attn_layer_norm.weight", [1024]], ["encoder.layers.4.self_attn_layer_norm.bias", [1024]], ["encoder.layers.4.fc1.weight", [4096, 1024]], ["encoder.layers.4.fc1.bias", [4096]], ["encoder.layers.4.fc2.weight", [1024, 4096]], ["encoder.layers.4.fc2.bias", [1024]], ["encoder.layers.4.final_layer_norm.weight", [1024]], ["encoder.layers.4.final_layer_norm.bias", [1024]], ["encoder.layers.5.self_attn.k_proj.weight", [1024, 1024]], ["encoder.layers.5.self_attn.k_proj.bias", [1024]], ["encoder.layers.5.self_attn.v_proj.weight", [1024, 1024]], ["encoder.layers.5.self_attn.v_proj.bias", [1024]], ["encoder.layers.5.self_attn.q_proj.weight", [1024, 1024]], ["encoder.layers.5.self_attn.q_proj.bias", [1024]], ["encoder.layers.5.self_attn.out_proj.weight", [1024, 1024]], ["encoder.layers.5.self_attn.out_proj.bias", [1024]], ["encoder.layers.5.self_attn_layer_norm.weight", [1024]], ["encoder.layers.5.self_attn_layer_norm.bias", [1024]], ["encoder.layers.5.fc1.weight", [4096, 1024]], ["encoder.layers.5.fc1.bias", [4096]], ["encoder.layers.5.fc2.weight", [1024, 4096]], ["encoder.layers.5.fc2.bias", [1024]], ["encoder.layers.5.final_layer_norm.weight", [1024]], ["encoder.layers.5.final_layer_norm.bias", [1024]], ["encoder.layers.6.self_attn.k_proj.weight", [1024, 1024]], ["encoder.layers.6.self_attn.k_proj.bias", [1024]], ["encoder.layers.6.self_attn.v_proj.weight", [1024, 1024]], ["encoder.layers.6.self_attn.v_proj.bias", [1024]], ["encoder.layers.6.self_attn.q_proj.weight", [1024, 1024]], ["encoder.layers.6.self_attn.q_proj.bias", [1024]], ["encoder.layers.6.self_attn.out_proj.weight", [1024, 1024]], ["encoder.layers.6.self_attn.out_proj.bias", [1024]], ["encoder.layers.6.self_attn_layer_norm.weight", [1024]], ["encoder.layers.6.self_attn_layer_norm.bias", [1024]], ["encoder.layers.6.fc1.weight", [4096, 1024]], ["encoder.layers.6.fc1.bias", [4096]], ["encoder.layers.6.fc2.weight", [1024, 4096]], ["encoder.layers.6.fc2.bias", [1024]], ["encoder.layers.6.final_layer_norm.weight", [1024]], ["encoder.layers.6.final_layer_norm.bias", [1024]], ["encoder.layers.7.self_attn.k_proj.weight", [1024, 1024]], ["encoder.layers.7.self_attn.k_proj.bias", [1024]], ["encoder.layers.7.self_attn.v_proj.weight", [1024, 1024]], ["encoder.layers.7.self_attn.v_proj.bias", [1024]], ["encoder.layers.7.self_attn.q_proj.weight", [1024, 1024]], ["encoder.layers.7.self_attn.q_proj.bias", [1024]], ["encoder.layers.7.self_attn.out_proj.weight", [1024, 1024]], ["encoder.layers.7.self_attn.out_proj.bias", [1024]], ["encoder.layers.7.self_attn_layer_norm.weight", [1024]], ["encoder.layers.7.self_attn_layer_norm.bias", [1024]], ["encoder.layers.7.fc1.weight", [4096, 1024]], ["encoder.layers.7.fc1.bias", [4096]], ["encoder.layers.7.fc2.weight", [1024, 4096]], ["encoder.layers.7.fc2.bias", [1024]], ["encoder.layers.7.final_layer_norm.weight", [1024]], ["encoder.layers.7.final_layer_norm.bias", [1024]], ["encoder.layers.8.self_attn.k_proj.weight", [1024, 1024]], ["encoder.layers.8.self_attn.k_proj.bias", [1024]], ["encoder.layers.8.self_attn.v_proj.weight", [1024, 1024]], ["encoder.layers.8.self_attn.v_proj.bias", [1024]], ["encoder.layers.8.self_attn.q_proj.weight", [1024, 1024]], ["encoder.layers.8.self_attn.q_proj.bias", [1024]], ["encoder.layers.8.self_attn.out_proj.weight", [1024, 1024]], ["encoder.layers.8.self_attn.out_proj.bias", [1024]], ["encoder.layers.8.self_attn_layer_norm.weight", [1024]], ["encoder.layers.8.self_attn_layer_norm.bias", [1024]], ["encoder.layers.8.fc1.weight", [4096, 1024]], ["encoder.layers.8.fc1.bias", [4096]], ["encoder.layers.8.fc2.weight", [1024, 4096]], ["encoder.layers.8.fc2.bias", [1024]], ["encoder.layers.8.final_layer_norm.weight", [1024]], ["encoder.layers.8.final_layer_norm.bias", [1024]], ["encoder.layers.9.self_attn.k_proj.weight", [1024, 1024]], ["encoder.layers.9.self_attn.k_proj.bias", [1024]], ["encoder.layers.9.self_attn.v_proj.weight", [1024, 1024]], ["encoder.layers.9.self_attn.v_proj.bias", [1024]], ["encoder.layers.9.self_attn.q_proj.weight", [1024, 1024]], ["encoder.layers.9.self_attn.q_proj.bias", [1024]], ["encoder.layers.9.self_attn.out_proj.weight", [1024, 1024]], ["encoder.layers.9.self_attn.out_proj.bias", [1024]], ["encoder.layers.9.self_attn_layer_norm.weight", [1024]], ["encoder.layers.9.self_attn_layer_norm.bias", [1024]], ["encoder.layers.9.fc1.weight", [4096, 1024]], ["encoder.layers.9.fc1.bias", [4096]], ["encoder.layers.9.fc2.weight", [1024, 4096]], ["encoder.layers.9.fc2.bias", [1024]], ["encoder.layers.9.final_layer_norm.weight", [1024]], ["encoder.layers.9.final_layer_norm.bias", [1024]], ["encoder.layers.10.self_attn.k_proj.weight", [1024, 1024]], ["encoder.layers.10.self_attn.k_proj.bias", [1024]], ["encoder.layers.10.self_attn.v_proj.weight", [1024, 1024]], ["encoder.layers.10.self_attn.v_proj.bias", [1024]], ["encoder.layers.10.self_attn.q_proj.weight", [1024, 1024]], ["encoder.layers.10.self_attn.q_proj.bias", [1024]], ["encoder.layers.10.self_attn.out_proj.weight", [1024, 1024]], ["encoder.layers.10.self_attn.out_proj.bias", [1024]], ["encoder.layers.10.self_attn_layer_norm.weight", [1024]], ["encoder.layers.10.self_attn_layer_norm.bias", [1024]], ["encoder.layers.10.fc1.weight", [4096, 1024]], ["encoder.layers.10.fc1.bias", [4096]], ["encoder.layers.10.fc2.weight", [1024, 4096]], ["encoder.layers.10.fc2.bias", [1024]], ["encoder.layers.10.final_layer_norm.weight", [1024]], ["encoder.layers.10.final_layer_norm.bias", [1024]], ["encoder.layers.11.self_attn.k_proj.weight", [1024, 1024]], ["encoder.layers.11.self_attn.k_proj.bias", [1024]], ["encoder.layers.11.self_attn.v_proj.weight", [1024, 1024]], ["encoder.layers.11.self_attn.v_proj.bias", [1024]], ["encoder.layers.11.self_attn.q_proj.weight", [1024, 1024]], ["encoder.layers.11.self_attn.q_proj.bias", [1024]], ["encoder.layers.11.self_attn.out_proj.weight", [1024, 1024]], ["encoder.layers.11.self_attn.out_proj.bias", [1024]], ["encoder.layers.11.self_attn_layer_norm.weight", [1024]], ["encoder.layers.11.self_attn_layer_norm.bias", [1024]], ["encoder.layers.11.fc1.weight", [4096, 1024]], ["encoder.layers.11.fc1.bias", [4096]], ["encoder.layers.11.fc2.weight", [1024, 4096]], ["encoder.layers.11.fc2.bias", [1024]], ["encoder.layers.11.final_layer_norm.weight", [1024]], ["encoder.layers.11.final_layer_norm.bias", [1024]], ["encoder.layernorm_embedding.weight", [1024]], ["encoder.layernorm_embedding.bias", [1024]], ["decoder.embed_positions.weight", [1026, 1024]], ["decoder.layers.0.self_attn.k_proj.weight", [1024, 1024]], ["decoder.layers.0.self_attn.k_proj.bias", [1024]], ["decoder.layers.0.self_attn.v_proj.weight", [1024, 1024]], ["decoder.layers.0.self_attn.v_proj.bias", [1024]], ["decoder.layers.0.self_attn.q_proj.weight", [1024, 1024]], ["decoder.layers.0.self_attn.q_proj.bias", [1024]], ["decoder.layers.0.self_attn.out_proj.weight", [1024, 1024]], ["decoder.layers.0.self_attn.out_proj.bias", [1024]], ["decoder.layers.0.self_attn_layer_norm.weight", [1024]], ["decoder.layers.0.self_attn_layer_norm.bias", [1024]], ["decoder.layers.0.encoder_attn.k_proj.weight", [1024, 1024]], ["decoder.layers.0.encoder_attn.k_proj.bias", [1024]], ["decoder.layers.0.encoder_attn.v_proj.weight", [1024, 1024]], ["decoder.layers.0.encoder_attn.v_proj.bias", [1024]], ["decoder.layers.0.encoder_attn.q_proj.weight", [1024, 1024]], ["decoder.layers.0.encoder_attn.q_proj.bias", [1024]], ["decoder.layers.0.encoder_attn.out_proj.weight", [1024, 1024]], ["decoder.layers.0.encoder_attn.out_proj.bias", [1024]], ["decoder.layers.0.encoder_attn_layer_norm.weight", [1024]], ["decoder.layers.0.encoder_attn_layer_norm.bias", [1024]], ["decoder.layers.0.fc1.weight", [4096, 1024]], ["decoder.layers.0.fc1.bias", [4096]], ["decoder.layers.0.fc2.weight", [1024, 4096]], ["decoder.layers.0.fc2.bias", [1024]], ["decoder.layers.0.final_layer_norm.weight", [1024]], ["decoder.layers.0.final_layer_norm.bias", [1024]], ["decoder.layers.1.self_attn.k_proj.weight", [1024, 1024]], ["decoder.layers.1.self_attn.k_proj.bias", [1024]], ["decoder.layers.1.self_attn.v_proj.weight", [1024, 1024]], ["decoder.layers.1.self_attn.v_proj.bias", [1024]], ["decoder.layers.1.self_attn.q_proj.weight", [1024, 1024]], ["decoder.layers.1.self_attn.q_proj.bias", [1024]], ["decoder.layers.1.self_attn.out_proj.weight", [1024, 1024]], ["decoder.layers.1.self_attn.out_proj.bias", [1024]], ["decoder.layers.1.self_attn_layer_norm.weight", [1024]], ["decoder.layers.1.self_attn_layer_norm.bias", [1024]], ["decoder.layers.1.encoder_attn.k_proj.weight", [1024, 1024]], ["decoder.layers.1.encoder_attn.k_proj.bias", [1024]], ["decoder.layers.1.encoder_attn.v_proj.weight", [1024, 1024]], ["decoder.layers.1.encoder_attn.v_proj.bias", [1024]], ["decoder.layers.1.encoder_attn.q_proj.weight", [1024, 1024]], ["decoder.layers.1.encoder_attn.q_proj.bias", [1024]], ["decoder.layers.1.encoder_attn.out_proj.weight", [1024, 1024]], ["decoder.layers.1.encoder_attn.out_proj.bias", [1024]], ["decoder.layers.1.encoder_attn_layer_norm.weight", [1024]], ["decoder.layers.1.encoder_attn_layer_norm.bias", [1024]], ["decoder.layers.1.fc1.weight", [4096, 1024]], ["decoder.layers.1.fc1.bias", [4096]], ["decoder.layers.1.fc2.weight", [1024, 4096]], ["decoder.layers.1.fc2.bias", [1024]], ["decoder.layers.1.final_layer_norm.weight", [1024]], ["decoder.layers.1.final_layer_norm.bias", [1024]], ["decoder.layers.2.self_attn.k_proj.weight", [1024, 1024]], ["decoder.layers.2.self_attn.k_proj.bias", [1024]], ["decoder.layers.2.self_attn.v_proj.weight", [1024, 1024]], ["decoder.layers.2.self_attn.v_proj.bias", [1024]], ["decoder.layers.2.self_attn.q_proj.weight", [1024, 1024]], ["decoder.layers.2.self_attn.q_proj.bias", [1024]], ["decoder.layers.2.self_attn.out_proj.weight", [1024, 1024]], ["decoder.layers.2.self_attn.out_proj.bias", [1024]], ["decoder.layers.2.self_attn_layer_norm.weight", [1024]], ["decoder.layers.2.self_attn_layer_norm.bias", [1024]], ["decoder.layers.2.encoder_attn.k_proj.weight", [1024, 1024]], ["decoder.layers.2.encoder_attn.k_proj.bias", [1024]], ["decoder.layers.2.encoder_attn.v_proj.weight", [1024, 1024]], ["decoder.layers.2.encoder_attn.v_proj.bias", [1024]], ["decoder.layers.2.encoder_attn.q_proj.weight", [1024, 1024]], ["decoder.layers.2.encoder_attn.q_proj.bias", [1024]], ["decoder.layers.2.encoder_attn.out_proj.weight", [1024, 1024]], ["decoder.layers.2.encoder_attn.out_proj.bias", [1024]], ["decoder.layers.2.encoder_attn_layer_norm.weight", [1024]], ["decoder.layers.2.encoder_attn_layer_norm.bias", [1024]], ["decoder.layers.2.fc1.weight", [4096, 1024]], ["decoder.layers.2.fc1.bias", [4096]], ["decoder.layers.2.fc2.weight", [1024, 4096]], ["decoder.layers.2.fc2.bias", [1024]], ["decoder.layers.2.final_layer_norm.weight", [1024]], ["decoder.layers.2.final_layer_norm.bias", [1024]], ["decoder.layers.3.self_attn.k_proj.weight", [1024, 1024]], ["decoder.layers.3.self_attn.k_proj.bias", [1024]], ["decoder.layers.3.self_attn.v_proj.weight", [1024, 1024]], ["decoder.layers.3.self_attn.v_proj.bias", [1024]], ["decoder.layers.3.self_attn.q_proj.weight", [1024, 1024]], ["decoder.layers.3.self_attn.q_proj.bias", [1024]], ["decoder.layers.3.self_attn.out_proj.weight", [1024, 1024]], ["decoder.layers.3.self_attn.out_proj.bias", [1024]], ["decoder.layers.3.self_attn_layer_norm.weight", [1024]], ["decoder.layers.3.self_attn_layer_norm.bias", [1024]], ["decoder.layers.3.encoder_attn.k_proj.weight", [1024, 1024]], ["decoder.layers.3.encoder_attn.k_proj.bias", [1024]], ["decoder.layers.3.encoder_attn.v_proj.weight", [1024, 1024]], ["decoder.layers.3.encoder_attn.v_proj.bias", [1024]], ["decoder.layers.3.encoder_attn.q_proj.weight", [1024, 1024]], ["decoder.layers.3.encoder_attn.q_proj.bias", [1024]], ["decoder.layers.3.encoder_attn.out_proj.weight", [1024, 1024]], ["decoder.layers.3.encoder_attn.out_proj.bias", [1024]], ["decoder.layers.3.encoder_attn_layer_norm.weight", [1024]], ["decoder.layers.3.encoder_attn_layer_norm.bias", [1024]], ["decoder.layers.3.fc1.weight", [4096, 1024]], ["decoder.layers.3.fc1.bias", [4096]], ["decoder.layers.3.fc2.weight", [1024, 4096]], ["decoder.layers.3.fc2.bias", [1024]], ["decoder.layers.3.final_layer_norm.weight", [1024]], ["decoder.layers.3.final_layer_norm.bias", [1024]], ["decoder.layers.4.self_attn.k_proj.weight", [1024, 1024]], ["decoder.layers.4.self_attn.k_proj.bias", [1024]], ["decoder.layers.4.self_attn.v_proj.weight", [1024, 1024]], ["decoder.layers.4.self_attn.v_proj.bias", [1024]], ["decoder.layers.4.self_attn.q_proj.weight", [1024, 1024]], ["decoder.layers.4.self_attn.q_proj.bias", [1024]], ["decoder.layers.4.self_attn.out_proj.weight", [1024, 1024]], ["decoder.layers.4.self_attn.out_proj.bias", [1024]], ["decoder.layers.4.self_attn_layer_norm.weight", [1024]], ["decoder.layers.4.self_attn_layer_norm.bias", [1024]], ["decoder.layers.4.encoder_attn.k_proj.weight", [1024, 1024]], ["decoder.layers.4.encoder_attn.k_proj.bias", [1024]], ["decoder.layers.4.encoder_attn.v_proj.weight", [1024, 1024]], ["decoder.layers.4.encoder_attn.v_proj.bias", [1024]], ["decoder.layers.4.encoder_attn.q_proj.weight", [1024, 1024]], ["decoder.layers.4.encoder_attn.q_proj.bias", [1024]], ["decoder.layers.4.encoder_attn.out_proj.weight", [1024, 1024]], ["decoder.layers.4.encoder_attn.out_proj.bias", [1024]], ["decoder.layers.4.encoder_attn_layer_norm.weight", [1024]], ["decoder.layers.4.encoder_attn_layer_norm.bias", [1024]], ["decoder.layers.4.fc1.weight", [4096, 1024]], ["decoder.layers.4.fc1.bias", [4096]], ["decoder.layers.4.fc2.weight", [1024, 4096]], ["decoder.layers.4.fc2.bias", [1024]], ["decoder.layers.4.final_layer_norm.weight", [1024]], ["decoder.layers.4.final_layer_norm.bias", [1024]], ["decoder.layers.5.self_attn.k_proj.weight", [1024, 1024]], ["decoder.layers.5.self_attn.k_proj.bias", [1024]], ["decoder.layers.5.self_attn.v_proj.weight", [1024, 1024]], ["decoder.layers.5.self_attn.v_proj.bias", [1024]], ["decoder.layers.5.self_attn.q_proj.weight", [1024, 1024]], ["decoder.layers.5.self_attn.q_proj.bias", [1024]], ["decoder.layers.5.self_attn.out_proj.weight", [1024, 1024]], ["decoder.layers.5.self_attn.out_proj.bias", [1024]], ["decoder.layers.5.self_attn_layer_norm.weight", [1024]], ["decoder.layers.5.self_attn_layer_norm.bias", [1024]], ["decoder.layers.5.encoder_attn.k_proj.weight", [1024, 1024]], ["decoder.layers.5.encoder_attn.k_proj.bias", [1024]], ["decoder.layers.5.encoder_attn.v_proj.weight", [1024, 1024]], ["decoder.layers.5.encoder_attn.v_proj.bias", [1024]], ["decoder.layers.5.encoder_attn.q_proj.weight", [1024, 1024]], ["decoder.layers.5.encoder_attn.q_proj.bias", [1024]], ["decoder.layers.5.encoder_attn.out_proj.weight", [1024, 1024]], ["decoder.layers.5.encoder_attn.out_proj.bias", [1024]], ["decoder.layers.5.encoder_attn_layer_norm.weight", [1024]], ["decoder.layers.5.encoder_attn_layer_norm.bias", [1024]], ["decoder.layers.5.fc1.weight", [4096, 1024]], ["decoder.layers.5.fc1.bias", [4096]], ["decoder.layers.5.fc2.weight", [1024, 4096]], ["decoder.layers.5.fc2.bias", [1024]], ["decoder.layers.5.final_layer_norm.weight", [1024]], ["decoder.layers.5.final_layer_norm.bias", [1024]], ["decoder.layers.6.self_attn.k_proj.weight", [1024, 1024]], ["decoder.layers.6.self_attn.k_proj.bias", [1024]], ["decoder.layers.6.self_attn.v_proj.weight", [1024, 1024]], ["decoder.layers.6.self_attn.v_proj.bias", [1024]], ["decoder.layers.6.self_attn.q_proj.weight", [1024, 1024]], ["decoder.layers.6.self_attn.q_proj.bias", [1024]], ["decoder.layers.6.self_attn.out_proj.weight", [1024, 1024]], ["decoder.layers.6.self_attn.out_proj.bias", [1024]], ["decoder.layers.6.self_attn_layer_norm.weight", [1024]], ["decoder.layers.6.self_attn_layer_norm.bias", [1024]], ["decoder.layers.6.encoder_attn.k_proj.weight", [1024, 1024]], ["decoder.layers.6.encoder_attn.k_proj.bias", [1024]], ["decoder.layers.6.encoder_attn.v_proj.weight", [1024, 1024]], ["decoder.layers.6.encoder_attn.v_proj.bias", [1024]], ["decoder.layers.6.encoder_attn.q_proj.weight", [1024, 1024]], ["decoder.layers.6.encoder_attn.q_proj.bias", [1024]], ["decoder.layers.6.encoder_attn.out_proj.weight", [1024, 1024]], ["decoder.layers.6.encoder_attn.out_proj.bias", [1024]], ["decoder.layers.6.encoder_attn_layer_norm.weight", [1024]], ["decoder.layers.6.encoder_attn_layer_norm.bias", [1024]], ["decoder.layers.6.fc1.weight", [4096, 1024]], ["decoder.layers.6.fc1.bias", [4096]], ["decoder.layers.6.fc2.weight", [1024, 4096]], ["decoder.layers.6.fc2.bias", [1024]], ["decoder.layers.6.final_layer_norm.weight", [1024]], ["decoder.layers.6.final_layer_norm.bias", [1024]], ["decoder.layers.7.self_attn.k_proj.weight", [1024, 1024]], ["decoder.layers.7.self_attn.k_proj.bias", [1024]], ["decoder.layers.7.self_attn.v_proj.weight", [1024, 1024]], ["decoder.layers.7.self_attn.v_proj.bias", [1024]], ["decoder.layers.7.self_attn.q_proj.weight", [1024, 1024]], ["decoder.layers.7.self_attn.q_proj.bias", [1024]], ["decoder.layers.7.self_attn.out_proj.weight", [1024, 1024]], ["decoder.layers.7.self_attn.out_proj.bias", [1024]], ["decoder.layers.7.self_attn_layer_norm.weight", [1024]], ["decoder.layers.7.self_attn_layer_norm.bias", [1024]], ["decoder.layers.7.encoder_attn.k_proj.weight", [1024, 1024]], ["decoder.layers.7.encoder_attn.k_proj.bias", [1024]], ["decoder.layers.7.encoder_attn.v_proj.weight", [1024, 1024]], ["decoder.layers.7.encoder_attn.v_proj.bias", [1024]], ["decoder.layers.7.encoder_attn.q_proj.weight", [1024, 1024]], ["decoder.layers.7.encoder_attn.q_proj.bias", [1024]], ["decoder.layers.7.encoder_attn.out_proj.weight", [1024, 1024]], ["decoder.layers.7.encoder_attn.out_proj.bias", [1024]], ["decoder.layers.7.encoder_attn_layer_norm.weight", [1024]], ["decoder.layers.7.encoder_attn_layer_norm.bias", [1024]], ["decoder.layers.7.fc1.weight", [4096, 1024]], ["decoder.layers.7.fc1.bias", [4096]], ["decoder.layers.7.fc2.weight", [1024, 4096]], ["decoder.layers.7.fc2.bias", [1024]], ["decoder.layers.7.final_layer_norm.weight", [1024]], ["decoder.layers.7.final_layer_norm.bias", [1024]], ["decoder.layers.8.self_attn.k_proj.weight", [1024, 1024]], ["decoder.layers.8.self_attn.k_proj.bias", [1024]], ["decoder.layers.8.self_attn.v_proj.weight", [1024, 1024]], ["decoder.layers.8.self_attn.v_proj.bias", [1024]], ["decoder.layers.8.self_attn.q_proj.weight", [1024, 1024]], ["decoder.layers.8.self_attn.q_proj.bias", [1024]], ["decoder.layers.8.self_attn.out_proj.weight", [1024, 1024]], ["decoder.layers.8.self_attn.out_proj.bias", [1024]], ["decoder.layers.8.self_attn_layer_norm.weight", [1024]], ["decoder.layers.8.self_attn_layer_norm.bias", [1024]], ["decoder.layers.8.encoder_attn.k_proj.weight", [1024, 1024]], ["decoder.layers.8.encoder_attn.k_proj.bias", [1024]], ["decoder.layers.8.encoder_attn.v_proj.weight", [1024, 1024]], ["decoder.layers.8.encoder_attn.v_proj.bias", [1024]], ["decoder.layers.8.encoder_attn.q_proj.weight", [1024, 1024]], ["decoder.layers.8.encoder_attn.q_proj.bias", [1024]], ["decoder.layers.8.encoder_attn.out_proj.weight", [1024, 1024]], ["decoder.layers.8.encoder_attn.out_proj.bias", [1024]], ["decoder.layers.8.encoder_attn_layer_norm.weight", [1024]], ["decoder.layers.8.encoder_attn_layer_norm.bias", [1024]], ["decoder.layers.8.fc1.weight", [4096, 1024]], ["decoder.layers.8.fc1.bias", [4096]], ["decoder.layers.8.fc2.weight", [1024, 4096]], ["decoder.layers.8.fc2.bias", [1024]], ["decoder.layers.8.final_layer_norm.weight", [1024]], ["decoder.layers.8.final_layer_norm.bias", [1024]], ["decoder.layers.9.self_attn.k_proj.weight", [1024, 1024]], ["decoder.layers.9.self_attn.k_proj.bias", [1024]], ["decoder.layers.9.self_attn.v_proj.weight", [1024, 1024]], ["decoder.layers.9.self_attn.v_proj.bias", [1024]], ["decoder.layers.9.self_attn.q_proj.weight", [1024, 1024]], ["decoder.layers.9.self_attn.q_proj.bias", [1024]], ["decoder.layers.9.self_attn.out_proj.weight", [1024, 1024]], ["decoder.layers.9.self_attn.out_proj.bias", [1024]], ["decoder.layers.9.self_attn_layer_norm.weight", [1024]], ["decoder.layers.9.self_attn_layer_norm.bias", [1024]], ["decoder.layers.9.encoder_attn.k_proj.weight", [1024, 1024]], ["decoder.layers.9.encoder_attn.k_proj.bias", [1024]], ["decoder.layers.9.encoder_attn.v_proj.weight", [1024, 1024]], ["decoder.layers.9.encoder_attn.v_proj.bias", [1024]], ["decoder.layers.9.encoder_attn.q_proj.weight", [1024, 1024]], ["decoder.layers.9.encoder_attn.q_proj.bias", [1024]], ["decoder.layers.9.encoder_attn.out_proj.weight", [1024, 1024]], ["decoder.layers.9.encoder_attn.out_proj.bias", [1024]], ["decoder.layers.9.encoder_attn_layer_norm.weight", [1024]], ["decoder.layers.9.encoder_attn_layer_norm.bias", [1024]], ["decoder.layers.9.fc1.weight", [4096, 1024]], ["decoder.layers.9.fc1.bias", [4096]], ["decoder.layers.9.fc2.weight", [1024, 4096]], ["decoder.layers.9.fc2.bias", [1024]], ["decoder.layers.9.final_layer_norm.weight", [1024]], ["decoder.layers.9.final_layer_norm.bias", [1024]], ["decoder.layers.10.self_attn.k_proj.weight", [1024, 1024]], ["decoder.layers.10.self_attn.k_proj.bias", [1024]], ["decoder.layers.10.self_attn.v_proj.weight", [1024, 1024]], ["decoder.layers.10.self_attn.v_proj.bias", [1024]], ["decoder.layers.10.self_attn.q_proj.weight", [1024, 1024]], ["decoder.layers.10.self_attn.q_proj.bias", [1024]], ["decoder.layers.10.self_attn.out_proj.weight", [1024, 1024]], ["decoder.layers.10.self_attn.out_proj.bias", [1024]], ["decoder.layers.10.self_attn_layer_norm.weight", [1024]], ["decoder.layers.10.self_attn_layer_norm.bias", [1024]], ["decoder.layers.10.encoder_attn.k_proj.weight", [1024, 1024]], ["decoder.layers.10.encoder_attn.k_proj.bias", [1024]], ["decoder.layers.10.encoder_attn.v_proj.weight", [1024, 1024]], ["decoder.layers.10.encoder_attn.v_proj.bias", [1024]], ["decoder.layers.10.encoder_attn.q_proj.weight", [1024, 1024]], ["decoder.layers.10.encoder_attn.q_proj.bias", [1024]], ["decoder.layers.10.encoder_attn.out_proj.weight", [1024, 1024]], ["decoder.layers.10.encoder_attn.out_proj.bias", [1024]], ["decoder.layers.10.encoder_attn_layer_norm.weight", [1024]], ["decoder.layers.10.encoder_attn_layer_norm.bias", [1024]], ["decoder.layers.10.fc1.weight", [4096, 1024]], ["decoder.layers.10.fc1.bias", [4096]], ["decoder.layers.10.fc2.weight", [1024, 4096]], ["decoder.layers.10.fc2.bias", [1024]], ["decoder.layers.10.final_layer_norm.weight", [1024]], ["decoder.layers.10.final_layer_norm.bias", [1024]], ["decoder.layers.11.self_attn.k_proj.weight", [1024, 1024]], ["decoder.layers.11.self_attn.k_proj.bias", [1024]], ["decoder.layers.11.self_attn.v_proj.weight", [1024, 1024]], ["decoder.layers.11.self_attn.v_proj.bias", [1024]], ["decoder.layers.11.self_attn.q_proj.weight", [1024, 1024]], ["decoder.layers.11.self_attn.q_proj.bias", [1024]], ["decoder.layers.11.self_attn.out_proj.weight", [1024, 1024]], ["decoder.layers.11.self_attn.out_proj.bias", [1024]], ["decoder.layers.11.self_attn_layer_norm.weight", [1024]], ["decoder.layers.11.self_attn_layer_norm.bias", [1024]], ["decoder.layers.11.encoder_attn.k_proj.weight", [1024, 1024]], ["decoder.layers.11.encoder_attn.k_proj.bias", [1024]], ["decoder.layers.11.encoder_attn.v_proj.weight", [1024, 1024]], ["decoder.layers.11.encoder_attn.v_proj.bias", [1024]], ["decoder.layers.11.encoder_attn.q_proj.weight", [1024, 1024]], ["decoder.layers.11.encoder_attn.q_proj.bias", [1024]], ["decoder.layers.11.encoder_attn.out_proj.weight", [1024, 1024]], ["decoder.layers.11.encoder_attn.out_proj.bias", [1024]], ["decoder.layers.11.encoder_attn_layer_norm.weight", [1024]], ["decoder.layers.11.encoder_attn_layer_norm.bias", [1024]], ["decoder.layers.11.fc1.weight", [4096, 1024]], ["decoder.layers.11.fc1.bias", [4096]], ["decoder.layers.11.fc2.weight", [1024, 4096]], ["decoder.layers.11.fc2.bias", [1024]], ["decoder.layers.11.final_layer_norm.weight", [1024]], ["decoder.layers.11.final_layer_norm.bias", [1024]], ["decoder.layernorm_embedding.weight", [1024]], ["decoder.layernorm_embedding.bias", [1024]]], "output_shape": [[8, 127, 1024], [8, 128, 1024]], "num_parameters": [51471360, 1050624, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 4194304, 4096, 4194304, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 4194304, 4096, 4194304, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 4194304, 4096, 4194304, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 4194304, 4096, 4194304, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 4194304, 4096, 4194304, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 4194304, 4096, 4194304, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 4194304, 4096, 4194304, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 4194304, 4096, 4194304, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 4194304, 4096, 4194304, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 4194304, 4096, 4194304, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 4194304, 4096, 4194304, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 4194304, 4096, 4194304, 1024, 1024, 1024, 1024, 1024, 1050624, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 4194304, 4096, 4194304, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 4194304, 4096, 4194304, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 4194304, 4096, 4194304, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 4194304, 4096, 4194304, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 4194304, 4096, 4194304, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 4194304, 4096, 4194304, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 4194304, 4096, 4194304, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 4194304, 4096, 4194304, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 4194304, 4096, 4194304, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 4194304, 4096, 4194304, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 4194304, 4096, 4194304, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 4194304, 4096, 4194304, 1024, 1024, 1024, 1024, 1024]}], "edges": []}