{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled19.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "cf6e593da0dd467ba7746eaf4e73a177": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a2b4237574344e968ccab302841e635a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_773b12745b8c4db2ad5cf08f23290c19",
              "IPY_MODEL_d15519ba325c408d9108859b788320dd"
            ]
          }
        },
        "a2b4237574344e968ccab302841e635a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "773b12745b8c4db2ad5cf08f23290c19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f7e729c3b7ea402995d884d94f0832c5",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1005b6d9fffd4e42bf533ad53cc6ee2a"
          }
        },
        "d15519ba325c408d9108859b788320dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ca292ac9b08d40ff825c04917da69cd8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "‚Äã",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 1.45MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_966d8083eeef4a5bb372ca96ade712b4"
          }
        },
        "f7e729c3b7ea402995d884d94f0832c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1005b6d9fffd4e42bf533ad53cc6ee2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ca292ac9b08d40ff825c04917da69cd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "966d8083eeef4a5bb372ca96ade712b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfEHOTY4FMSS",
        "outputId": "eb879b64-2817-4df2-a567-6aa66cabd633"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "root_path = 'gdrive/My Drive/insta_data/'\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import random\n",
        "from tqdm.notebook import tqdm\n",
        "import string\n",
        "import re\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "\n",
        "import glob \n",
        "fileNames = glob.glob(root_path + \"/*.csv\")\n",
        "\n",
        "lst = []\n",
        "\n",
        "for fileName in fileNames:\n",
        "    print(fileName)\n",
        "    df = pd.read_csv(fileName, index_col=None, header=0)\n",
        "    lst.append(df)\n",
        "\n",
        "wf = pd.concat(lst, axis=0, ignore_index=True)\n",
        "\n",
        "post_df = wf[\"text\"]\n",
        "hashtag_df = wf[\"hashtags\"]\n",
        "comments_df = wf[\"comments\"]\n",
        "\n",
        "#https://stackoverflow.com/questions/33404752/removing-emojis-from-a-string-in-python\n",
        "def deEmojify(text):\n",
        "    regrex_pattern = re.compile(pattern = \"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           \"]+\", flags = re.UNICODE)\n",
        "    return regrex_pattern.sub(r'',str(text))\n",
        "\n",
        "# appending first 25 comments into one big comment\n",
        "def appendComments(comment):\n",
        "  lst = comment.split(\"|\")\n",
        "  final_comment = \"\"\n",
        "  for idx,entry in enumerate(lst):\n",
        "    if idx == 25:\n",
        "      break\n",
        "    entry.strip()\n",
        "    final_comment +=  entry + \" \"\n",
        "  return final_comment\n",
        "\n",
        "def cleanComment(comment):\n",
        "  deEmojified_comment = deEmojify(comment)\n",
        "  return appendComments(deEmojified_comment)\n",
        "\n",
        "\n",
        "wf[\"comments\"] = wf[\"comments\"].apply(lambda x:cleanComment(x))\n",
        "#combining comments and actual post\n",
        "wf[\"text_with_comments\"] = wf[\"text\"]+ \" \"+ wf[\"comments\"]\n",
        "# print(df['hashtags'])\n",
        "# print(df['text_with_comments'])\n",
        "df = pd.DataFrame()\n",
        "df['text_with_comments']=wf['text_with_comments']\n",
        "df['hashtags']=wf['hashtags']\n",
        "print(df.head())\n",
        "print(df.shape)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "gdrive/My Drive/insta_data/mitpics.csv\n",
            "gdrive/My Drive/insta_data/yoga.csv\n",
            "gdrive/My Drive/insta_data/hidden_shots_.csv\n",
            "gdrive/My Drive/insta_data/viceindia.csv\n",
            "gdrive/My Drive/insta_data/healthyfoodvideos.csv\n",
            "gdrive/My Drive/insta_data/pbsnature.csv\n",
            "gdrive/My Drive/insta_data/eucouncil.csv\n",
            "gdrive/My Drive/insta_data/afpsport.csv\n",
            "gdrive/My Drive/insta_data/nature.research.csv\n",
            "gdrive/My Drive/insta_data/ageofempiresgame.csv\n",
            "gdrive/My Drive/insta_data/ageofempires.csv\n",
            "gdrive/My Drive/insta_data/oxford_uni.csv\n",
            "gdrive/My Drive/insta_data/talentmoves.csv\n",
            "gdrive/My Drive/insta_data/educationaboutearth.csv\n",
            "gdrive/My Drive/insta_data/nature_africa.csv\n",
            "gdrive/My Drive/insta_data/abcwnn.csv\n",
            "gdrive/My Drive/insta_data/aamaadmiparty.csv\n",
            "gdrive/My Drive/insta_data/voxdotcom.csv\n",
            "gdrive/My Drive/insta_data/life.csv\n",
            "gdrive/My Drive/insta_data/world.csv\n",
            "gdrive/My Drive/insta_data/nytimesopinionart.csv\n",
            "gdrive/My Drive/insta_data/commonwealth_sec.csv\n",
            "gdrive/My Drive/insta_data/wanderlustfest.csv\n",
            "gdrive/My Drive/insta_data/ptiphotos.csv\n",
            "gdrive/My Drive/insta_data/sciencechannel.csv\n",
            "gdrive/My Drive/insta_data/scroll_in.csv\n",
            "gdrive/My Drive/insta_data/healthy.foodyss.csv\n",
            "                                  text_with_comments                                           hashtags\n",
            "0  üëè One of MIT's newest Nobel laureates Esther D...  mit womeninscience womeninstem nobelprize nobe...\n",
            "1  Congratulations to MIT‚Äôs Abhijit Banerjee and ...  mit nobelprize nobellaureate economicsciences ...\n",
            "2  Last Friday mechanical engineering seniors tak...  thisismit mit productengineering teambuilding ...\n",
            "3  Nearly 10 years ago Priyanka Bakaya MBA ‚Äô11 fo...  mit sustainability entrepreneurs plasticintofu...\n",
            "4  At MIT.nano we think every day is Nano Day. Bu...  mit nanotechnology nanotecnologia nanoscience ...\n",
            "(16253, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 698,
          "referenced_widgets": [
            "cf6e593da0dd467ba7746eaf4e73a177",
            "a2b4237574344e968ccab302841e635a",
            "773b12745b8c4db2ad5cf08f23290c19",
            "d15519ba325c408d9108859b788320dd",
            "f7e729c3b7ea402995d884d94f0832c5",
            "1005b6d9fffd4e42bf533ad53cc6ee2a",
            "ca292ac9b08d40ff825c04917da69cd8",
            "966d8083eeef4a5bb372ca96ade712b4"
          ]
        },
        "id": "j4VpLnxlFXUf",
        "outputId": "b0a58ec1-233c-47eb-8bee-3daf12d3b580"
      },
      "source": [
        "def cleanerEng(x):\n",
        "  x = str(x)\n",
        "  x = x.lower()\n",
        "  x = re.sub(r'[^a-z0-9]+',' ',x)\n",
        "  return x\n",
        "\n",
        "def cleanerHindi(x):\n",
        "  x = str(x)\n",
        "  x = re.sub(r'[-.‡•§|,?;:<>&$‚Çπ]+',' ',x)\n",
        "  return x\n",
        "\n",
        "punctuations = string.punctuation\n",
        "table = punctuations.maketrans(punctuations+string.ascii_uppercase,\n",
        "                               \" \"*len(punctuations)+string.ascii_lowercase,)\n",
        "\n",
        "def cleanSentence(sentence):\n",
        "  # sentence.strip()\n",
        "  sen = sentence.translate(table)\n",
        "  return sen\n",
        "\n",
        "def addTokens(x,start=False):\n",
        "  x.append('<END>')\n",
        "  if start:\n",
        "    x.insert(0,'<START>')\n",
        "  return list(x)\n",
        "  \n",
        "\n",
        "df.iloc[:,0] = df['text_with_comments'].apply(func=cleanerEng)\n",
        "df.iloc[:,0] = df['text_with_comments'].apply(func=cleanSentence)\n",
        "df.iloc[:,0] = df['text_with_comments'].apply(func= lambda x : (str(x).split()))\n",
        "df.iloc[:,0] = df['text_with_comments'].apply(func= addTokens,start=False)\n",
        "df.iloc[:,1] = df['hashtags'].apply(func= lambda x : (str(x).split()))\n",
        "df.iloc[:,1] = df['hashtags'].apply(func= addTokens,start=False)\n",
        "# df.iloc[:,1] = df['hindi'].apply(func= addTokens,start=True)\n",
        "\n",
        "articles = df['text_with_comments'].to_numpy()\n",
        "labels = df['hashtags'].to_numpy()\n",
        "\n",
        "!pip install transformers\n",
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer\n",
        "tokenizer=BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/83/e74092e7f24a08d751aa59b37a9fc572b2e4af3918cb66f7766c3affb1b4/transformers-3.5.1-py3-none-any.whl (1.3MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.3MB 5.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting sentencepiece==0.1.91\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.1MB 23.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 890kB 35.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Collecting tokenizers==0.9.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/34/b39eb9994bc3c999270b69c9eea40ecc6f0e97991dba28282b9fd32d44ee/tokenizers-0.9.3-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2.9MB 45.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=ce4086d2f518bd6be24ffaf5a62e2b657b8915a8d146e868ed13ae6e7486ba52\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.9.3 transformers-3.5.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cf6e593da0dd467ba7746eaf4e73a177",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti‚Ä¶"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abEYYztrFyN1"
      },
      "source": [
        "max_length_test = 50\n",
        "def map_example_to_dict(input_ids, attention_masks, token_type_ids, label):\n",
        "  return {\n",
        "      \"input_ids\": input_ids,\n",
        "      \"token_type_ids\": token_type_ids,\n",
        "      \"attention_mask\": attention_masks,\n",
        "  }, label\n",
        "\n",
        "def encode_examples(posts,labels):\n",
        "  # prepare list, so that we can build up final TensorFlow dataset from slices.\n",
        "  input_ids_list = []\n",
        "  token_type_ids_list = []\n",
        "  attention_mask_list = []\n",
        "  label_list = []\n",
        "  for post, label in zip(posts,labels):\n",
        "    if not isinstance(post, str):\n",
        "      continue\n",
        "    bert_input = tokenizer.encode_plus(post,\n",
        "                                       truncation=True,           \n",
        "                                      add_special_tokens = True, # add [CLS], [SEP]\n",
        "                                      max_length = max_length_test, # max length of the text that can go to BERT\n",
        "                                      padding='max_length', # add [PAD] tokens\n",
        "                                      return_attention_mask = True, # add attention mask to not focus on pad token\n",
        "\n",
        "                                      )\n",
        "  \n",
        "    input_ids_list.append(bert_input['input_ids'])\n",
        "    token_type_ids_list.append(bert_input['token_type_ids'])\n",
        "    attention_mask_list.append(bert_input['attention_mask'])\n",
        "    label_list.append([label])\n",
        "  return tf.data.Dataset.from_tensor_slices((input_ids_list, attention_mask_list, token_type_ids_list, label_list)).map(map_example_to_dict)\n",
        "\n",
        "def convert_to_tfdataset(articles,labels,batch_size = 30):\n",
        "  dataset = encode_examples(articles,labels)\n",
        "  dataset = dataset.shuffle(dataset.__len__().numpy()).batch(batch_size)\n",
        "  return dataset\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOFdWckQF099"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split( articles, labels, test_size=0.1, random_state=21,shuffle = True)\n",
        "train_dataset = convert_to_tfdataset(X_train,y_train)\n",
        "test_dataset = convert_to_tfdataset(X_test,y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nj4b-AwsF3j3"
      },
      "source": [
        "from transformers import BertConfig\n",
        "model_name = 'bert-base-uncased'\n",
        "from transformers import TFBertModel\n",
        "bert_model = TFBertModel.from_pretrained(model_name,config = config1)\n",
        "config1 = BertConfig.from_pretrained(model_name)\n",
        "config1.num_labels = 2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwmO-Up_GXBl"
      },
      "source": [
        "from transformers import TFBertModel\n",
        "\n",
        "model = TFBertModel.from_pretrained(model_name,config = config1)\n",
        "learning_rate = 2e-5\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=1e-08)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhwkdPLpGgdj"
      },
      "source": [
        "bert_history = model.fit(train_dataset,epochs=5,verbose=1,validation_data=test_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}