{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "world.csv\n",
      "eucouncil.csv\n",
      "README.md\n",
      "abcwnn.csv\n",
      "nytimesopinionart.csv\n",
      "yoga.csv\n",
      "daringplanet.csv\n",
      "Error tokenizing data. C error: Buffer overflow caught - possible malformed input file.\n",
      "\n",
      "hidden_shots_.csv\n",
      "talentmoves.csv\n",
      "nature.research.csv\n",
      "healthyfoodvideos.csv\n",
      "commonwealth_sec.csv\n",
      "healthy.foodyss.csv\n",
      "wanderlustfest.csv\n",
      "afpsport.csv\n",
      "ageofempires.csv\n",
      "ptiphotos.csv\n",
      "ageofempiresgame.csv\n",
      "mitpics.csv\n",
      "viceindia.csv\n",
      "sciencechannel.csv\n",
      "voxdotcom.csv\n",
      "life.csv\n",
      "scroll_in.csv\n",
      "educationaboutearth.csv\n",
      "oxford_uni.csv\n",
      "merge.py\n",
      "nature_africa.csv\n",
      "aamaadmiparty.csv\n",
      "pbsnature.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ID</th>\n",
       "      <th>shortcode</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>comments</th>\n",
       "      <th>likes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ethiopian Prime Minister Abiy Ahmed has been a...</td>\n",
       "      <td>ethiopia eritrea worldpeace peace nobelpeacepr...</td>\n",
       "      <td>|Wow .. I wish Modi Indian prime minister  PM ...</td>\n",
       "      <td>27279.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>For the first time in 40 years Iranian women w...</td>\n",
       "      <td>iran iranianwomen iransoccer soccer worldcup w...</td>\n",
       "      <td>|That is a progress then.. we are almost in 20...</td>\n",
       "      <td>45503.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A critically endangered eastern bongo was just...</td>\n",
       "      <td>easternbongo bongo animal animals cute chester...</td>\n",
       "      <td>|We need help breaking the record for most pen...</td>\n",
       "      <td>9145.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Six elephants fell to their deaths after tryin...</td>\n",
       "      <td>thailand thai elephant elephants nationalpark ...</td>\n",
       "      <td>|This is what we need in our community‚Äôs. Peop...</td>\n",
       "      <td>68705.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wednesday marks one year since Washington Post...</td>\n",
       "      <td>jamalkhashoggi justiceforjamalkhashoggi journa...</td>\n",
       "      <td>|–ü—Ä–∏–≤—Ç–¥–∞ –ù–∞—Ç–∞—à—É–ª–Ø –Ø —É–∂–µ –∫–∞–∫ —Ç—Ä–µ—Ç–∏–π –º–µ—Å–Ø—Ü –¥–æ–º–∞ ...</td>\n",
       "      <td>12438.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0  ID  shortcode  \\\n",
       "0          1 NaN        NaN   \n",
       "1          2 NaN        NaN   \n",
       "2          3 NaN        NaN   \n",
       "3          4 NaN        NaN   \n",
       "4          5 NaN        NaN   \n",
       "\n",
       "                                                text  \\\n",
       "0  Ethiopian Prime Minister Abiy Ahmed has been a...   \n",
       "1  For the first time in 40 years Iranian women w...   \n",
       "2  A critically endangered eastern bongo was just...   \n",
       "3  Six elephants fell to their deaths after tryin...   \n",
       "4  Wednesday marks one year since Washington Post...   \n",
       "\n",
       "                                            hashtags  \\\n",
       "0  ethiopia eritrea worldpeace peace nobelpeacepr...   \n",
       "1  iran iranianwomen iransoccer soccer worldcup w...   \n",
       "2  easternbongo bongo animal animals cute chester...   \n",
       "3  thailand thai elephant elephants nationalpark ...   \n",
       "4  jamalkhashoggi justiceforjamalkhashoggi journa...   \n",
       "\n",
       "                                            comments    likes  \n",
       "0  |Wow .. I wish Modi Indian prime minister  PM ...  27279.0  \n",
       "1  |That is a progress then.. we are almost in 20...  45503.0  \n",
       "2  |We need help breaking the record for most pen...   9145.0  \n",
       "3  |This is what we need in our community‚Äôs. Peop...  68705.0  \n",
       "4  |–ü—Ä–∏–≤—Ç–¥–∞ –ù–∞—Ç–∞—à—É–ª–Ø –Ø —É–∂–µ –∫–∞–∫ —Ç—Ä–µ—Ç–∏–π –º–µ—Å–Ø—Ü –¥–æ–º–∞ ...  12438.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "for file in os.listdir('../data/insta_non_mentalhealth/'):\n",
    "    print(file)\n",
    "    if file[-3:] == \"csv\":\n",
    "        try:\n",
    "#             print('True')\n",
    "            path = \"../data/insta_non_mentalhealth/\"+str(file)\n",
    "            df = pd.read_csv(path)\n",
    "            data.append(df)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "df = pd.concat(data, axis=0, ignore_index=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/33404752/removing-emojis-from-a-string-in-python\n",
    "def deEmojify(text):\n",
    "    regrex_pattern = re.compile(pattern = \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags = re.UNICODE)\n",
    "    return regrex_pattern.sub(r'',str(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending first 25 comments into one big comment\n",
    "def appendComments(comment):\n",
    "  lst = comment.split(\"|\")\n",
    "  final_comment = \"\"\n",
    "  for idx,entry in enumerate(lst):\n",
    "    if idx == 25:\n",
    "      break\n",
    "    entry.strip()\n",
    "    final_comment +=  entry + \" \"\n",
    "  return final_comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanComment(comment):\n",
    "  deEmojified_comment = deEmojify(comment)\n",
    "  return appendComments(deEmojified_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"comments\"] = df[\"comments\"].apply(lambda x:cleanComment(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combining comments and actual post\n",
    "df[\"text_with_comments\"] = df[\"text\"]+ \" \"+ df[\"comments\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing punctuations,and converting upper to lower case letters\n",
    "\n",
    "punctuations = string.punctuation\n",
    "table = punctuations.maketrans(punctuations+string.ascii_uppercase,\n",
    "                               \" \"*len(punctuations)+string.ascii_lowercase,)\n",
    "\n",
    "def cleanPosts(sentence):\n",
    "  sentence.strip()\n",
    "  sen = sentence.translate(table)\n",
    "  return sen\n",
    "\n",
    "def cleanHashTags(sentence):\n",
    "  sentence.strip()\n",
    "  sen = sentence.translate(table)\n",
    "#   return \"$start \" + sen + \" end$\"\n",
    "  return sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_df = df[\"text_with_comments\"].apply(lambda w:cleanPosts(str(w)))\n",
    "hashtag_df = df[\"hashtags\"].apply(lambda w: cleanHashTags(str(w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Likes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we only consider hashtags that are appearing atleast 10 times\n",
    "def count_hashtag_likes(all_posts_hashtags,all_posts_likes):\n",
    "  hashtag_likesCount = defaultdict(int)\n",
    "  hashtagAppearanceCount = defaultdict(int)\n",
    "  hashtag_first10_likesCount = defaultdict(int)\n",
    "  for hashtags,count in zip(all_posts_hashtags,all_posts_likes):\n",
    "    hashtags = str(hashtags).split()\n",
    "    for hashtag in hashtags:\n",
    "      hashtagAppearanceCount[hashtag] += 1\n",
    "      if hashtagAppearanceCount[hashtag] >= 10:\n",
    "        if hashtag not in hashtag_likesCount:\n",
    "          hashtag_likesCount[hashtag] = hashtag_first10_likesCount[hashtag] + count\n",
    "        hashtag_likesCount[hashtag] += count\n",
    "        continue\n",
    "      hashtag_first10_likesCount[hashtag] += count      \n",
    "  return hashtag_likesCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2036\n"
     ]
    }
   ],
   "source": [
    "hashtag_likesCount = count_hashtag_likes(hashtag_df.values.tolist(),df[\"likes\"].values.tolist())\n",
    "print(len(hashtag_likesCount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2036\n"
     ]
    }
   ],
   "source": [
    "#  list of hashtags appearing atleast 10 times\n",
    "frequent_hashtags = hashtag_likesCount.keys()\n",
    "print(len(frequent_hashtags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X, X_val, Y, Y_val = train_test_split(post_df,hashtag_df, test_size=0.10,shuffle = True)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.10,shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13687       ipl mumbaiindians csk chennaisuperkings cskvmi\n",
       "4174     hidshots wanderfolk lifeofadventure igmasters ...\n",
       "1002                        europeans rugby lgbt lgbtiq eu\n",
       "1458     refugees migrants refugeecrisis migrationeu ju...\n",
       "5054     healthyeating delicious healthylifestyle healt...\n",
       "Name: hashtags, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14790\n",
      "1463\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_with_comments</th>\n",
       "      <th>hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13687</th>\n",
       "      <td>mumbai indians clinched their fourth title wit...</td>\n",
       "      <td>ipl mumbaiindians csk chennaisuperkings cskvmi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4174</th>\n",
       "      <td>pc visualsofjulius    throwing boiling water i...</td>\n",
       "      <td>hidshots wanderfolk lifeofadventure igmasters ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>in his inclusive rugby team gerald from irelan...</td>\n",
       "      <td>europeans rugby lgbt lgbtiq eu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>looking ahead to tomorrow s justice and home a...</td>\n",
       "      <td>refugees migrants refugeecrisis migrationeu ju...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5054</th>\n",
       "      <td>üòãbreakfast sausage   guacamoleü•ë stacks üí™üèºüòç qui...</td>\n",
       "      <td>healthyeating delicious healthylifestyle healt...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      text_with_comments  \\\n",
       "13687  mumbai indians clinched their fourth title wit...   \n",
       "4174   pc visualsofjulius    throwing boiling water i...   \n",
       "1002   in his inclusive rugby team gerald from irelan...   \n",
       "1458   looking ahead to tomorrow s justice and home a...   \n",
       "5054   üòãbreakfast sausage   guacamoleü•ë stacks üí™üèºüòç qui...   \n",
       "\n",
       "                                                hashtags  \n",
       "13687     ipl mumbaiindians csk chennaisuperkings cskvmi  \n",
       "4174   hidshots wanderfolk lifeofadventure igmasters ...  \n",
       "1002                      europeans rugby lgbt lgbtiq eu  \n",
       "1458   refugees migrants refugeecrisis migrationeu ju...  \n",
       "5054   healthyeating delicious healthylifestyle healt...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ftrain = pd.concat([X_train, X_val])\n",
    "Y_ftrain = pd.concat([Y_train, Y_val])\n",
    "frames = [X_ftrain, Y_ftrain]\n",
    "train_df = pd.concat(frames, axis=1, join='inner')\n",
    "# print(result.loc[[8737]])\n",
    "print(len(train_df))\n",
    "frames = [X_test, Y_test]\n",
    "test_df = pd.concat(frames, axis=1, join='inner')\n",
    "print(len(test_df))\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_text</th>\n",
       "      <th>target_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13221</th>\n",
       "      <td>the eiffel tower officially opened 129 years a...</td>\n",
       "      <td>eiffeltower paris france</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15864</th>\n",
       "      <td>no  bjp4india no  incindia no  aamaadmiparty\\n...</td>\n",
       "      <td>ripbravehearts pulwamaattack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2102</th>\n",
       "      <td>try day friday   and they brought reena and i ...</td>\n",
       "      <td>nothanks wnn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2658</th>\n",
       "      <td>donald trump‚Äôs muslim ban is cowardly and dan...</td>\n",
       "      <td>art illustration muslimban</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2315</th>\n",
       "      <td>to live and die in paris  people have been dy...</td>\n",
       "      <td>art illustration death paris france funeral pl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              input_text  \\\n",
       "13221  the eiffel tower officially opened 129 years a...   \n",
       "15864  no  bjp4india no  incindia no  aamaadmiparty\\n...   \n",
       "2102   try day friday   and they brought reena and i ...   \n",
       "2658    donald trump‚Äôs muslim ban is cowardly and dan...   \n",
       "2315    to live and die in paris  people have been dy...   \n",
       "\n",
       "                                             target_text  \n",
       "13221                           eiffeltower paris france  \n",
       "15864                       ripbravehearts pulwamaattack  \n",
       "2102                                        nothanks wnn  \n",
       "2658                          art illustration muslimban  \n",
       "2315   art illustration death paris france funeral pl...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = train_df.rename(\n",
    "    columns={\"text_with_comments\": \"input_text\", \"hashtags\": \"target_text\"}\n",
    ")\n",
    "train_df.head()\n",
    "test_df = test_df.rename(\n",
    "    columns={\"index\":\"prefix\", \"text_with_comments\": \"input_text\", \"hashtags\": \"target_text\"}\n",
    ")\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from simpletransformers.seq2seq import Seq2SeqModel, Seq2SeqArgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "import pandas as pd\n",
    "import requests, zipfile\n",
    "try:\n",
    "    from StringIO import StringIO\n",
    "except ImportError:\n",
    "    from io import StringIO\n",
    "import os\n",
    "\n",
    "def download_data():\n",
    "    if 'train_FD004.txt' not in os.listdir('data'):\n",
    "        print('Downloading Data...')\n",
    "        # Download the data\n",
    "        r = requests.get(\"https://ti.arc.nasa.gov/c/6/\", stream=True)\n",
    "        z = zipfile.ZipFile(StringIO.StringIO(r.content))\n",
    "        z.extractall('data')\n",
    "    else:\n",
    "        print('Using previously downloaded data')\n",
    "    \n",
    "def load_data(data_path):   \n",
    "    operational_settings = ['operational_setting_{}'.format(i + 1) for i in range (3)]\n",
    "    sensor_columns = ['sensor_measurement_{}'.format(i + 1) for i in range(26)]\n",
    "    cols = ['engine_no', 'time_in_cycles'] + operational_settings + sensor_columns\n",
    "    data = pd.read_csv(data_path, sep=' ', header=None, names=cols)\n",
    "    data = data.drop(cols[-5:], axis=1)\n",
    "    data['index'] = data.index\n",
    "    data.index = data['index']\n",
    "    data['time'] = pd.date_range('1/1/2000', periods=data.shape[0], freq='600s')\n",
    "    print('Loaded data with:\\n{} Recordings\\n{} Engines'.format(\n",
    "        data.shape[0], len(data['engine_no'].unique())))\n",
    "    print('21 Sensor Measurements\\n3 Operational Settings')\n",
    "    return data\n",
    "\n",
    "def new_labels(data, labels):\n",
    "    ct_ids = []\n",
    "    ct_times = []\n",
    "    ct_labels = []\n",
    "    data = data.copy()\n",
    "    data['RUL'] = labels\n",
    "    gb = data.groupby(['engine_no'])\n",
    "    for engine_no_df in gb:\n",
    "        instances = engine_no_df[1].shape[0]\n",
    "        r = randint(5, instances - 1)\n",
    "        ct_ids.append(engine_no_df[1].iloc[r,:]['engine_no'])\n",
    "        ct_times.append(engine_no_df[1].iloc[r,:]['time'])\n",
    "        ct_labels.append(engine_no_df[1].iloc[r,:]['RUL'])\n",
    "    ct = pd.DataFrame({'engine_no': ct_ids,\n",
    "                       'cutoff_time': ct_times,\n",
    "                       'RUL': ct_labels})\n",
    "    ct = ct[['engine_no', 'cutoff_time', 'RUL']]\n",
    "    ct.index = ct['engine_no']\n",
    "    ct.index = ct.index.rename('index')\n",
    "    return ct\n",
    "\n",
    "def make_cutoff_times(data):\n",
    "    gb = data.groupby(['engine_no'])\n",
    "    labels = []\n",
    "\n",
    "\n",
    "    for engine_no_df in gb:\n",
    "        instances = engine_no_df[1].shape[0]\n",
    "        label = [instances - i - 1 for i in range(instances)]\n",
    "        labels += label\n",
    "    \n",
    "    return new_labels(data, labels)\n",
    "\n",
    "def feature_importances(X, reg, feats=5):\n",
    "    feature_imps = [(imp, X.columns[i]) \n",
    "                    for i, imp in enumerate(reg.feature_importances_)]\n",
    "    feature_imps.sort()\n",
    "    feature_imps.reverse()\n",
    "    for i, f in enumerate(feature_imps[0:feats]):\n",
    "        print('{}: {} [{:.3f}]'.format(i + 1, f[1], f[0]))\n",
    "    print('-----\\n')\n",
    "    return [f[1] for f in feature_imps[:feats]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = Seq2SeqArgs()\n",
    "model_args.do_sample = True\n",
    "model_args.eval_batch_size = 64\n",
    "model_args.evaluate_during_training = True\n",
    "model_args.evaluate_during_training_steps = 2500\n",
    "model_args.evaluate_during_training_verbose = True\n",
    "model_args.fp16 = False\n",
    "model_args.learning_rate = 5e-5\n",
    "model_args.max_length = 128\n",
    "model_args.max_seq_length = 128\n",
    "model_args.num_beams = None\n",
    "model_args.num_return_sequences = 3\n",
    "model_args.num_train_epochs = 2\n",
    "model_args.overwrite_output_dir = True\n",
    "model_args.reprocess_input_data = True\n",
    "model_args.save_eval_checkpoints = False\n",
    "model_args.save_steps = -1\n",
    "model_args.top_k = 50\n",
    "model_args.top_p = 0.95\n",
    "model_args.train_batch_size = 8\n",
    "model_args.use_multiprocessing = False\n",
    "# model_args.use_cuda = False\n",
    "model_args.wandb_project = \"Paraphrasing with BART\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2SeqModel(\n",
    "    encoder_decoder_type=\"bart\",\n",
    "    encoder_decoder_name=\"facebook/bart-large\",\n",
    "    use_cuda=False,\n",
    "    args=model_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.seq2seq.seq2seq_utils: Creating features from dataset file at cache_dir/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84aae774dfa240e8847965856f9b3000",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=14790.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.seq2seq.seq2seq_model: Training started\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37f8cef150004982b23cf7e47a2a6af7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Epoch'), FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmmvv\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.10<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">playful-blaze-4</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/mmvv/Paraphrasing%20with%20BART\" target=\"_blank\">https://wandb.ai/mmvv/Paraphrasing%20with%20BART</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/mmvv/Paraphrasing%20with%20BART/runs/1r7qpioe\" target=\"_blank\">https://wandb.ai/mmvv/Paraphrasing%20with%20BART/runs/1r7qpioe</a><br/>\n",
       "                Run data is saved locally in <code>/home/medha/College/Sem5/IRE/Major_Project/HashtagGeneration/bert/wandb/run-20201118_112804-1r7qpioe</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99ef629e102f44489a2f61a85ec8a347",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Running Epoch 0 of 2'), FloatProgress(value=0.0, max=1849.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.train_model(train_df, eval_data=test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_predict = [\n",
    "    str(input_text)\n",
    "    for input_text in zip(test_df[\"input_text\"].tolist())\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth = test_df[\"target_text\"].tolist()\n",
    "\n",
    "preds = model.predict(to_predict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir('./results'):\n",
    "    os.makedir('./results')\n",
    "    \n",
    "with open(f\"results/predictions_{datetime.now()}.txt\", \"w\") as f:\n",
    "    for i, text in enumerate(test_df[\"input_text\"].tolist()):\n",
    "        f.write(str(text) + \"\\n\\n\")\n",
    "\n",
    "        f.write(\"Truth:\\n\")\n",
    "        f.write(truth[i] + \"\\n\\n\")\n",
    "\n",
    "        f.write(\"Prediction:\\n\")\n",
    "        for pred in preds[i]:\n",
    "            f.write(str(pred) + \"\\n\")\n",
    "        f.write(\n",
    "            \"________________________________________________________________________________\\n\"\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
