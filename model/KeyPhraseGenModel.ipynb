{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of KeyPhraseGeneration_IRE",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fNJL7sis9ir"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import string\n",
        "import re\n",
        "from collections import defaultdict"
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThENIJNGuyFT",
        "outputId": "724a2dbe-2096-489c-9583-e70603ce5ad4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        }
      },
      "source": [
        "import glob\n",
        "\n",
        "path = r'/content/drive/My Drive/Instagram_Data' \n",
        "fileNames = glob.glob(path + \"/*.csv\")\n",
        "\n",
        "lst = []\n",
        "\n",
        "for fileName in fileNames:\n",
        "    print(fileName)\n",
        "    df = pd.read_csv(fileName, index_col=None, header=0)\n",
        "    lst.append(df)\n",
        "\n",
        "df = pd.concat(lst, axis=0, ignore_index=True)"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Instagram_Data/mitpics.csv\n",
            "/content/drive/My Drive/Instagram_Data/yoga.csv\n",
            "/content/drive/My Drive/Instagram_Data/world.csv\n",
            "/content/drive/My Drive/Instagram_Data/viceindia.csv\n",
            "/content/drive/My Drive/Instagram_Data/abcwnn.csv\n",
            "/content/drive/My Drive/Instagram_Data/aamaadmiparty.csv\n",
            "/content/drive/My Drive/Instagram_Data/talentmoves.csv\n",
            "/content/drive/My Drive/Instagram_Data/ageofempires.csv\n",
            "/content/drive/My Drive/Instagram_Data/ageofempiresgame.csv\n",
            "/content/drive/My Drive/Instagram_Data/voxdotcom.csv\n",
            "/content/drive/My Drive/Instagram_Data/oxford_uni.csv\n",
            "/content/drive/My Drive/Instagram_Data/nature_africa.csv\n",
            "/content/drive/My Drive/Instagram_Data/life.csv\n",
            "/content/drive/My Drive/Instagram_Data/pbsnature.csv\n",
            "/content/drive/My Drive/Instagram_Data/healthyfoodvideos.csv\n",
            "/content/drive/My Drive/Instagram_Data/nature.research.csv\n",
            "/content/drive/My Drive/Instagram_Data/eucouncil.csv\n",
            "/content/drive/My Drive/Instagram_Data/hidden_shots_.csv\n",
            "/content/drive/My Drive/Instagram_Data/educationaboutearth.csv\n",
            "/content/drive/My Drive/Instagram_Data/afpsport.csv\n",
            "/content/drive/My Drive/Instagram_Data/nytimesopinionart.csv\n",
            "/content/drive/My Drive/Instagram_Data/scroll_in.csv\n",
            "/content/drive/My Drive/Instagram_Data/commonwealth_sec.csv\n",
            "/content/drive/My Drive/Instagram_Data/ptiphotos.csv\n",
            "/content/drive/My Drive/Instagram_Data/wanderlustfest.csv\n",
            "/content/drive/My Drive/Instagram_Data/sciencechannel.csv\n",
            "/content/drive/My Drive/Instagram_Data/healthy.foodyss.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pa-EMMf1yhUb",
        "outputId": "8ac0c168-a33a-4b18-86bc-c8f256b46829",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>ID</th>\n",
              "      <th>shortcode</th>\n",
              "      <th>text</th>\n",
              "      <th>hashtags</th>\n",
              "      <th>comments</th>\n",
              "      <th>likes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>👏 One of MIT's newest Nobel laureates Esther D...</td>\n",
              "      <td>mit womeninscience womeninstem nobelprize nobe...</td>\n",
              "      <td>|Congratulations!!🙌🙌🙌|What was the award for?|...</td>\n",
              "      <td>3785.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Congratulations to MIT’s Abhijit Banerjee and ...</td>\n",
              "      <td>mit nobelprize nobellaureate economicsciences ...</td>\n",
              "      <td>|Congratulations!|congratulations🎉🎉🎉🎉|@jarad_p...</td>\n",
              "      <td>4645.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Last Friday mechanical engineering seniors tak...</td>\n",
              "      <td>thisismit mit productengineering teambuilding ...</td>\n",
              "      <td>|🔥🔥🔥🔥🔥🔥🔥🔥|Is \"cracker causeway\" kind of a give...</td>\n",
              "      <td>2919.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Nearly 10 years ago Priyanka Bakaya MBA ’11 fo...</td>\n",
              "      <td>mit sustainability entrepreneurs plasticintofu...</td>\n",
              "      <td>|enerjide çevreci adim atmak geleceğimizi güçl...</td>\n",
              "      <td>2860.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>At MIT.nano we think every day is Nano Day. Bu...</td>\n",
              "      <td>mit nanotechnology nanotecnologia nanoscience ...</td>\n",
              "      <td>|Smart dust|@takkksh|@photechno|@jarad_ponce|@...</td>\n",
              "      <td>5555.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Unnamed: 0  ID  ...                                           comments   likes\n",
              "0          1 NaN  ...  |Congratulations!!🙌🙌🙌|What was the award for?|...  3785.0\n",
              "1          2 NaN  ...  |Congratulations!|congratulations🎉🎉🎉🎉|@jarad_p...  4645.0\n",
              "2          3 NaN  ...  |🔥🔥🔥🔥🔥🔥🔥🔥|Is \"cracker causeway\" kind of a give...  2919.0\n",
              "3          4 NaN  ...  |enerjide çevreci adim atmak geleceğimizi güçl...  2860.0\n",
              "4          5 NaN  ...  |Smart dust|@takkksh|@photechno|@jarad_ponce|@...  5555.0\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-MHV0hiNhvZ",
        "outputId": "caff3510-2ce7-44ef-bf1c-4de6c506c042",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(len(df))"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16253\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hG37E12xA4g",
        "outputId": "059a70cc-c0e2-44ed-eb52-f0443ef5f06e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        }
      },
      "source": [
        "print(df[\"text\"].iloc[1])"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Congratulations to MIT’s Abhijit Banerjee and Esther Duflo have won the Nobel Prize in economics “for their experimental approach to alleviating global poverty.\" They share the prize with Michael Kremer of Harvard.\n",
            "\n",
            "Reposted from @nobelprize_org (@get_regrann) \n",
            "The research conducted by this year’s Laureates has considerably improved our ability to fight global poverty. In just two decades their new experiment-based approach has transformed development economics which is now a flourishing field of research. \n",
            "Despite recent dramatic improvements one of humanity’s most urgent issues is the reduction of global poverty in all its forms. More than 700 million people still subsist on extremely low incomes. Every year around five million children under the age of five still die of diseases that could often have been prevented or cured with inexpensive treatments. Half of the world’s children still leave school without basic literacy and numeracy skills. \n",
            "This year’s Laureates have introduced a new approach to obtaining reliable answers about the best ways to fight global poverty. In brief it involves dividing this issue into smaller more manageable questions – for example the most effective interventions for improving educational outcomes or child health. They have shown that these smaller more precise questions are often best answered via carefully designed experiments among the people who are most affected. \n",
            "The 2019 Economic Sciences Laureates have played a decisive role in reshaping research in development economics. Over just 20 years the subject has become a flourishing primarily experimental area of mainstream economics. This new experiment-based research has already helped in alleviating global poverty and has great potential to further improve the lives of the most impoverished people on the planet.  See link in profile for news article.\n",
            "\n",
            "#mit #NobelPrize #NobelLaureate #EconomicSciences #economics #poverty #endpoverty #research #experiment #science #society #prize #award #NobelPrize2019 #NobelPrizeannouncement #economicsprize #globalpoverty #education #health #healthcare #childhealth #development  #regrann\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2b4CClHBzjvP",
        "outputId": "851ab1c0-b8f8-49de-c58f-6c720283a95c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(df[\"hashtags\"].iloc[1])"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mit nobelprize nobellaureate economicsciences economics poverty endpoverty research experiment science society prize award nobelprize2019 nobelprizeannouncement economicsprize globalpoverty education health healthcare childhealth development regrann\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEKRcv3y0P29",
        "outputId": "b20676ef-6d4d-43b2-c6ef-db5c9bb701a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(df[\"comments\"].iloc[1])"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "|Congratulations!|congratulations🎉🎉🎉🎉|@jarad_ponce|Congratulations|😍😍😍🔥🔥|Congratulations 🎊 🎊 🎊 🎊|🇮🇳|Great!! Congratulations to the winners and to the MIT|Congratulations from South Africa👏|The reason for I love MIT 🔥🔥🔥 🇮🇳💪💗|Congratulations! 🙏|🎉🙏😁|🙏🏻🙌🏼|There is no such thing as a nobel prize in economics|@e.lauer|Congratulations from India|MIT MITMITMIT MIT ❤️❤️❤️❤️|शुभकामनाएं|❤️|Amazing!!! Congratulations 😍|Many congratulations..|👏👏👏👏👏👏👏👏|❤️|excellent🙌🙌🙌|Congrats 👏👏👏|@cuartosectoreco|Enhorabuena!!|Really glad to have such eminent personal in MIT N now I know why MIT is best university 😀|Congratulations!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRHggXkIFFgC"
      },
      "source": [
        "## Data Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjG_ei2GLUOc"
      },
      "source": [
        "### Cleaning Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7WsJPPw8D1N"
      },
      "source": [
        "#https://stackoverflow.com/questions/33404752/removing-emojis-from-a-string-in-python\n",
        "def deEmojify(text):\n",
        "    regrex_pattern = re.compile(pattern = \"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           \"]+\", flags = re.UNICODE)\n",
        "    return regrex_pattern.sub(r'',str(text))"
      ],
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEdQeQ7_GFvC"
      },
      "source": [
        "# appending first 25 comments into one big comment\n",
        "def appendComments(comment):\n",
        "  lst = comment.split(\"|\")\n",
        "  final_comment = \"\"\n",
        "  for idx,entry in enumerate(lst):\n",
        "    if idx == 25:\n",
        "      break\n",
        "    entry.strip()\n",
        "    final_comment +=  entry + \" \"\n",
        "  return final_comment"
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCzTCqbOGJCf"
      },
      "source": [
        "def cleanComment(comment):\n",
        "  deEmojified_comment = deEmojify(comment)\n",
        "  return appendComments(deEmojified_comment)"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXLNIwty-Os6"
      },
      "source": [
        "df[\"comments\"] = df[\"comments\"].apply(lambda x:cleanComment(x))"
      ],
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGKlZTx6DucL"
      },
      "source": [
        "#combining comments and actual post\n",
        "df[\"text_with_comments\"] = df[\"text\"]+ \" \"+ df[\"comments\"]"
      ],
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhvWwy2O0pOK"
      },
      "source": [
        "# removing punctuations,and converting upper to lower case letters\n",
        "\n",
        "punctuations = string.punctuation\n",
        "table = punctuations.maketrans(punctuations+string.ascii_uppercase,\n",
        "                               \" \"*len(punctuations)+string.ascii_lowercase,)\n",
        "\n",
        "def cleanPosts(sentence):\n",
        "  sentence.strip()\n",
        "  sen = sentence.translate(table)\n",
        "  return sen\n",
        "\n",
        "def cleanHashTags(sentence):\n",
        "  sentence.strip()\n",
        "  sen = sentence.translate(table)\n",
        "  return \"$start \" + sen + \" end$\"\n"
      ],
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71PRmWo96ZAm"
      },
      "source": [
        "post_df = df[\"text_with_comments\"].apply(lambda w:cleanPosts(str(w)))\n",
        "hashtag_df = df[\"hashtags\"].apply(lambda w: cleanHashTags(str(w)))"
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9wiB2s_NMzx",
        "outputId": "b3af9fba-de76-4b0c-a3f8-a76fb256088f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(hashtag_df.iloc[1])"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "$start mit nobelprize nobellaureate economicsciences economics poverty endpoverty research experiment science society prize award nobelprize2019 nobelprizeannouncement economicsprize globalpoverty education health healthcare childhealth development regrann end$\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRMHCl1fHAjh"
      },
      "source": [
        "#### Hashtags and Likes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1_bDswAHIbw"
      },
      "source": [
        "# here we only consider hashtags that are appearing atleast 10 times\n",
        "def count_hashtag_likes(all_posts_hashtags,all_posts_likes):\n",
        "  hashtag_likesCount = defaultdict(int)\n",
        "  hashtagAppearanceCount = defaultdict(int)\n",
        "  hashtag_first10_likesCount = defaultdict(int)\n",
        "  for hashtags,count in zip(all_posts_hashtags,all_posts_likes):\n",
        "    hashtags = str(hashtags).split()\n",
        "    for hashtag in hashtags:\n",
        "      hashtagAppearanceCount[hashtag] += 1\n",
        "      if hashtagAppearanceCount[hashtag] >= 10:\n",
        "        if hashtag not in hashtag_likesCount:\n",
        "          hashtag_likesCount[hashtag] = hashtag_first10_likesCount[hashtag] + count\n",
        "        hashtag_likesCount[hashtag] += count\n",
        "        continue\n",
        "      hashtag_first10_likesCount[hashtag] += count      \n",
        "  return hashtag_likesCount      "
      ],
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTUWJXuLJAgH",
        "outputId": "10c2e671-26ac-4e08-c64b-b587a81d19eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "hashtag_likesCount = count_hashtag_likes(hashtag_df.values.tolist(),df[\"likes\"].values.tolist())\n",
        "print(len(hashtag_likesCount))"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2038\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IDYmG8FZLOc",
        "outputId": "8a8df016-9b18-464a-c6fa-5b85ddc0c109",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# list of hashtags appearing atleast 10 times\n",
        "frequent_hashtags = hashtag_likesCount.keys()\n",
        "print(len(frequent_hashtags))"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2038\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h10EQKDrLass"
      },
      "source": [
        "#### Splitting Data to Train ,Test and Validation Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUs4l7YgjI57"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X, X_val, Y, Y_val = train_test_split(post_df,hashtag_df, test_size=0.10,shuffle = True)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.10,shuffle = True)"
      ],
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9ENbUmJPgTL",
        "outputId": "7a37a03b-349d-465f-c549-518f96a898d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "print(X_train.shape,Y_train.shape)\n",
        "print(X_val.shape,Y_val.shape)\n",
        "print(X_test.shape,Y_test.shape)"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(13164,) (13164,)\n",
            "(1626,) (1626,)\n",
            "(1463,) (1463,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0qgms1Vdoqx",
        "outputId": "01b8c451-8d00-4ec7-ae1d-2e8770c18038",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "type(X_test)"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pandas.core.series.Series"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3RRpI3YMO1R"
      },
      "source": [
        "### Creating vector representations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xjRi2tFLHXo"
      },
      "source": [
        "class ConstructIndexes():\n",
        "  def __init__(self, data):\n",
        "    \n",
        "    self.data = data\n",
        "    self.word2idx = {}\n",
        "    self.idx2word = {}\n",
        "    self.vocab = set()\n",
        "\n",
        "    self.create_index()\n",
        "        \n",
        "  def create_index(self):\n",
        "    # we consider words appearing atleas 10 times\n",
        "    wordAppearanceCount = defaultdict(int)\n",
        "    for phrase in self.data:\n",
        "      for word in phrase.split():\n",
        "        wordAppearanceCount[word] += 1\n",
        "        if wordAppearanceCount[word] >= 10:\n",
        "          self.vocab.add(word)\n",
        "            \n",
        "    # sort the vocab\n",
        "    self.vocab = sorted(self.vocab)\n",
        "\n",
        "    # add a padding token with index 0\n",
        "    self.word2idx['<pad>'] = 0\n",
        "        \n",
        "    # word to index mapping\n",
        "    for index, word in enumerate(self.vocab):\n",
        "        self.word2idx[word] = index + 1 # +1 because of pad token\n",
        "        \n",
        "    # index to word mapping\n",
        "    for word, index in self.word2idx.items():\n",
        "        self.idx2word[index] = word      "
      ],
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Rl4eVsQaiZa"
      },
      "source": [
        "# creating word2idx, idx2word on train data\n",
        "# we use utils constructed on train to convert test and validation data\n",
        "post_utils = ConstructIndexes(X_train.values.tolist())\n",
        "hashtag_utils = ConstructIndexes(Y_train.values.tolist())"
      ],
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJecUoRmeKNN",
        "outputId": "8a69cf16-0a51-4fcd-f194-8e4fdb6c28fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print(len(post_utils.word2idx))\n",
        "print(len(hashtag_utils.word2idx))"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13617\n",
            "1697\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F35oC8UaM7BT"
      },
      "source": [
        "# creating numeric vector corresponding to each post+comment\n",
        "def toNumVec(constructIndexClass,df):\n",
        "  tensor = []\n",
        "  for phrase in df:\n",
        "    phrase_lst = []\n",
        "    for word in phrase.split():\n",
        "      if word in constructIndexClass.word2idx:\n",
        "        phrase_lst.append(constructIndexClass.word2idx[word])\n",
        "    tensor.append(phrase_lst)\n",
        "  return tensor"
      ],
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpqVZsoxNeGB",
        "outputId": "b3239ef2-5310-4a29-e0a2-2d9b7c45cf89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Converting X_train,Y_train text sentences into numeric vectors\n",
        "input_tensor_train = toNumVec(post_utils,X_train)\n",
        "target_tensor_train = toNumVec(hashtag_utils,Y_train)\n",
        "print(len(input_tensor_train),len(target_tensor_train))"
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13164 13164\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnAj9KWfOFMy",
        "outputId": "62412694-d779-4486-a584-b7654702e5f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Converting X_val,Y_val text sentences into numeric vectors\n",
        "input_tensor_val = toNumVec(post_utils,X_val)\n",
        "target_tensor_val = toNumVec(hashtag_utils,Y_val)\n",
        "print(len(input_tensor_val),len(target_tensor_val))"
      ],
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1626 1626\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uu1_vUF6ORob",
        "outputId": "37996b40-5bac-462b-822d-853ceb7279ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Converting X_test,Y_test text sentences into numeric vectors\n",
        "input_tensor_test = toNumVec(post_utils,X_test)\n",
        "target_tensor_test = toNumVec(hashtag_utils,Y_test)\n",
        "print(len(input_tensor_test),len(target_tensor_test))"
      ],
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1463 1463\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SAlBEbKaldc",
        "outputId": "e96d7b05-ae88-4e1a-9095-47ab6d4690ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "print(input_tensor_train[1])\n",
        "print(target_tensor_train[1])"
      ],
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[5811, 6986, 12654, 11610, 10471, 8621, 12310, 5504, 4786, 11715, 10256, 719, 8963, 776, 6487, 8763, 13412, 4571, 12146, 5460, 11715, 4890, 4345, 13412, 6408, 8757, 7694, 2888, 2890, 5962, 8759, 8763, 8767, 5849, 5052, 11543, 9973, 337, 1357, 1997, 11161, 337, 3005, 6988, 6120, 11161, 337, 3002, 10471, 11543, 9973, 11161, 337, 3002, 8787, 11161, 337, 3002, 10471, 5274, 5845, 3121, 4604, 5849, 11785, 7003, 6120, 12763, 11610, 6957, 719, 8007, 1997, 6957, 10695, 3002, 11543, 9973, 337, 1357, 2387, 10695, 6988, 11610, 2387, 12547, 337, 3552, 1357, 8748, 1357, 2387, 11574, 6988, 6120, 1357, 10471, 3002, 1166, 1166, 1166, 3002, 6889, 4922, 6170, 5488, 10695, 1357, 5274, 5845, 3121, 13398, 10414, 6957, 6835, 337, 9115, 11161, 337, 3002, 5845, 3121, 10471, 8007, 10471, 6988, 6120, 10695, 3002, 776, 1357, 2387, 6994]\n",
            "[1, 697, 1010, 314, 316, 787, 1144, 1147, 1150, 747, 643, 467]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4ZfyvP7Qfkg"
      },
      "source": [
        "#### Padding Numeric Vectors to same size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCDWASfhfw49",
        "outputId": "7d87705b-8780-44fb-ce53-9f909fbab5d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "max_length_inp = max([len(t) for t in input_tensor_train])\n",
        "max_length_tar = max([len(t) for t in target_tensor_train])\n",
        "print(max_length_inp,max_length_tar)"
      ],
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1275 42\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3stKqVCixcM"
      },
      "source": [
        "def pad_sequences(x, max_len):\n",
        "  padded_x = np.zeros((max_len), dtype=np.int64)\n",
        "  if len(x) > max_len:\n",
        "    padded_x = x[:max_len]\n",
        "  else:\n",
        "    padded_x[:len(x)] = x\n",
        "  return padded_x"
      ],
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8ZSVEkLi_LY"
      },
      "source": [
        "# padding zeros at the end of train tensors\n",
        "input_tensor_train = np.array([pad_sequences(x, max_length_inp) for x in input_tensor_train if len(x) > 0])\n",
        "target_tensor_train = np.array([pad_sequences(x, max_length_tar) for x in target_tensor_train if len(x) > 0])"
      ],
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "id5qzfHgRK5-"
      },
      "source": [
        "# padding zeros at the end of validation tensors\n",
        "input_tensor_val = np.array([pad_sequences(x, max_length_inp) for x in input_tensor_val if len(x) > 0])\n",
        "target_tensor_val = np.array([pad_sequences(x, max_length_tar) for x in target_tensor_val if len(x) > 0])"
      ],
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqrLta8JSy1Q"
      },
      "source": [
        "# padding zeros at the end of test tensors\n",
        "input_tensor_test= np.array([pad_sequences(x, max_length_inp) for x in input_tensor_test if len(x) > 0])\n",
        "target_tensor_test = np.array([pad_sequences(x, max_length_tar) for x in target_tensor_test if len(x) > 0])"
      ],
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-mrrZCsnUW7"
      },
      "source": [
        "### Import Pytorch Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScVOBd6Pm_qI",
        "outputId": "e43e2b70-ce00-40fd-a326-b02138b78b4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import torch\n",
        "import torch.functional as F\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import unicodedata\n",
        "import re\n",
        "import time\n",
        "\n",
        "print(torch.__version__)\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.6.0+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owXXuSbfmhzl"
      },
      "source": [
        "#### DataLoader for batch inputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtWFJnT2nRPN"
      },
      "source": [
        "# conver the data to tensors and pass to the Dataloader \n",
        "# to create an batch iterator\n",
        "\n",
        "class MyData(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.data = X\n",
        "        self.target = y\n",
        "        \n",
        "        self.length = [ np.sum(1 - np.equal(x, 0)) for x in X]\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        x = self.data[index]\n",
        "        y = self.target[index]\n",
        "        x_len = self.length[index]\n",
        "        return x,y,x_len\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twH7dF5_njz9"
      },
      "source": [
        "## Parameters\n",
        "Let's define the hyperparameters and other things we need for training our NMT model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i50tXF4nngbx"
      },
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "N_BATCH = BUFFER_SIZE//BATCH_SIZE\n",
        "embedding_dim = 64\n",
        "units = 128\n",
        "vocab_inp_size = len(post_utils.word2idx)\n",
        "vocab_tar_size = len(hashtag_utils.word2idx)\n",
        "\n",
        "train_dataset = MyData(input_tensor_train, target_tensor_train)\n",
        "val_dataset = MyData(input_tensor_val, target_tensor_val)\n",
        "\n",
        "dataset = DataLoader(train_dataset, batch_size = BATCH_SIZE, \n",
        "                     drop_last=True,\n",
        "                     shuffle=True)"
      ],
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KM_FKrUSo_DG"
      },
      "source": [
        "### Encoder Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYp4j_Sunx1O"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.enc_units = enc_units\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
        "        self.gru = nn.GRU(self.embedding_dim, self.enc_units)\n",
        "        \n",
        "    def forward(self, x, lens, device):\n",
        "        # x: batch_size, max_length \n",
        "        \n",
        "        # x: batch_size, max_length, embedding_dim\n",
        "        x = self.embedding(x) \n",
        "                \n",
        "        # x transformed = max_len X batch_size X embedding_dim\n",
        "        # x = x.permute(1,0,2)\n",
        "        x = pack_padded_sequence(x, lens) # unpad\n",
        "    \n",
        "        self.hidden = self.initialize_hidden_state(device)\n",
        "        \n",
        "        # output: max_length, batch_size, enc_units\n",
        "        # self.hidden: 1, batch_size, enc_units\n",
        "        output, self.hidden = self.gru(x, self.hidden) # gru returns hidden state of all timesteps as well as hidden state at last timestep\n",
        "        \n",
        "        # pad the sequence to the max length in the batch\n",
        "        output, _ = pad_packed_sequence(output)\n",
        "        \n",
        "        return output, self.hidden\n",
        "\n",
        "    def initialize_hidden_state(self, device):\n",
        "        return torch.zeros((1, self.batch_sz, self.enc_units)).to(device)"
      ],
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8t6bC5zbn9iv"
      },
      "source": [
        "### sort batch function to be able to use with pad_packed_sequence\n",
        "def sort_batch(X, y, lengths):\n",
        "    lengths, indx = lengths.sort(dim=0, descending=True)\n",
        "    X = X[indx]\n",
        "    y = y[indx]\n",
        "    return X.transpose(0,1), y, lengths # transpose (batch x seq) to (seq x batch)"
      ],
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8p7zBlOoBZj"
      },
      "source": [
        "### Testing the Encoder\n",
        "Before proceeding with training, we should always try to test out model behavior such as the size of outputs just to make that things are going as expected. In PyTorch this can be done easily since everything comes in eager execution by default."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4h1Ta83oDmo",
        "outputId": "15dcaf0c-5d58-4290-9a94-ccb9326597b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "### Testing Encoder part\n",
        "# TODO: put whether GPU is available or not\n",
        "# Device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "encoder.to(device)\n",
        "# obtain one sample from the data iterator\n",
        "it = iter(dataset)\n",
        "x, y, x_len = next(it)\n",
        "\n",
        "# sort the batch first to be able to use with pac_pack_sequence\n",
        "xs, ys, lens = sort_batch(x, y, x_len)\n",
        "\n",
        "enc_output, enc_hidden = encoder(xs.to(device), lens, device)\n",
        "\n",
        "print(enc_output.size()) # max_length, batch_size, enc_units"
      ],
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([510, 64, 128])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qf2QSZexpIZe"
      },
      "source": [
        "### Decoder Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvBHv9YQo6O9"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units, enc_units, batch_sz):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.dec_units = dec_units\n",
        "        self.enc_units = enc_units\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
        "        self.gru = nn.GRU(self.embedding_dim + self.enc_units, \n",
        "                          self.dec_units,\n",
        "                          batch_first=True)\n",
        "        self.fc = nn.Linear(self.enc_units, self.vocab_size)\n",
        "        \n",
        "        # used for attention\n",
        "        self.W1 = nn.Linear(self.enc_units, self.dec_units)\n",
        "        self.W2 = nn.Linear(self.enc_units, self.dec_units)\n",
        "        self.V = nn.Linear(self.enc_units, 1)\n",
        "    \n",
        "    def forward(self, x, hidden, enc_output):\n",
        "        # enc_output original: (max_length, batch_size, enc_units)\n",
        "        # enc_output converted == (batch_size, max_length, hidden_size)\n",
        "        enc_output = enc_output.permute(1,0,2)\n",
        "        # hidden shape == (batch_size, hidden size)\n",
        "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "        # we are doing this to perform addition to calculate the score\n",
        "        \n",
        "        # hidden shape == (batch_size, hidden size)\n",
        "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "        hidden_with_time_axis = hidden.permute(1, 0, 2)\n",
        "        \n",
        "        # score: (batch_size, max_length, hidden_size) # Bahdanaus's\n",
        "        # we get 1 at the last axis because we are applying tanh(FC(EO) + FC(H)) to self.V\n",
        "        # It doesn't matter which FC we pick for each of the inputs\n",
        "        score = torch.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis))\n",
        "        \n",
        "        #score = torch.tanh(self.W2(hidden_with_time_axis) + self.W1(enc_output))\n",
        "          \n",
        "        # attention_weights shape == (batch_size, max_length, 1)\n",
        "        # we get 1 at the last axis because we are applying score to self.V\n",
        "        attention_weights = torch.softmax(self.V(score), dim=1)\n",
        "        \n",
        "        # context_vector shape after sum == (batch_size, hidden_size)\n",
        "        context_vector = attention_weights * enc_output\n",
        "        context_vector = torch.sum(context_vector, dim=1)\n",
        "        \n",
        "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "        # takes case of the right portion of the model above (illustrated in red)\n",
        "        x = self.embedding(x)\n",
        "        \n",
        "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "        #x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "        # ? Looks like attention vector in diagram of source\n",
        "        x = torch.cat((context_vector.unsqueeze(1), x), -1)\n",
        "        \n",
        "        # passing the concatenated vector to the GRU\n",
        "        # output: (batch_size, 1, hidden_size)\n",
        "        output, state = self.gru(x)\n",
        "        \n",
        "        \n",
        "        # output shape == (batch_size * 1, hidden_size)\n",
        "        output =  output.view(-1, output.size(2))\n",
        "        \n",
        "        # output shape == (batch_size * 1, vocab)\n",
        "        x = self.fc(output)\n",
        "        \n",
        "        return x, state, attention_weights\n",
        "    \n",
        "    def initialize_hidden_state(self):\n",
        "        return torch.zeros((1, self.batch_sz, self.dec_units))"
      ],
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WQd4BMwpSl5"
      },
      "source": [
        "#### Testing the Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKgPyYfHpOK1",
        "outputId": "de4f2229-716c-4808-ae92-975fbbb41db0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "# Device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "encoder.to(device)\n",
        "# obtain one sample from the data iterator\n",
        "it = iter(dataset)\n",
        "x, y, x_len = next(it)\n",
        "\n",
        "print(\"Input: \", x.shape)\n",
        "print(\"Output: \", y.shape)\n",
        "\n",
        "# sort the batch first to be able to use with pac_pack_sequence\n",
        "xs, ys, lens = sort_batch(x, y, x_len)\n",
        "\n",
        "enc_output, enc_hidden = encoder(xs.to(device), lens, device)\n",
        "print(\"Encoder Output: \", enc_output.shape) # batch_size X max_length X enc_units\n",
        "print(\"Encoder Hidden: \", enc_hidden.shape) # batch_size X enc_units (corresponds to the last state)\n",
        "\n",
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, units, BATCH_SIZE)\n",
        "decoder = decoder.to(device)\n",
        "\n",
        "#print(enc_hidden.squeeze(0).shape)\n",
        "\n",
        "dec_hidden = enc_hidden#.squeeze(0)\n",
        "dec_input = torch.tensor([[hashtag_utils.word2idx['$start']]] * BATCH_SIZE)\n",
        "print(\"Decoder Input: \", dec_input.shape)\n",
        "print(\"--------\")\n",
        "\n",
        "for t in range(1, y.size(1)):\n",
        "    # enc_hidden: 1, batch_size, enc_units\n",
        "    # output: max_length, batch_size, enc_units\n",
        "    predictions, dec_hidden, _ = decoder(dec_input.to(device), \n",
        "                                         dec_hidden.to(device), \n",
        "                                         enc_output.to(device))\n",
        "    \n",
        "    print(\"Prediction: \", predictions.shape)\n",
        "    print(\"Decoder Hidden: \", dec_hidden.shape)\n",
        "    \n",
        "    #loss += loss_function(y[:, t].to(device), predictions.to(device))\n",
        "    \n",
        "    dec_input = y[:, t].unsqueeze(1)\n",
        "    print(dec_input.shape)\n",
        "    break"
      ],
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input:  torch.Size([64, 1275])\n",
            "Output:  torch.Size([64, 42])\n",
            "Encoder Output:  torch.Size([775, 64, 128])\n",
            "Encoder Hidden:  torch.Size([1, 64, 128])\n",
            "Decoder Input:  torch.Size([64, 1])\n",
            "--------\n",
            "Prediction:  torch.Size([64, 1697])\n",
            "Decoder Hidden:  torch.Size([1, 64, 128])\n",
            "torch.Size([64, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zOxoFPRUa59"
      },
      "source": [
        "## Training Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9VMQwwwqlVE"
      },
      "source": [
        "def train(epochs,encoder,decoder,device,ytrain_utils,\n",
        "          loss_function,batch_size,optimizer,dataLoader):\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "      start = time.time()\n",
        "      total_loss = 0\n",
        "\n",
        "      encoder.train()\n",
        "      decoder.train()\n",
        "      \n",
        "      for (batch, (inp, targ, inp_len)) in enumerate(dataLoader):\n",
        "          \n",
        "          loss = 0\n",
        "\n",
        "          xs, ys, lens = sort_batch(inp, targ, inp_len)\n",
        "          \n",
        "          enc_output, enc_hidden = encoder(xs.to(device), lens, device)\n",
        "          dec_hidden = enc_hidden\n",
        "          \n",
        "          # use teacher forcing - feeding the target as the next input (via dec_input)\n",
        "          dec_input = torch.tensor([[ytrain_utils.word2idx['$start']]] * batch_size)\n",
        "          \n",
        "          # run code below for every timestep in the ys batch\n",
        "          for t in range(1, ys.size(1)):\n",
        "              predictions, dec_hidden, _ = decoder(dec_input.to(device), \n",
        "                                          dec_hidden.to(device), \n",
        "                                          enc_output.to(device))\n",
        "              loss += loss_function(ys[:, t].to(device), predictions.to(device))\n",
        "              #loss += loss_\n",
        "              dec_input = ys[:, t].unsqueeze(1)\n",
        "              \n",
        "          \n",
        "          batch_loss = (loss / int(ys.size(1)))\n",
        "          total_loss += batch_loss\n",
        "          \n",
        "          optimizer.zero_grad()\n",
        "          \n",
        "          loss.backward()\n",
        "\n",
        "          optimizer.step()\n",
        "          \n",
        "          '''if batch % 100 == 0:\n",
        "              print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                          batch,\n",
        "                                                          batch_loss.detach().item()))'''\n",
        "          \n",
        "          \n",
        "      ### TODO: Save checkpoint for model\n",
        "      print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                          total_loss / N_BATCH))\n",
        "      print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
        "    "
      ],
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZKIRwE_qWFw"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    \"\"\" Only consider non-zero inputs in the loss; mask needed \"\"\"\n",
        "    #mask = 1 - np.equal(real, 0) # assign 0 to all above 0 and 1 to all 0s\n",
        "    #print(mask)\n",
        "    mask = real.ge(1).type(torch.cuda.FloatTensor)\n",
        "    \n",
        "    loss_ = criterion(pred, real) * mask \n",
        "    return torch.mean(loss_)"
      ],
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQf6n3mCqg-1"
      },
      "source": [
        "\n",
        "BATCH_SIZE = 64\n",
        "## TODO: Combine the encoder and decoder into one class\n",
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, units, BATCH_SIZE)\n",
        "\n",
        "encoder.to(device)\n",
        "decoder.to(device)\n",
        "\n",
        "optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), \n",
        "                       lr=0.001)"
      ],
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5Ijsl1sWd4p",
        "outputId": "0463176d-4c4b-424d-8a5b-56a6f23473c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "epochs = 50\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "train(epochs,encoder,decoder,device,hashtag_utils,\n",
        "      loss_function,BATCH_SIZE,optimizer,dataset)"
      ],
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Loss 0.5687\n",
            "Time taken for 1 epoch 72.29287242889404 sec\n",
            "\n",
            "Epoch 2 Loss 0.4400\n",
            "Time taken for 1 epoch 72.55029320716858 sec\n",
            "\n",
            "Epoch 3 Loss 0.3965\n",
            "Time taken for 1 epoch 72.8018729686737 sec\n",
            "\n",
            "Epoch 4 Loss 0.3660\n",
            "Time taken for 1 epoch 73.026859998703 sec\n",
            "\n",
            "Epoch 5 Loss 0.3426\n",
            "Time taken for 1 epoch 72.92470335960388 sec\n",
            "\n",
            "Epoch 6 Loss 0.3248\n",
            "Time taken for 1 epoch 72.8529109954834 sec\n",
            "\n",
            "Epoch 7 Loss 0.3104\n",
            "Time taken for 1 epoch 73.24533677101135 sec\n",
            "\n",
            "Epoch 8 Loss 0.2971\n",
            "Time taken for 1 epoch 73.49484252929688 sec\n",
            "\n",
            "Epoch 9 Loss 0.2851\n",
            "Time taken for 1 epoch 73.4104552268982 sec\n",
            "\n",
            "Epoch 10 Loss 0.2744\n",
            "Time taken for 1 epoch 72.5112555027008 sec\n",
            "\n",
            "Epoch 11 Loss 0.2631\n",
            "Time taken for 1 epoch 72.75166869163513 sec\n",
            "\n",
            "Epoch 12 Loss 0.2531\n",
            "Time taken for 1 epoch 72.49494504928589 sec\n",
            "\n",
            "Epoch 13 Loss 0.2430\n",
            "Time taken for 1 epoch 73.1577959060669 sec\n",
            "\n",
            "Epoch 14 Loss 0.2331\n",
            "Time taken for 1 epoch 72.13152861595154 sec\n",
            "\n",
            "Epoch 15 Loss 0.2236\n",
            "Time taken for 1 epoch 73.03208780288696 sec\n",
            "\n",
            "Epoch 16 Loss 0.2144\n",
            "Time taken for 1 epoch 72.54461741447449 sec\n",
            "\n",
            "Epoch 17 Loss 0.2051\n",
            "Time taken for 1 epoch 72.64089274406433 sec\n",
            "\n",
            "Epoch 18 Loss 0.1964\n",
            "Time taken for 1 epoch 72.83820414543152 sec\n",
            "\n",
            "Epoch 19 Loss 0.1879\n",
            "Time taken for 1 epoch 73.60470533370972 sec\n",
            "\n",
            "Epoch 20 Loss 0.1801\n",
            "Time taken for 1 epoch 72.6729576587677 sec\n",
            "\n",
            "Epoch 21 Loss 0.1719\n",
            "Time taken for 1 epoch 72.73363590240479 sec\n",
            "\n",
            "Epoch 22 Loss 0.1652\n",
            "Time taken for 1 epoch 72.86533308029175 sec\n",
            "\n",
            "Epoch 23 Loss 0.1583\n",
            "Time taken for 1 epoch 72.52253437042236 sec\n",
            "\n",
            "Epoch 24 Loss 0.1508\n",
            "Time taken for 1 epoch 73.15725803375244 sec\n",
            "\n",
            "Epoch 25 Loss 0.1443\n",
            "Time taken for 1 epoch 72.34178590774536 sec\n",
            "\n",
            "Epoch 26 Loss 0.1378\n",
            "Time taken for 1 epoch 71.92796874046326 sec\n",
            "\n",
            "Epoch 27 Loss 0.1328\n",
            "Time taken for 1 epoch 72.34561085700989 sec\n",
            "\n",
            "Epoch 28 Loss 0.1266\n",
            "Time taken for 1 epoch 73.37945628166199 sec\n",
            "\n",
            "Epoch 29 Loss 0.1204\n",
            "Time taken for 1 epoch 71.99559450149536 sec\n",
            "\n",
            "Epoch 30 Loss 0.1154\n",
            "Time taken for 1 epoch 71.82488489151001 sec\n",
            "\n",
            "Epoch 31 Loss 0.1106\n",
            "Time taken for 1 epoch 73.60640811920166 sec\n",
            "\n",
            "Epoch 32 Loss 0.1063\n",
            "Time taken for 1 epoch 72.42158937454224 sec\n",
            "\n",
            "Epoch 33 Loss 0.1003\n",
            "Time taken for 1 epoch 72.82510375976562 sec\n",
            "\n",
            "Epoch 34 Loss 0.0961\n",
            "Time taken for 1 epoch 72.33767533302307 sec\n",
            "\n",
            "Epoch 35 Loss 0.0919\n",
            "Time taken for 1 epoch 73.11254382133484 sec\n",
            "\n",
            "Epoch 36 Loss 0.0878\n",
            "Time taken for 1 epoch 72.34162878990173 sec\n",
            "\n",
            "Epoch 37 Loss 0.0839\n",
            "Time taken for 1 epoch 72.6276228427887 sec\n",
            "\n",
            "Epoch 38 Loss 0.0802\n",
            "Time taken for 1 epoch 72.52525496482849 sec\n",
            "\n",
            "Epoch 39 Loss 0.0831\n",
            "Time taken for 1 epoch 71.88051676750183 sec\n",
            "\n",
            "Epoch 40 Loss 0.0774\n",
            "Time taken for 1 epoch 72.26676869392395 sec\n",
            "\n",
            "Epoch 41 Loss 0.0706\n",
            "Time taken for 1 epoch 72.21899700164795 sec\n",
            "\n",
            "Epoch 42 Loss 0.0667\n",
            "Time taken for 1 epoch 72.4083034992218 sec\n",
            "\n",
            "Epoch 43 Loss 0.0669\n",
            "Time taken for 1 epoch 72.5995101928711 sec\n",
            "\n",
            "Epoch 44 Loss 0.0621\n",
            "Time taken for 1 epoch 72.03959894180298 sec\n",
            "\n",
            "Epoch 45 Loss 0.0583\n",
            "Time taken for 1 epoch 72.93598055839539 sec\n",
            "\n",
            "Epoch 46 Loss 0.0558\n",
            "Time taken for 1 epoch 73.18213391304016 sec\n",
            "\n",
            "Epoch 47 Loss 0.0540\n",
            "Time taken for 1 epoch 72.75243473052979 sec\n",
            "\n",
            "Epoch 48 Loss 0.0532\n",
            "Time taken for 1 epoch 72.24504590034485 sec\n",
            "\n",
            "Epoch 49 Loss 0.0516\n",
            "Time taken for 1 epoch 72.85075759887695 sec\n",
            "\n",
            "Epoch 50 Loss 0.0497\n",
            "Time taken for 1 epoch 73.14844870567322 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Im8ABO4vBtZo"
      },
      "source": [
        "### Predict hashtags"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgnxaaF7nJbI",
        "outputId": "cb74218f-f7b7-4ec2-e6e7-18fe63cafbe2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "encoder_pred = Encoder(vocab_inp_size, embedding_dim, units, 1)\n",
        "decoder_pred = Decoder(vocab_tar_size, embedding_dim, units, units,1)\n",
        "\n",
        "encoder_pred.to(device)\n",
        "decoder_pred.to(device)\n"
      ],
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Decoder(\n",
              "  (embedding): Embedding(1697, 64)\n",
              "  (gru): GRU(192, 128, batch_first=True)\n",
              "  (fc): Linear(in_features=128, out_features=1697, bias=True)\n",
              "  (W1): Linear(in_features=128, out_features=128, bias=True)\n",
              "  (W2): Linear(in_features=128, out_features=128, bias=True)\n",
              "  (V): Linear(in_features=128, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 181
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_nGxVj0B0ah"
      },
      "source": [
        "#### Loading weights into another model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uT0NV35qZps",
        "outputId": "fac139f4-705b-4368-8b41-f6d9bfe154cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "encoder_pred.load_state_dict(encoder.state_dict())\n",
        "decoder_pred.load_state_dict(decoder.state_dict())"
      ],
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 182
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O98VDtNptfOB"
      },
      "source": [
        "softmax = nn.Softmax(dim=1)\n",
        "\n",
        "def predict(X_test):\n",
        "  results = []\n",
        "  encoder_pred.eval()\n",
        "  decoder_pred.eval()\n",
        "\n",
        "  for xs in enumerate(X_test):\n",
        "    xs = xs[1]\n",
        "    length = np.sum(1 - np.equal(xs, 0))\n",
        "    xs = torch.tensor(xs).view(-1,1)\n",
        "    length = torch.tensor(length).view(-1,)\n",
        "\n",
        "    enc_output, enc_hidden = encoder_pred(xs.to(device), length, device)\n",
        "    dec_hidden = enc_hidden\n",
        "        \n",
        "    dec_input = torch.tensor([[hashtag_utils.word2idx['$start']]])\n",
        "    curr_hashtags = []\n",
        "    # run code below till we generate \"end$\" tag or 10 hashtags\n",
        "    for t in range(1,10):\n",
        "      predictions, dec_hidden, _ = decoder_pred(dec_input.to(device), \n",
        "                                          dec_hidden.to(device), \n",
        "                                          enc_output.to(device))\n",
        "      #print(predictions.size())\n",
        "      top_val,top_idx  = softmax(predictions).topk(1,dim = 1)\n",
        "      #print(top_idx.item())\n",
        "      if top_idx == hashtag_utils.word2idx[\"end$\"]:\n",
        "        break\n",
        "      pred_hashtag = hashtag_utils.idx2word[top_idx.item()]\n",
        "      curr_hashtags.append(pred_hashtag)\n",
        "      dec_input = torch.tensor([top_idx]).unsqueeze(1)\n",
        "\n",
        "    results.append(curr_hashtags)\n",
        "\n",
        "  return results"
      ],
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDJGPUYGB9zb"
      },
      "source": [
        "#predict hashtags  on test data\n",
        "predicted_hashtags = predict(input_tensor_test)"
      ],
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijat3BwSdErZ",
        "outputId": "2f9d9f60-58fc-4617-9f86-fef7200e372d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        }
      },
      "source": [
        "for i in range(10):\n",
        "  print(Y_test.iloc[i],predicted_hashtags[i])"
      ],
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "$start art illustration racism history end$ ['mythbusters', 'starwars']\n",
            "$start naturepbs end$ ['metoo', 'timesup']\n",
            "$start folkmusic pop experimentalmusic music indianmusic gujarat end$ ['commonwealth', 'commonwealthsecretariat', 'connectedcommonwealth', 'ourcommonwealth', 'chogm2018', 'chogm18', 'chogm', 'chogm2018', 'commonwealth']\n",
            "$start politics mueller muellernews end$ ['science']\n",
            "$start atalbiharivajpayee narendramodi merrychristmas christmas modi india photography end$ ['merrychristmas', 'happyholidays', 'selfie', 'photooftheday', 'science', 'sciencechannel']\n",
            "$start tbt wlaspen wlsnowmass end$ ['cover', 'talent', 'voice', 'vocals', 'wow', 'singing', 'singer', 'karaoke', 'song']\n",
            "$start notredame notredameparis notredamecathedral fire fires flame blaze spire burning tragedy firefighter firefighters firemen firefighterlife cathedral cathedrale monument monuments landmark church history paris parisfrance france french france français end$ ['fire', 'tragedy', 'church', 'learning', 'didyouknow', 'themoreyouknow', 'wow', 'videooftheday', 'human']\n",
            "$start witchhunting discrimination marginalized communities lynching assam end$ ['ireland', 'abortion', 'abortionrights', 'women', 'woman', 'women', 'girl', 'girls', 'feminism']\n",
            "$start oxford university allsouls college archway lawn oxforduniversity end$ ['architecture', 'history', 'architecture', 'oxford', 'oxforduniversity']\n",
            "$start smoothie chocolate cookies banana baking strawberry fruit dairyfree plantbased diet healthyeating nutrition weightloss glutenfree mealprep brownies dessert veganfood protein recipe end$ ['deats', 'mealprep', 'recipes', 'recipe', 'wholefoods', 'nutrition', 'calories', 'healthyfood', 'recipe']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}