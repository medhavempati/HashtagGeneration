{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KeyPhraseGenModelWithLikes.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fNJL7sis9ir"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import string\n",
        "import re\n",
        "from collections import defaultdict"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThENIJNGuyFT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b58bb3d-016d-49c9-fa2f-dd9fc4a862d7"
      },
      "source": [
        "import glob\n",
        "\n",
        "path = r'/content/drive/My Drive/Instagram_Data' \n",
        "fileNames = glob.glob(path + \"/*.csv\")\n",
        "\n",
        "lst = []\n",
        "\n",
        "for fileName in fileNames:\n",
        "    print(fileName)\n",
        "    df = pd.read_csv(fileName, index_col=None, header=0)\n",
        "    lst.append(df)\n",
        "\n",
        "df = pd.concat(lst, axis=0, ignore_index=True)"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Instagram_Data/mitpics.csv\n",
            "/content/drive/My Drive/Instagram_Data/yoga.csv\n",
            "/content/drive/My Drive/Instagram_Data/world.csv\n",
            "/content/drive/My Drive/Instagram_Data/viceindia.csv\n",
            "/content/drive/My Drive/Instagram_Data/abcwnn.csv\n",
            "/content/drive/My Drive/Instagram_Data/aamaadmiparty.csv\n",
            "/content/drive/My Drive/Instagram_Data/talentmoves.csv\n",
            "/content/drive/My Drive/Instagram_Data/ageofempires.csv\n",
            "/content/drive/My Drive/Instagram_Data/ageofempiresgame.csv\n",
            "/content/drive/My Drive/Instagram_Data/voxdotcom.csv\n",
            "/content/drive/My Drive/Instagram_Data/oxford_uni.csv\n",
            "/content/drive/My Drive/Instagram_Data/nature_africa.csv\n",
            "/content/drive/My Drive/Instagram_Data/life.csv\n",
            "/content/drive/My Drive/Instagram_Data/pbsnature.csv\n",
            "/content/drive/My Drive/Instagram_Data/healthyfoodvideos.csv\n",
            "/content/drive/My Drive/Instagram_Data/nature.research.csv\n",
            "/content/drive/My Drive/Instagram_Data/eucouncil.csv\n",
            "/content/drive/My Drive/Instagram_Data/hidden_shots_.csv\n",
            "/content/drive/My Drive/Instagram_Data/educationaboutearth.csv\n",
            "/content/drive/My Drive/Instagram_Data/afpsport.csv\n",
            "/content/drive/My Drive/Instagram_Data/nytimesopinionart.csv\n",
            "/content/drive/My Drive/Instagram_Data/scroll_in.csv\n",
            "/content/drive/My Drive/Instagram_Data/commonwealth_sec.csv\n",
            "/content/drive/My Drive/Instagram_Data/ptiphotos.csv\n",
            "/content/drive/My Drive/Instagram_Data/wanderlustfest.csv\n",
            "/content/drive/My Drive/Instagram_Data/sciencechannel.csv\n",
            "/content/drive/My Drive/Instagram_Data/healthy.foodyss.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pa-EMMf1yhUb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "64cd4de0-9d22-4b16-ae46-2c56be5d9126"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>ID</th>\n",
              "      <th>shortcode</th>\n",
              "      <th>text</th>\n",
              "      <th>hashtags</th>\n",
              "      <th>comments</th>\n",
              "      <th>likes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>👏 One of MIT's newest Nobel laureates Esther D...</td>\n",
              "      <td>mit womeninscience womeninstem nobelprize nobe...</td>\n",
              "      <td>|Congratulations!!🙌🙌🙌|What was the award for?|...</td>\n",
              "      <td>3785.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Congratulations to MIT’s Abhijit Banerjee and ...</td>\n",
              "      <td>mit nobelprize nobellaureate economicsciences ...</td>\n",
              "      <td>|Congratulations!|congratulations🎉🎉🎉🎉|@jarad_p...</td>\n",
              "      <td>4645.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Last Friday mechanical engineering seniors tak...</td>\n",
              "      <td>thisismit mit productengineering teambuilding ...</td>\n",
              "      <td>|🔥🔥🔥🔥🔥🔥🔥🔥|Is \"cracker causeway\" kind of a give...</td>\n",
              "      <td>2919.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Nearly 10 years ago Priyanka Bakaya MBA ’11 fo...</td>\n",
              "      <td>mit sustainability entrepreneurs plasticintofu...</td>\n",
              "      <td>|enerjide çevreci adim atmak geleceğimizi güçl...</td>\n",
              "      <td>2860.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>At MIT.nano we think every day is Nano Day. Bu...</td>\n",
              "      <td>mit nanotechnology nanotecnologia nanoscience ...</td>\n",
              "      <td>|Smart dust|@takkksh|@photechno|@jarad_ponce|@...</td>\n",
              "      <td>5555.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Unnamed: 0  ID  ...                                           comments   likes\n",
              "0          1 NaN  ...  |Congratulations!!🙌🙌🙌|What was the award for?|...  3785.0\n",
              "1          2 NaN  ...  |Congratulations!|congratulations🎉🎉🎉🎉|@jarad_p...  4645.0\n",
              "2          3 NaN  ...  |🔥🔥🔥🔥🔥🔥🔥🔥|Is \"cracker causeway\" kind of a give...  2919.0\n",
              "3          4 NaN  ...  |enerjide çevreci adim atmak geleceğimizi güçl...  2860.0\n",
              "4          5 NaN  ...  |Smart dust|@takkksh|@photechno|@jarad_ponce|@...  5555.0\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-MHV0hiNhvZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "901b4613-e42f-4816-d4a6-67c8433d8f85"
      },
      "source": [
        "print(len(df))"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16253\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hG37E12xA4g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7e3da24-c10f-41f4-b851-9fb39e025831"
      },
      "source": [
        "print(df[\"text\"].iloc[1])"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Congratulations to MIT’s Abhijit Banerjee and Esther Duflo have won the Nobel Prize in economics “for their experimental approach to alleviating global poverty.\" They share the prize with Michael Kremer of Harvard.\n",
            "\n",
            "Reposted from @nobelprize_org (@get_regrann) \n",
            "The research conducted by this year’s Laureates has considerably improved our ability to fight global poverty. In just two decades their new experiment-based approach has transformed development economics which is now a flourishing field of research. \n",
            "Despite recent dramatic improvements one of humanity’s most urgent issues is the reduction of global poverty in all its forms. More than 700 million people still subsist on extremely low incomes. Every year around five million children under the age of five still die of diseases that could often have been prevented or cured with inexpensive treatments. Half of the world’s children still leave school without basic literacy and numeracy skills. \n",
            "This year’s Laureates have introduced a new approach to obtaining reliable answers about the best ways to fight global poverty. In brief it involves dividing this issue into smaller more manageable questions – for example the most effective interventions for improving educational outcomes or child health. They have shown that these smaller more precise questions are often best answered via carefully designed experiments among the people who are most affected. \n",
            "The 2019 Economic Sciences Laureates have played a decisive role in reshaping research in development economics. Over just 20 years the subject has become a flourishing primarily experimental area of mainstream economics. This new experiment-based research has already helped in alleviating global poverty and has great potential to further improve the lives of the most impoverished people on the planet.  See link in profile for news article.\n",
            "\n",
            "#mit #NobelPrize #NobelLaureate #EconomicSciences #economics #poverty #endpoverty #research #experiment #science #society #prize #award #NobelPrize2019 #NobelPrizeannouncement #economicsprize #globalpoverty #education #health #healthcare #childhealth #development  #regrann\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2b4CClHBzjvP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1589ec03-008d-48a3-ca28-b6f63b29674f"
      },
      "source": [
        "print(df[\"hashtags\"].iloc[1])"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mit nobelprize nobellaureate economicsciences economics poverty endpoverty research experiment science society prize award nobelprize2019 nobelprizeannouncement economicsprize globalpoverty education health healthcare childhealth development regrann\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEKRcv3y0P29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52befb7c-3802-4d8b-994f-78c197c2cbcd"
      },
      "source": [
        "print(df[\"comments\"].iloc[1])"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "|Congratulations!|congratulations🎉🎉🎉🎉|@jarad_ponce|Congratulations|😍😍😍🔥🔥|Congratulations 🎊 🎊 🎊 🎊|🇮🇳|Great!! Congratulations to the winners and to the MIT|Congratulations from South Africa👏|The reason for I love MIT 🔥🔥🔥 🇮🇳💪💗|Congratulations! 🙏|🎉🙏😁|🙏🏻🙌🏼|There is no such thing as a nobel prize in economics|@e.lauer|Congratulations from India|MIT MITMITMIT MIT ❤️❤️❤️❤️|शुभकामनाएं|❤️|Amazing!!! Congratulations 😍|Many congratulations..|👏👏👏👏👏👏👏👏|❤️|excellent🙌🙌🙌|Congrats 👏👏👏|@cuartosectoreco|Enhorabuena!!|Really glad to have such eminent personal in MIT N now I know why MIT is best university 😀|Congratulations!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRHggXkIFFgC"
      },
      "source": [
        "## Data Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjG_ei2GLUOc"
      },
      "source": [
        "### Cleaning Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7WsJPPw8D1N"
      },
      "source": [
        "#https://stackoverflow.com/questions/33404752/removing-emojis-from-a-string-in-python\n",
        "def deEmojify(text):\n",
        "    regrex_pattern = re.compile(pattern = \"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           \"]+\", flags = re.UNICODE)\n",
        "    return regrex_pattern.sub(r'',str(text))"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEdQeQ7_GFvC"
      },
      "source": [
        "# appending first 25 comments into one big comment\n",
        "def appendComments(comment):\n",
        "  lst = comment.split(\"|\")\n",
        "  final_comment = \"\"\n",
        "  for idx,entry in enumerate(lst):\n",
        "    if idx == 25:\n",
        "      break\n",
        "    entry.strip()\n",
        "    final_comment +=  entry + \" \"\n",
        "  return final_comment"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCzTCqbOGJCf"
      },
      "source": [
        "def cleanComment(comment):\n",
        "  deEmojified_comment = deEmojify(comment)\n",
        "  return appendComments(deEmojified_comment)"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXLNIwty-Os6"
      },
      "source": [
        "df[\"comments\"] = df[\"comments\"].apply(lambda x:cleanComment(x))"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGKlZTx6DucL"
      },
      "source": [
        "#combining comments and actual post\n",
        "df[\"text_with_comments\"] = df[\"text\"]+ \" \"+ df[\"comments\"]"
      ],
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhvWwy2O0pOK"
      },
      "source": [
        "# removing punctuations,and converting upper to lower case letters\n",
        "\n",
        "punctuations = string.punctuation\n",
        "table = punctuations.maketrans(punctuations+string.ascii_uppercase,\n",
        "                               \" \"*len(punctuations)+string.ascii_lowercase,)\n",
        "\n",
        "def cleanPosts(sentence):\n",
        "  sentence.strip()\n",
        "  sen = sentence.translate(table)\n",
        "  return sen\n",
        "\n",
        "def cleanHashTags(sentence):\n",
        "  sentence.strip()\n",
        "  sen = sentence.translate(table)\n",
        "  return \"$start \" + sen + \" end$\"\n"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71PRmWo96ZAm"
      },
      "source": [
        "post_df = df[\"text_with_comments\"].apply(lambda w:cleanPosts(str(w)))\n",
        "hashtag_df = df[\"hashtags\"].apply(lambda w: cleanHashTags(str(w)))"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9wiB2s_NMzx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa986357-d906-494a-ae66-5fe96924d27e"
      },
      "source": [
        "print(hashtag_df.iloc[1])"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "$start mit nobelprize nobellaureate economicsciences economics poverty endpoverty research experiment science society prize award nobelprize2019 nobelprizeannouncement economicsprize globalpoverty education health healthcare childhealth development regrann end$\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRMHCl1fHAjh"
      },
      "source": [
        "#### Hashtags and Likes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1_bDswAHIbw"
      },
      "source": [
        "# here we only consider hashtags that are appearing atleast 10 times\n",
        "def count_hashtag_likes(all_posts_hashtags,all_posts_likes):\n",
        "  hashtag_likesCount = defaultdict(int)\n",
        "  hashtagAppearanceCount = defaultdict(int)\n",
        "  hashtag_first10_likesCount = defaultdict(int)\n",
        "  for hashtags,count in zip(all_posts_hashtags,all_posts_likes):\n",
        "    hashtags = str(hashtags).split()\n",
        "    for hashtag in hashtags:\n",
        "      hashtagAppearanceCount[hashtag] += 1\n",
        "      if hashtagAppearanceCount[hashtag] >= 10:\n",
        "        if hashtag not in hashtag_likesCount:\n",
        "          hashtag_likesCount[hashtag] = hashtag_first10_likesCount[hashtag] + count\n",
        "        hashtag_likesCount[hashtag] += count\n",
        "        continue\n",
        "      hashtag_first10_likesCount[hashtag] += count      \n",
        "  return hashtag_likesCount      "
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTUWJXuLJAgH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69cc5ce5-d421-450e-f83f-dadb39f19c46"
      },
      "source": [
        "hashtag_likesCount = count_hashtag_likes(hashtag_df.values.tolist(),df[\"likes\"].values.tolist())\n",
        "print(len(hashtag_likesCount))"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2038\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IDYmG8FZLOc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5404b635-09a3-4261-db89-98eaf89fc138"
      },
      "source": [
        "# list of hashtags appearing atleast 10 times\n",
        "frequent_hashtags = hashtag_likesCount.keys()\n",
        "print(len(frequent_hashtags))"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2038\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h10EQKDrLass"
      },
      "source": [
        "#### Splitting Data to Train ,Test and Validation Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUs4l7YgjI57"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X, X_val, Y, Y_val = train_test_split(post_df,hashtag_df, test_size=0.10,shuffle = True)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.10,shuffle = True)"
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9ENbUmJPgTL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66e1367a-5e6f-4bbf-c634-547be33c5735"
      },
      "source": [
        "print(X_train.shape,Y_train.shape)\n",
        "print(X_val.shape,Y_val.shape)\n",
        "print(X_test.shape,Y_test.shape)"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(13164,) (13164,)\n",
            "(1626,) (1626,)\n",
            "(1463,) (1463,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0qgms1Vdoqx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8ee7b3f-59f4-4fad-b9d0-e0a6e18c51ea"
      },
      "source": [
        "type(X_test)"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pandas.core.series.Series"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3RRpI3YMO1R"
      },
      "source": [
        "### Creating vector representations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xjRi2tFLHXo"
      },
      "source": [
        "class ConstructIndexes():\n",
        "  def __init__(self, data):\n",
        "    \n",
        "    self.data = data\n",
        "    self.word2idx = {}\n",
        "    self.idx2word = {}\n",
        "    self.vocab = set()\n",
        "\n",
        "    self.create_index()\n",
        "        \n",
        "  def create_index(self):\n",
        "    # we consider words appearing atleas 10 times\n",
        "    wordAppearanceCount = defaultdict(int)\n",
        "    for phrase in self.data:\n",
        "      for word in phrase.split():\n",
        "        wordAppearanceCount[word] += 1\n",
        "        if wordAppearanceCount[word] >= 10:\n",
        "          self.vocab.add(word)\n",
        "            \n",
        "    # sort the vocab\n",
        "    self.vocab = sorted(self.vocab)\n",
        "\n",
        "    # add a padding token with index 0\n",
        "    self.word2idx['<pad>'] = 0\n",
        "        \n",
        "    # word to index mapping\n",
        "    for index, word in enumerate(self.vocab):\n",
        "        self.word2idx[word] = index + 1 # +1 because of pad token\n",
        "        \n",
        "    # index to word mapping\n",
        "    for word, index in self.word2idx.items():\n",
        "        self.idx2word[index] = word      "
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Rl4eVsQaiZa"
      },
      "source": [
        "# creating word2idx, idx2word on train data\n",
        "# we use utils constructed on train to convert test and validation data\n",
        "post_utils = ConstructIndexes(X_train.values.tolist())\n",
        "hashtag_utils = ConstructIndexes(Y_train.values.tolist())"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJecUoRmeKNN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c29f01b0-c815-481d-8ce7-4cd2f851de5f"
      },
      "source": [
        "print(len(post_utils.word2idx))\n",
        "print(len(hashtag_utils.word2idx))"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13638\n",
            "1716\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F35oC8UaM7BT"
      },
      "source": [
        "# creating numeric vector corresponding to each post+comment\n",
        "def toNumVec(constructIndexClass,df):\n",
        "  tensor = []\n",
        "  for phrase in df:\n",
        "    phrase_lst = []\n",
        "    for word in phrase.split():\n",
        "      if word in constructIndexClass.word2idx:\n",
        "        phrase_lst.append(constructIndexClass.word2idx[word])\n",
        "    tensor.append(phrase_lst)\n",
        "  return tensor"
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpqVZsoxNeGB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec13bc79-99bc-4570-d77a-ba3e7d28254c"
      },
      "source": [
        "# Converting X_train,Y_train text sentences into numeric vectors\n",
        "input_tensor_train = toNumVec(post_utils,X_train)\n",
        "target_tensor_train = toNumVec(hashtag_utils,Y_train)\n",
        "print(len(input_tensor_train),len(target_tensor_train))"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13164 13164\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnAj9KWfOFMy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df31855d-02ad-42ca-b096-b2d23038454d"
      },
      "source": [
        "# Converting X_val,Y_val text sentences into numeric vectors\n",
        "input_tensor_val = toNumVec(post_utils,X_val)\n",
        "target_tensor_val = toNumVec(hashtag_utils,Y_val)\n",
        "print(len(input_tensor_val),len(target_tensor_val))"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1626 1626\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uu1_vUF6ORob",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abc44d10-858a-45d6-a044-32546b7f2b2e"
      },
      "source": [
        "# Converting X_test,Y_test text sentences into numeric vectors\n",
        "input_tensor_test = toNumVec(post_utils,X_test)\n",
        "target_tensor_test = toNumVec(hashtag_utils,Y_test)\n",
        "print(len(input_tensor_test),len(target_tensor_test))"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1463 1463\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SAlBEbKaldc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b68027a9-aeb7-4ed9-b88b-0ebee29137af"
      },
      "source": [
        "print(input_tensor_train[1])\n",
        "print(target_tensor_train[1])"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[12660, 2546, 11717, 8961, 7338, 13406, 2025, 1926, 9662, 8444, 13449, 12660, 5323, 9745, 11717, 8441, 1909, 11553, 2371, 7004, 538, 12677, 11553, 1504, 6104, 12855, 9912, 13287, 7712, 9702, 3031, 11717, 12748, 5819, 8441, 797, 1962, 11539, 870, 5819, 11553, 12023, 7716, 4739, 8276, 8961, 7338, 1926, 7922, 10707, 797, 4004, 8926, 4628, 1513, 9999, 797, 7128, 525, 12322, 8350, 3032, 8441, 4739, 8125, 7838, 3630, 13425, 7838, 3630, 13425, 8012, 1373, 2490, 13406, 10691, 10164, 12635, 12660, 9494, 11615, 6104, 346, 12003, 1162, 4739, 6367, 12562, 12911, 12780, 4175, 713, 11553, 1297, 8214, 346, 9831, 5819, 8125, 8108, 346, 10682, 4534, 5819, 10519, 5733, 11553, 3051, 4578, 3725, 5699, 10125, 11569, 8276, 2338, 3140, 346, 2849, 8214, 12856, 577, 12635, 11548, 1353, 10899, 12786, 5699, 3575, 11333, 11610, 5699, 4110, 10125, 8961, 7338, 8420, 6848, 11615, 5203, 12911, 1979, 11333, 8985, 8788, 6848, 11615, 12911, 9469, 7141, 7337, 7580, 11553, 8837, 6142, 7838, 12671, 11717, 10254, 11553, 8837, 1838, 12635, 6135, 8982, 4628, 346, 3140, 5733, 5631, 11061, 5699, 6905, 8276, 8961, 7338, 797, 12496, 5344, 5229, 346, 4739, 12874]\n",
            "[1, 1257, 331, 1115, 471]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4ZfyvP7Qfkg"
      },
      "source": [
        "#### Padding Numeric Vectors to same size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCDWASfhfw49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f77cc310-13c3-48b8-a795-f5a1339a2e4a"
      },
      "source": [
        "max_length_inp = max([len(t) for t in input_tensor_train])\n",
        "max_length_tar = max([len(t) for t in target_tensor_train])\n",
        "print(max_length_inp,max_length_tar)"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1277 42\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3stKqVCixcM"
      },
      "source": [
        "def pad_sequences(x, max_len):\n",
        "  padded_x = np.zeros((max_len), dtype=np.int64)\n",
        "  if len(x) > max_len:\n",
        "    padded_x = x[:max_len]\n",
        "  else:\n",
        "    padded_x[:len(x)] = x\n",
        "  return padded_x"
      ],
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8ZSVEkLi_LY"
      },
      "source": [
        "# padding zeros at the end of train tensors\n",
        "input_tensor_train = np.array([pad_sequences(x, max_length_inp) for x in input_tensor_train if len(x) > 0])\n",
        "target_tensor_train = np.array([pad_sequences(x, max_length_tar) for x in target_tensor_train if len(x) > 0])"
      ],
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "id5qzfHgRK5-"
      },
      "source": [
        "# padding zeros at the end of validation tensors\n",
        "input_tensor_val = np.array([pad_sequences(x, max_length_inp) for x in input_tensor_val if len(x) > 0])\n",
        "target_tensor_val = np.array([pad_sequences(x, max_length_tar) for x in target_tensor_val if len(x) > 0])"
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqrLta8JSy1Q"
      },
      "source": [
        "# padding zeros at the end of test tensors\n",
        "input_tensor_test= np.array([pad_sequences(x, max_length_inp) for x in input_tensor_test if len(x) > 0])\n",
        "target_tensor_test = np.array([pad_sequences(x, max_length_tar) for x in target_tensor_test if len(x) > 0])"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-mrrZCsnUW7"
      },
      "source": [
        "### Import Pytorch Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScVOBd6Pm_qI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46b60285-5dae-47c8-f7ae-c2c82b72e3f9"
      },
      "source": [
        "import torch\n",
        "import torch.functional as F\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import unicodedata\n",
        "import re\n",
        "import time\n",
        "\n",
        "print(torch.__version__)\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.7.0+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owXXuSbfmhzl"
      },
      "source": [
        "#### DataLoader for batch inputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtWFJnT2nRPN"
      },
      "source": [
        "# conver the data to tensors and pass to the Dataloader \n",
        "# to create an batch iterator\n",
        "\n",
        "class MyData(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.data = X\n",
        "        self.target = y\n",
        "        \n",
        "        self.length = [ np.sum(1 - np.equal(x, 0)) for x in X]\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        x = self.data[index]\n",
        "        y = self.target[index]\n",
        "        x_len = self.length[index]\n",
        "        return x,y,x_len\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twH7dF5_njz9"
      },
      "source": [
        "## Parameters\n",
        "Let's define the hyperparameters and other things we need for training our NMT model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i50tXF4nngbx"
      },
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "N_BATCH = BUFFER_SIZE//BATCH_SIZE\n",
        "embedding_dim = 64\n",
        "units = 128\n",
        "vocab_inp_size = len(post_utils.word2idx)\n",
        "vocab_tar_size = len(hashtag_utils.word2idx)\n",
        "\n",
        "train_dataset = MyData(input_tensor_train, target_tensor_train)\n",
        "val_dataset = MyData(input_tensor_val, target_tensor_val)\n",
        "\n",
        "dataset = DataLoader(train_dataset, batch_size = BATCH_SIZE, \n",
        "                     drop_last=True,\n",
        "                     shuffle=True)"
      ],
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KM_FKrUSo_DG"
      },
      "source": [
        "### Encoder Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYp4j_Sunx1O"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.enc_units = enc_units\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
        "        self.gru = nn.GRU(self.embedding_dim, self.enc_units)\n",
        "        \n",
        "    def forward(self, x, lens, device):\n",
        "        # x: batch_size, max_length \n",
        "        \n",
        "        # x: batch_size, max_length, embedding_dim\n",
        "        x = self.embedding(x) \n",
        "                \n",
        "        # x transformed = max_len X batch_size X embedding_dim\n",
        "        # x = x.permute(1,0,2)\n",
        "        x = pack_padded_sequence(x, lens) # unpad\n",
        "    \n",
        "        self.hidden = self.initialize_hidden_state(device)\n",
        "        \n",
        "        # output: max_length, batch_size, enc_units\n",
        "        # self.hidden: 1, batch_size, enc_units\n",
        "        output, self.hidden = self.gru(x, self.hidden) # gru returns hidden state of all timesteps as well as hidden state at last timestep\n",
        "        \n",
        "        # pad the sequence to the max length in the batch\n",
        "        output, _ = pad_packed_sequence(output)\n",
        "        \n",
        "        return output, self.hidden\n",
        "\n",
        "    def initialize_hidden_state(self, device):\n",
        "        return torch.zeros((1, self.batch_sz, self.enc_units)).to(device)"
      ],
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8t6bC5zbn9iv"
      },
      "source": [
        "### sort batch function to be able to use with pad_packed_sequence\n",
        "def sort_batch(X, y, lengths):\n",
        "    lengths, indx = lengths.sort(dim=0, descending=True)\n",
        "    X = X[indx]\n",
        "    y = y[indx]\n",
        "    return X.transpose(0,1), y, lengths # transpose (batch x seq) to (seq x batch)"
      ],
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8p7zBlOoBZj"
      },
      "source": [
        "### Testing the Encoder\n",
        "Before proceeding with training, we should always try to test out model behavior such as the size of outputs just to make that things are going as expected. In PyTorch this can be done easily since everything comes in eager execution by default."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4h1Ta83oDmo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca0f754c-e900-4086-8852-75384507f039"
      },
      "source": [
        "### Testing Encoder part\n",
        "# TODO: put whether GPU is available or not\n",
        "# Device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "encoder.to(device)\n",
        "# obtain one sample from the data iterator\n",
        "it = iter(dataset)\n",
        "x, y, x_len = next(it)\n",
        "\n",
        "# sort the batch first to be able to use with pac_pack_sequence\n",
        "xs, ys, lens = sort_batch(x, y, x_len)\n",
        "\n",
        "enc_output, enc_hidden = encoder(xs.to(device), lens, device)\n",
        "\n",
        "print(enc_output.size()) # max_length, batch_size, enc_units"
      ],
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([537, 64, 128])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-sRvdc1ova3B",
        "outputId": "ed570f0d-2e33-481c-c419-f4f4fa05dc7e"
      },
      "source": [
        "print(encoder)"
      ],
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder(\n",
            "  (embedding): Embedding(13638, 64)\n",
            "  (gru): GRU(64, 128)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qf2QSZexpIZe"
      },
      "source": [
        "### Decoder Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvBHv9YQo6O9"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units, enc_units, batch_sz):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.dec_units = dec_units\n",
        "        self.enc_units = enc_units\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
        "        self.gru = nn.GRU(self.embedding_dim + self.enc_units, \n",
        "                          self.dec_units,\n",
        "                          batch_first=True)\n",
        "        self.fc = nn.Linear(self.enc_units, self.vocab_size)\n",
        "        \n",
        "        # used for attention\n",
        "        self.W1 = nn.Linear(self.enc_units, self.dec_units)\n",
        "        self.W2 = nn.Linear(self.enc_units, self.dec_units)\n",
        "        self.V = nn.Linear(self.enc_units, 1)\n",
        "    \n",
        "    def forward(self, x, hidden, enc_output):\n",
        "        # enc_output original: (max_length, batch_size, enc_units)\n",
        "        # enc_output converted == (batch_size, max_length, hidden_size)\n",
        "        enc_output = enc_output.permute(1,0,2)\n",
        "        # hidden shape == (batch_size, hidden size)\n",
        "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "        # we are doing this to perform addition to calculate the score\n",
        "        \n",
        "        # hidden shape == (batch_size, hidden size)\n",
        "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "        hidden_with_time_axis = hidden.permute(1, 0, 2)\n",
        "        \n",
        "        # score: (batch_size, max_length, hidden_size) # Bahdanaus's\n",
        "        # we get 1 at the last axis because we are applying tanh(FC(EO) + FC(H)) to self.V\n",
        "        # It doesn't matter which FC we pick for each of the inputs\n",
        "        score = torch.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis))\n",
        "        \n",
        "        #score = torch.tanh(self.W2(hidden_with_time_axis) + self.W1(enc_output))\n",
        "          \n",
        "        # attention_weights shape == (batch_size, max_length, 1)\n",
        "        # we get 1 at the last axis because we are applying score to self.V\n",
        "        attention_weights = torch.softmax(self.V(score), dim=1)\n",
        "        \n",
        "        # context_vector shape after sum == (batch_size, hidden_size)\n",
        "        context_vector = attention_weights * enc_output\n",
        "        context_vector = torch.sum(context_vector, dim=1)\n",
        "        \n",
        "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "        # takes case of the right portion of the model above (illustrated in red)\n",
        "        x = self.embedding(x)\n",
        "        \n",
        "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "        #x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "        # ? Looks like attention vector in diagram of source\n",
        "        x = torch.cat((context_vector.unsqueeze(1), x), -1)\n",
        "        \n",
        "        # passing the concatenated vector to the GRU\n",
        "        # output: (batch_size, 1, hidden_size)\n",
        "        output, state = self.gru(x)\n",
        "        \n",
        "        \n",
        "        # output shape == (batch_size * 1, hidden_size)\n",
        "        output =  output.view(-1, output.size(2))\n",
        "        \n",
        "        # output shape == (batch_size * 1, vocab)\n",
        "        x = self.fc(output)\n",
        "        \n",
        "        return x, state, attention_weights\n",
        "    \n",
        "    def initialize_hidden_state(self):\n",
        "        return torch.zeros((1, self.batch_sz, self.dec_units))"
      ],
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WQd4BMwpSl5"
      },
      "source": [
        "#### Testing the Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKgPyYfHpOK1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25202cc7-fb91-4d5b-f1e3-c66292f94631"
      },
      "source": [
        "# Device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "encoder.to(device)\n",
        "# obtain one sample from the data iterator\n",
        "it = iter(dataset)\n",
        "x, y, x_len = next(it)\n",
        "\n",
        "print(\"Input: \", x.shape)\n",
        "print(\"Output: \", y.shape)\n",
        "\n",
        "# sort the batch first to be able to use with pac_pack_sequence\n",
        "xs, ys, lens = sort_batch(x, y, x_len)\n",
        "\n",
        "enc_output, enc_hidden = encoder(xs.to(device), lens, device)\n",
        "print(\"Encoder Output: \", enc_output.shape) # batch_size X max_length X enc_units\n",
        "print(\"Encoder Hidden: \", enc_hidden.shape) # batch_size X enc_units (corresponds to the last state)\n",
        "\n",
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, units, BATCH_SIZE)\n",
        "decoder = decoder.to(device)\n",
        "\n",
        "#print(enc_hidden.squeeze(0).shape)\n",
        "\n",
        "dec_hidden = enc_hidden#.squeeze(0)\n",
        "dec_input = torch.tensor([[hashtag_utils.word2idx['$start']]] * BATCH_SIZE)\n",
        "print(\"Decoder Input: \", dec_input.shape)\n",
        "print(\"--------\")\n",
        "\n",
        "for t in range(1, y.size(1)):\n",
        "    # enc_hidden: 1, batch_size, enc_units\n",
        "    # output: max_length, batch_size, enc_units\n",
        "    predictions, dec_hidden, _ = decoder(dec_input.to(device), \n",
        "                                         dec_hidden.to(device), \n",
        "                                         enc_output.to(device))\n",
        "    \n",
        "    print(\"Prediction: \", predictions.shape)\n",
        "    print(\"Decoder Hidden: \", dec_hidden.shape)\n",
        "    \n",
        "    #loss += loss_function(y[:, t].to(device), predictions.to(device))\n",
        "    \n",
        "    dec_input = y[:, t].unsqueeze(1)\n",
        "    print(dec_input.shape)\n",
        "    break"
      ],
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input:  torch.Size([64, 1277])\n",
            "Output:  torch.Size([64, 42])\n",
            "Encoder Output:  torch.Size([778, 64, 128])\n",
            "Encoder Hidden:  torch.Size([1, 64, 128])\n",
            "Decoder Input:  torch.Size([64, 1])\n",
            "--------\n",
            "Prediction:  torch.Size([64, 1716])\n",
            "Decoder Hidden:  torch.Size([1, 64, 128])\n",
            "torch.Size([64, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zOxoFPRUa59"
      },
      "source": [
        "## Training Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9VMQwwwqlVE"
      },
      "source": [
        "def train(epochs,encoder,decoder,device,ytrain_utils,\n",
        "          loss_function,batch_size,optimizer,dataLoader):\n",
        "  loss_lst = []\n",
        "  for epoch in range(epochs):\n",
        "      start = time.time()\n",
        "      total_loss = 0\n",
        "\n",
        "      encoder.train()\n",
        "      decoder.train()\n",
        "      \n",
        "      for (batch, (inp, targ, inp_len)) in enumerate(dataLoader):\n",
        "          \n",
        "          loss = 0\n",
        "\n",
        "          xs, ys, lens = sort_batch(inp, targ, inp_len)\n",
        "          \n",
        "          enc_output, enc_hidden = encoder(xs.to(device), lens, device)\n",
        "          dec_hidden = enc_hidden\n",
        "          \n",
        "          # use teacher forcing - feeding the target as the next input (via dec_input)\n",
        "          dec_input = torch.tensor([[ytrain_utils.word2idx['$start']]] * batch_size)\n",
        "\n",
        "          \n",
        "           weighted_avg = np.zeros(ys.shape)\n",
        "          for i in range(ys.size(0)):\n",
        "            total_count = 0\n",
        "            temp = np.zeros(ys.size(1))\n",
        "            for j in range(1,ys.size(1)):\n",
        "              if ys[i][j] == 0 or hashtag_utils.idx2word[ys[i][j].item()] == \"end$\":\n",
        "                break;\n",
        "              temp[j] = hashtag_likesCount[hashtag_utils.idx2word[ys[i][j].item()]]\n",
        "              if math.isnan(temp[j]):\n",
        "                temp[j]=0\n",
        "              total_count += temp[j]\n",
        "            if total_count == 0:\n",
        "              continue\n",
        "            weighted_avg[i]  = temp / total_count\n",
        "          \n",
        "          \n",
        "          # run code below for every timestep in the ys batch\n",
        "          for t in range(1, ys.size(1)):\n",
        "              predictions, dec_hidden, _ = decoder(dec_input.to(device), \n",
        "                                          dec_hidden.to(device), \n",
        "                                          enc_output.to(device))\n",
        "              \n",
        "              #predictions = predictions.cpu().detach().numpy()\n",
        "              for i in range(ys.size(0)):\n",
        "                word_idx = ys[i][t]\n",
        "                #print(predictions[i][word_idx],weighted_avg[i,t])\n",
        "                mul_tensor = torch.ones([predictions.size(1)], dtype=torch.float64).to(device)\n",
        "                mul_tensor[word_idx] = weighted_avg[i,t]\n",
        "                predictions[i] *= mul_tensor\n",
        "\n",
        "              #print(np.any(predictions==np.nan))\n",
        "\n",
        "\n",
        "              loss += loss_function(ys[:, t].to(device), predictions.to(device))\n",
        "              #loss += loss_\n",
        "              dec_input = ys[:, t].unsqueeze(1)\n",
        "              \n",
        "          \n",
        "          batch_loss = (loss / int(ys.size(1)))\n",
        "          total_loss += batch_loss\n",
        "          \n",
        "          optimizer.zero_grad()\n",
        "          \n",
        "          loss.backward()\n",
        "\n",
        "          optimizer.step()\n",
        "          \n",
        "          '''if batch % 100 == 0:\n",
        "              print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                          batch,\n",
        "                                                          batch_loss.detach().item()))'''\n",
        "      loss_lst.append(total_loss / N_BATCH )\n",
        "          \n",
        "      ### TODO: Save checkpoint for model\n",
        "      print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                          total_loss / N_BATCH))\n",
        "      print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
        "\n",
        "  return loss_lst\n",
        "    "
      ],
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZKIRwE_qWFw"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    \"\"\" Only consider non-zero inputs in the loss; mask needed \"\"\"\n",
        "    #mask = 1 - np.equal(real, 0) # assign 0 to all above 0 and 1 to all 0s\n",
        "    #print(mask)\n",
        "    mask = real.ge(1).type(torch.cuda.FloatTensor)\n",
        "    \n",
        "    loss_ = criterion(pred, real) * mask \n",
        "    return torch.mean(loss_)"
      ],
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQf6n3mCqg-1"
      },
      "source": [
        "\n",
        "BATCH_SIZE = 64\n",
        "## TODO: Combine the encoder and decoder into one class\n",
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, units, BATCH_SIZE)\n",
        "\n",
        "encoder.to(device)\n",
        "decoder.to(device)\n",
        "\n",
        "optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), \n",
        "                       lr=0.001)"
      ],
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5Ijsl1sWd4p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9d54126-6024-4ecf-f51c-397cec7f02ee"
      },
      "source": [
        "import math\n",
        "epochs = 10\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "loss_lst = train(epochs,encoder,decoder,device,hashtag_utils,\n",
        "      loss_function,BATCH_SIZE,optimizer,dataset)"
      ],
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Loss 0.5668\n",
            "Time taken for 1 epoch 43.352526903152466 sec\n",
            "\n",
            "Epoch 2 Loss 0.4432\n",
            "Time taken for 1 epoch 42.97640085220337 sec\n",
            "\n",
            "Epoch 3 Loss 0.4032\n",
            "Time taken for 1 epoch 43.625099420547485 sec\n",
            "\n",
            "Epoch 4 Loss 0.3737\n",
            "Time taken for 1 epoch 43.08341884613037 sec\n",
            "\n",
            "Epoch 5 Loss 0.3496\n",
            "Time taken for 1 epoch 44.00958609580994 sec\n",
            "\n",
            "Epoch 6 Loss 0.3306\n",
            "Time taken for 1 epoch 43.536887407302856 sec\n",
            "\n",
            "Epoch 7 Loss 0.3143\n",
            "Time taken for 1 epoch 43.9101619720459 sec\n",
            "\n",
            "Epoch 8 Loss 0.2992\n",
            "Time taken for 1 epoch 43.56103706359863 sec\n",
            "\n",
            "Epoch 9 Loss 0.2864\n",
            "Time taken for 1 epoch 44.05646467208862 sec\n",
            "\n",
            "Epoch 10 Loss 0.2736\n",
            "Time taken for 1 epoch 43.800177335739136 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "Zuu-_8fvBQDB",
        "outputId": "f7acd743-eb98-439d-fe6e-b70351f466a5"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(loss_lst)\n",
        "plt.xlabel(\"Epoch_Count\")\n",
        "plt.ylabel(\"Train_Loss\")\n",
        "plt.title(\"Seq2Seq with Attention Model Loss Graph\")\n",
        "plt.show()"
      ],
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAEXCAYAAABYsbiOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxV9Z3/8dcnCYR9TViTsCubChIBd0Ws0Frc60KttjpWK9VuM6P9daat7XSbjtNFtLVWbd2oonVwX1BQVJagKKsQwhYgEPY9Icnn98c5wUvMDRfIzcnyfj4e58E96/eTw733c7/f7znfY+6OiIhIdVKiDkBEROovJQkREYlLSUJEROJSkhARkbiUJEREJC4lCRERiUtJQiJlZj80s4dqWH+jmc2qy5hqm5n9ycz+I+o4EmVmM8zs5gS3dTPrn+yY6jsz6x2ei7SoY6ltShIRMbOzzOx9M9tpZtvM7D0zO60WjvslM5tlZjvMrMjMHjKztjHrh5jZ62GZO8xsvpl98XjLPVbu/gt3vzmMrdY+aOEX3XYzS6+yfLWZjY2Zr9UPd3VJzd1vdfef1cbxq5T1kzD2O6ssvzNc/pPaLvNoHE2ySULZA8xsipkVm9kuM1thZn80s6wo4mnIlCQiYGbtgBeBPwKdgJ7AT4GSWjh8e+DnQA9gUHjs/45Z/wLwBtAN6ALcAeyqhXLrDTPrDZwNODAh0mCSbznwtSrLbgiXN0lhzWYOsAEY7u7tgDOBlcBZcfZpdDWAWuPumup4AnKBHUfY5hvAUmA78BrQK2bdhcAyYCdwHzATuDnOcS4HFoavMwi+ODvUUO7FwAJgB/A+cHLMuuHAh8Bu4B/AFODncY6zBhgRvp4YljsknL8JeD58/RPg8fD12nC7PeF0OnAjMAv4bXguVgHjj3Du/hN4D7gXeDFm+WNABbA/PP6/VVdmAuffgVuBFeF5mgwYQVI+AJSHx9oRbv9o7HkC/gXIB7YB04AeRzp2nL/zJ8DjYZyV53YIsCRc/pMEy6zx/ZTAuegfJ74ZVPO+JPhx+qPwPbIZ+DvQPlzXIox9a/j3zwO6hutuBAoI3n+rgIlxyn0ceOEI75HzgELg34Gi8L3RkeDHW3H4t74IZFX5e34JzCX4YfV/QKdwXe/wXNwQvqe2AP8v6u+a2phUk4jGcqDczP5mZuPNrGPsSjO7BPghwRd8JvAu8FS4LgN4juBDlkHw6+jMGso6B1gcvt5K8EXxuJldamZdq5Q7HHgY+CbQGfgzMM3M0s2sOfA8wYepE/AMcEUN5c4k+CACnEvw4T4nZn5mnFghSGJt3P2DcH4U8Gn49/4G+KuZWQ1lfw14Ipwuqvw73f16gg/wl8Pj/6a6Mms6/zEuBk4DTga+Alzk7ksJvuA/CI/VoWpgZjaG4IvmK0B3gi/KKUc6dg1/KwT/J5W1iRvC+YTKPNL7KcFzcbRuDKfzgb5AG4LkVBl/eyCb4D14K7DfzFoDfyD4gdAWOIPgx0x1xgLPJhBHN4L3ci/gFoLk9Ug4n0PwY+K+Kvt8jSBpdgfKwphinQWcCFwA/KeZDUogjvot6izVVCeCX52PEvyaKSP4dVf5i+kV4KaYbVOAfQRv3q8Bs2PWWXiM6n6xXUjwi+iEmGVZBG/8lQS/qt8BBoTrHgB+VuUYnxJ8qZ9DUH23mHXvE78mcRMwLXy9FLgZmBLOrwFODV//hM9qEr0Jfo2lxRznRiA/Zr5VuE23OOWeBRwEMsL5ZcB3Y9avBsbGzFdXZtzzH847cFbM+qeBu2LinVUlpkcrzxPwV+A3MevahPH2PtKxq/lbf0LwqzmHIPk1C//NJqYmUVOZR3o/JXgujrYmMR34Vsz8iWE8aQRfwIfVYMNtWhPULK4AWh7hs1UGjIuZnxTuuwf4S7jsPKAUaFHDcYYB26v8Pb+KmR8cHiM15n0UW/OYC1xTm98bUUyqSUTE3Ze6+43ungUMJehD+F24uhfw+7BjeQdBE4ER9C/0ANbFHMdj5yuZ2WjgSeBKd18es32hu09y935hOXsJqvuV5X6/styw7OywzB7A+rC8Smtq+BNnAmebWXeCD9HTwJlhf0F74v8KrE5RTPz7wpdt4mx7A/C6u28J558Mlx2Nms7/52Ii+NKMF09VPYg5b+6+h6CGd8zHdve1BDXEXwAr3L3q+6GmMo/0fkrkXBytw+IJX6cBXQlqQa8BU8xsg5n9xsyaufte4GqCmsVGM3vJzAbGOf5Wgl/6lX/TfR7U6n5HkEgrFbv7gcoZM2tlZn82szVmtovgB1QHM0uN2Sf23KwJj5cRs+xY3xf1lpJEPeDuywh+bQ4NF60DvunuHWKmlu7+PrCR4IsbgLDZJTv2eGGz0TTgG+4+vYZy1xG0eceW+19Vym3l7k+F5fas0syTU8Ox8wk+JN8G3nH3XQQfoFsIfmlXVLdbvOMlwsxaEjSpnBte2VUEfBc4xcxOiVNGdWXWdP6P5Eh/wwaCL97KmFsTNKusT+DYNfk78H0+S/iJlnmk99PxnIt4DouH4H1UBmxy94Pu/lN3H0zQpHQxYVOau7/m7hcSJIBlwF/iHH86QfPYkVT9v/o+Qa1mlAed3ZVNkbHv+dhzk0NQA9pCI6YkEQEzG2hm36+8HM/MsoFrgdnhJn8C7jazIeH69mZ2VbjuJWCImV0eXpFxB0HbauWxhwKvAt929xeqlNvRzH5qZv3NLCVsj/5GTLl/AW41s1EWaB1eUtsW+IDgg3yHmTUzs8uBkUf4U2cSVPUr+x9mVJmvqpigCazvEY4bz6UEncaDCZoKhhE0673LZ232m6ocv7oyazr/R7IJyAr7cKrzFPB1MxsWXp77C2COu69O8Pjx/AP4AkGN7WjKrPH9xPGdC4A0M2sRMzUL4/mumfUxszZhPP9w9zIzO9/MTgp/ve8i+BKuMLOuZnZJmOBKCJqOqvuhAUEz3Nlmdq+Z9QzjziB4L9SkLUE/xA4z6wT8uJptvmpmg82sFXAPMNXdy4/ifDQ4ShLR2E3QGTvHzPYSfEkvIvglg7v/E/g1QZV7V7hufLhuC3AV8CuCavUAgit5Kn2foIPxr2a2J5wqO65LCdpO3yT4AC4i+MDdGB47j+AqmPsI+jLyY9aVEvw6u5GgyeFqgg7Pmswk+OC9E2f+MGFT0n8B74XNG6OPcPyqbgAecfe17l5UOYV/z8TwS/CXwI/C4/+gujJrOv8JeIvgQoEiM/vcL0x3fxP4D4KO1Y1AP+Cao/w7P8fd97v7m+6+/2jKPNL76TjPBQT9XPtjpkcILo54jOB9sIrgirBvh9t3A6YSvD+XErxnHiP4rvoeQS1kG0E/2W1xzsVygs9XFvCxme0O/6YN4XmI53dAS4KawWyCH1tVPUZQ6y8iuBLrjhr/+kbADm9ilobIzGYQdP7GvXM5SeU+ChS6+4/qslyRKET1OYuaahIiIhKXkoSIiMSl5iYREYlLNQkREYmrUQ1qlZGR4b179446DBGRBmX+/Plb3D2zunWNKkn07t2bvLy8qMMQEWlQzCzu6AlqbhIRkbiUJEREJC4lCRERiUtJQkRE4lKSEBGRuJQkREQkLiUJERGJS0kC2Ln/IA+9W8C2vaVRhyIiUq8oSQAbd+7n5y8t5dn5hVGHIiJSryhJAAO7tSO3V0eenLuWigoNeCgiUklJIjRxdA6rtuzlg4KtUYciIlJvKEmExg/tTodWzXhiTtwhTEREmhwliVCLZqlcNSKL1xdvYvOuA1GHIyJSLyhJxLh2ZA5lFc7TeeuiDkVEpF5QkojRN7MNZ/bvzFNz11GuDmwRESWJqiaO6sX6HfuZuXxz1KGIiEQu6UnCzMaZ2admlm9md1Wz/kYzKzazBeF0c8y68pjl05IdK8CFg7uS2TadJ2avrYviRETqtaQ+mc7MUoHJwIVAITDPzKa5+5Iqm/7D3SdVc4j97j4smTFW1Sw1hatzs7l/Rj7rd+ynZ4eWdVm8iEi9kuyaxEgg390L3L0UmAJckuQyj9s1I7Nx4B9zVZsQkaYt2UmiJxB7qVBhuKyqK8zsEzObambZMctbmFmemc02s0uTGmmMrI6tOO+ETKbMW8fB8oq6KlZEpN6pDx3XLwC93f1k4A3gbzHrerl7LnAd8Dsz61d1ZzO7JUwkecXFxbUW1MRRvdi8u4TpSzfV2jFFRBqaZCeJ9UBszSArXHaIu29195Jw9iFgRMy69eG/BcAMYHjVAtz9QXfPdffczMzMWgv8/IFd6NG+BU/MUZOTiDRdyU4S84ABZtbHzJoD1wCHXaVkZt1jZicAS8PlHc0sPXydAZwJVO3wTprUFOOakTm8u2ILq7fsratiRUTqlaQmCXcvAyYBrxF8+T/t7ovN7B4zmxBudoeZLTazj4E7gBvD5YOAvHD528CvqrkqKqmuPi2b1BTjKXVgi0gTZe6N587i3Nxcz8vLq9Vj3vrYfOas2srsH15AelpqrR5bRKQ+MLP5Yf/v59SHjut6beLoHLbvO8iri4qiDkVEpM4pSRzBmf0y6NW5le7AFpEmSUniCFJSjOtG5jB39TaWb9oddTgiInVKSSIBV47IonlqCk/qclgRaWKUJBLQuU0640/qxrMfFrKvtCzqcERE6oySRIImjurF7gNlvPjxxqhDERGpM0oSCTqtd0cGdGmjZ2CLSJOiJJEgM2PiqBw+LtzJovU7ow5HRKROKEkchctOzaJFsxSN5yQiTYaSxFFo37IZE07pwf8tWM/uAwejDkdEJOmUJI7SdaN6sa+0nOcXbIg6FBGRpFOSOEqnZLVnSI92PDF7DY1p3CsRkeooSRyloAO7F8uKdvPh2h1RhyMiklRKEsdgwrAetElP0+WwItLoKUkcgzbpaVw6vAcvfrKRHftKow5HRCRplCSO0XUje1FaVsHU+YVRhyIikjRKEsdocI92nJrTgSfnrFUHtog0WkoSx2HiqF4UbNnLBwVbow5FRCQplCSOw5dO7k77ls10B7aINFpKEsehRbNUrhyRxWuLiijeXRJ1OCIitU5J4jhdNyqHsgrn6bx1UYciIlLrlCSOU7/MNpzetzNPzV1LeYU6sEWkcVGSqAUTR+dQuH0/76wojjoUEZFapSRRC74wuBsZbZrrGdgi0ugoSdSC5mkpfCU3m+lLN7Fx5/6owxERqTVJTxJmNs7MPjWzfDO7q5r1N5pZsZktCKebY9bdYGYrwumGZMd6PK4dmYMDU+aqA1tEGo+kJgkzSwUmA+OBwcC1Zja4mk3/4e7DwumhcN9OwI+BUcBI4Mdm1jGZ8R6P7E6tOGdAJlPmraWsvCLqcEREakWyaxIjgXx3L3D3UmAKcEmC+14EvOHu29x9O/AGMC5JcdaKiaNy2LSrhOnLNkcdiohIrUh2kugJxLa/FIbLqrrCzD4xs6lmln00+5rZLWaWZ2Z5xcXRXl00ZmAXurVroTuwRaTRqA8d1y8Avd39ZILawt+OZmd3f9Ddc909NzMzMykBJiotNYVrRmbzzvJi1m7dF2ksIiK1IdlJYj2QHTOfFS47xN23unvlmBYPASMS3bc+uua0HFJTjCfnqjYhIg1fspPEPGCAmfUxs+bANcC02A3MrHvM7ARgafj6NeALZtYx7LD+QrisXuvWvgUXDOzCM3nrKCkrjzocEZHjktQk4e5lwCSCL/elwNPuvtjM7jGzCeFmd5jZYjP7GLgDuDHcdxvwM4JEMw+4J1xW700c3Yute0t5bfGmqEMRETku1pgemJObm+t5eXlRh0FFhXPub9+mR/uW/OObp0cdjohIjcxsvrvnVreuPnRcNzopKcZ1I3sxZ9U28jfvjjocEZFjpiSRJFflZtEs1XQ5rIg0aEoSSZLRJp1xQ7vz7PxC9peqA1tEGiYliSSaOCqHXQfKePGTDVGHIiJyTJQkkmhUn070y2yteyZEpMFSkkgiM2PiqF58tHYHizfsjDocEZGjpiSRZFecmkV6WooeSCQiDZKSRJK1b9WML5/Sg+c/Ws+ekrKowxEROSpKEnVg4qgc9paW838L6v3QUyIih1GSqAPDsjswqHs7Hp+9lsZ0h7uINH5KEnUg6MDOYenGXSxYtyPqcEREEqYkUUcuHd6T1s1TdQe2iDQoShJ1pE16GpcM78kLH29g576DUYcjIpIQJYk6dN3IHErKKnj2w8KoQxERSYiSRB0a2rM9w7I78MScNerAFpEGQUmijk0clcPK4r3MWdUgnp8kIk2ckkQdu/jkHrRrkaYObBFpEJQk6ljL5qlcMSKLVxdtZMuekqjDERGpkZJEBCaOyuFgufNMnjqwRaR+U5KIQP8ubRnVpxNPzV1LRYU6sEWk/lKSiMjE0b1Yu20fs/K3RB2KiEhcShIRuWhIVzq3bs4Tc9ZEHYqISFxKEhFJT0vlqtxs3ly6maKdB6IOR0SkWkoSEbpuZA7lFc4/5q2LOhQRkWollCTMrLWZpYSvTzCzCWbWLLmhNX45nVtxzgmZTJm3lrLyiqjDERH5nERrEu8ALcysJ/A6cD3waCI7mtk4M/vUzPLN7K4atrvCzNzMcsP53ma238wWhNOfEoy1QbluZA4bdx7g7U+Low5FRORzEk0S5u77gMuB+939KmDIEXcySwUmA+OBwcC1Zja4mu3aAncCc6qsWunuw8Lp1gRjbVAuGNSFru3S1YEtIvVSwknCzE4HJgIvhctSE9hvJJDv7gXuXgpMAS6pZrufAb8GmlwPbrPUFK4+LYeZy4tZt21f1OGIiBwm0STxHeBu4J/uvtjM+gJvJ7BfTyC2V7YwXHaImZ0KZLv7S3xeHzP7yMxmmtnZ1RVgZreYWZ6Z5RUXN8wmm2tOy8aAp+ZqPCcRqV8SShLuPtPdJ7j7r8MO7C3ufsfxFh4e617g+9Ws3gjkuPtw4HvAk2bWrprYHnT3XHfPzczMPN6QItGjQ0vGDOzK03nrKC1TB7aI1B+JXt30pJm1M7PWwCJgiZn9awK7rgeyY+azwmWV2gJDgRlmthoYDUwzs1x3L3H3rQDuPh9YCZyQSLwN0cTROWzZU8rrS4qiDkVE5JBEm5sGu/su4FLgFaAPwRVORzIPGGBmfcysOXANMK1ypbvvdPcMd+/t7r2B2cAEd88zs8yw45uweWsAUJDoH9bQnDMgk6yOLXlitpqcRKT+SDRJNAvvi7gUmObuB4Ejjkzn7mXAJOA1YCnwdNincY+ZTTjC7ucAn5jZAmAqcKu7N9on9aSmGNeOzOGDgq089G6BnlwnIvVCWoLb/RlYDXwMvGNmvYBdiezo7i8DL1dZ9p9xtj0v5vWzwLMJxtcofOPMPnxSuIOfv7SUhet38qvLT6Zl80QuIhMRSQ471l+sZpYW1hTqjdzcXM/Ly4s6jONSUeHcPyOf/3ljOYO6tePP148gu1OrqMMSkUbMzOa7e2516xLtuG5vZvdWXmpqZv8DtK7VKAWAlBRj0pgBPHzDaazbvo8J981i1goNJy4i0Ui0T+JhYDfwlXDaBTySrKAEzh/YhWmTziKjTTpfe3gOf3lH/RQiUvcSTRL93P3H4Z3TBe7+U6BvMgMT6JPRmn/efiYXDenGf728lDunLGB/aXnUYYlIE5JokthvZmdVzpjZmcD+5IQksdqkp3H/xFP514tO5IVPNnD5A+9r+A4RqTOJJolbgclmtjq86e0+4JtJi0oOY2bcfn5/Hr7xNNZv38eX75vFuysa5hAkItKwJDosx8fufgpwMnByOFTGmKRGJp9z/olBP0XXti244eG5/HnmSvVTiEhSHdWT6dx9V3jnNQTjKUkd653Rmue+dQbjhnbjl68s49tPfcS+0np1JbKINCLH8/hSq7Uo5Ki0Tk9j8nWn8u/jBvLSwo1cfv/7rN2qfgoRqX3HkyTUzhEhM+O28/rx6NdHsnHnAb583yzeWa5+ChGpXTUmCTPbbWa7qpl2Az3qKEapwbknZDJt0pl0b9+CGx+ZywMz1E8hIrWnxiTh7m3dvV01U1t3PzTuk5l1TH6oEk+vzkE/xfiTuvPrV5cx6cmP2FuifgoROX7H09wUa3otHUeOUavmadx37XDuGj+QVxYF/RRrtu6NOiwRaeBqK0moE7seMDNuPTfopyjadYAv/3EWMz7dHHVYItKA1VaSUCN4PXLOCZm8MOksenRoydcfncfkt/PVTyEix6S2koTUMzmdW/Hct87g4pN78N+vfcrtT36ofgoROWpqbmrEWjVP4w/XDOP/fXEQry4q4rL732PVFvVTiEjiEk4SZpZqZj3MLKdyill9QRJik1pgZvzLOX35+zdGsXl3CRPum8Xb6qcQkQQl+tChbwObgDeAl8Lpxcr1jfnZ043FWQMyeGHSWWR3bMU3Hp3HfW+tUD+FiBxRojWJO4ET3X2Iu58UTicnMzCpfdmdWvHsbWfw5ZN78NvXl3Pb4x+yR/0UIlKDRJPEOmBnMgORutGyeSq/v2YYP/rSIF5fUsRlk9VPISLxJZokCoAZZna3mX2vckpmYJI8ZsbNZ/fl8ZtGsWVP0E/x1rJNUYclIvVQokliLUF/RHOgbcwkDdgZ/TN44dtnkdOpFTf9LY8/Tl9BRYX6KUTkM9aYOi9zc3M9Ly8v6jAanP2l5dz93Cc8v2ADXxjclf/5yim0bdEs6rBEpI6Y2Xx3z61uXVp1C2N2/J27f8fMXqCau6rdfUItxSgRatk8lf+9ehgnZXXgFy8v5dLJ7/Hg13Lpl9km6tBEJGJHam56LPz3t8D/VDMdkZmNM7NPzSzfzO6qYbsrzMzNLDdm2d3hfp+a2UWJlCfHxsy46aw+PHbTSLbvO8jFf5jF5LfzOXCwPOrQRCRCSW1uMrNUYDlwIVAIzAOudfclVbZrS3DvRXNgkrvnmdlg4ClgJMGzK94ETnD3uN9aam6qHRt27OeeF5bw6uIienVuxX98aTAXDOqCmW6sF2mMampuSvRmugFmNtXMlphZQeWUwK4jgXx3L3D3UmAKcEk12/0M+DVwIGbZJcAUdy9x91VAfng8SbIeHVryp+tH8NhNI2mWmsLNf8/jxkfmsbJ4T9ShiUgdS/TqpkeAB4Ay4Hzg78DjCezXk+Aei0qF4bJDzOxUINvdXzrafcP9bzGzPDPLKy7W4ztr09kDMnnlzrP5j4sH8+Ga7Yz73Tv88uWlugFPpAlJNEm0dPfpBM1Ta9z9J8CXjrdwM0sB7gW+f6zHcPcH3T3X3XMzMzOPNySpollqCjed1Ye3fnAelw3vyZ/fKeD8387guQ8LdbmsSBOQaJIoCb/QV5jZJDO7DEjk0pf1QHbMfFa4rFJbYCjBjXqrgdHAtLDz+kj7Sh3KbJvOb648hedvP5MeHVryvac/5so/vc/CQt2IL9KYHc3YTa2AO4ARwFeBGxLYbx4wwMz6mFlz4BpgWuVKd9/p7hnu3tvdewOzgQnunhdud42ZpZtZH2AAMDfBeCVJhmV34J+3ncFvrjyZtdv2MWHyLO5+7hO27imJOjQRSYIa75OAQ1coXe3uPwD2AF9P9ODuXmZmk4DXgFTgYXdfbGb3AHnuPq2GfReb2dPAEoK+kNtrurJJ6k5KivGV3GzGDe3GH95cwaPvr+alTzbyvQtP4Kuje5GWqmdZiTQWNV4Ca2Zp4Rf9bHcfXYdxHRNdAhuN/M27+ekLS3h3xRZO7NqWH08YzBn9MqIOS0QSdDyXwFY273xkZtPM7Hozu7xyqt0wpaHq36Utf//GSP58/Qj2lpZx3V/mcPsTH7J+x/6oQxOR43TE5qZQC2ArMIZgeA4L/30uSXFJA2NmXDSkG+eekMmD7xRw/4x8pi/bxG3n9ueb5/alRbPUqEMUkWNwpOamQoJLVCuTQuwtt+7u9yY3vKOj5qb6Y/2O/fzipaW8tHAjWR1b8qMvDeaiIV1117ZIPXQ8zU2pBJe6tiG4XLVNlUmkWj07tGTyxFN58l9G0bp5Grc+Pp+vPTyX/M27ow5NRI7CkWoSH7r7qXUYz3FRTaJ+Kiuv4PHZa7j3jeXsKy3nhjN6c+fYAbTTcOQi9cLx1CTUNiDHLS01hRvP7MPbPziPq3Kzefi9VYz57Qyezlunu7ZF6rkjJYkL6iQKaRI6t0nnl5efxLTbg6fh/dvUT7jsgfdZsG5H1KGJSBw1Jgl331ZXgUjTcVJWe5697Qz+9+pT2LhjP5dOfo9/feZjinfrrm2R+ka3xkokzIzLhmfx1g/O45vn9OX5BesZ89sZPPRuAQfLK6IOT0RCShISqTbpadz9xUG8+p1zOLVXR37+0lLG//5d3l2hYd9F6gMlCakX+mW24dGvn8ZDX8ultKyC6/86l28+lsfarfuiDk2kSUv0jmuRpDMzxg7uylkDMvjrrFXc91Y+by6dwaXDenLbef3o30W35ojUtaQ+47qu6T6JxmXTrgP8eWYBT85dQ0lZBeOHduP28/szpEf7qEMTaVRquk9CSULqvS17Snh41ioe+2ANu0vKOP/ETCaN6c+IXp2iDk2kUVCSkEZh5/6D/P391Tz83iq27zvI6X07M2lMf87o11ljQokcByUJaVT2lpTx1Ny1PPhOAZt3lzAsuwOTzu/PBYO6KFmIHAMlCWmUDhwsZ+r8Qv40cyWF2/czsFtbbj+/P188qTupKUoWIolSkpBG7WB5BdMWbOD+GfmsLN5Ln4zW3HZuPy4d3pPmabrKW+RIlCSkSaiocF5dXMR9b+WzZOMuenZoyS3n9OXq07L10CORGihJSJPi7sz4tJj73s5n/prtZLRJ5+az+/DV0b1ok65bg0SqUpKQJsndmV2wjclv5zMrfwvtWzbjxjN68/Uze9OhVfOowxOpN5QkpMlbsG5HeAf3Jlo3T+Wro3tx09l96NK2RdShiUROSUIktKxoF5PfXslLn2ygWWoKV5+WzTfP7UfPDi2jDk0kMkoSIlWs2rKXB2bk89yH6wG4bHgwPlTfTI0PJU3P8Ty+tDYKH2dmn5pZvpndVc36W81soZktMLNZZjY4XN7bzPaHyxeY2Z+SHas0HX0yWvObK09h5r+dz8RROUz7eANj753JpCc/ZOnGXVGHJ1JvJLUmYWapwHLgQqAQmAdc6+5LYrZp5+67wtcTgG+5+zgz6w286O0kjOYAABCGSURBVO5DEy1PNQk5VsW7S3hoVgGPf7CGvaXljB3UhdvP78/wnI5RhyaSdFHWJEYC+e5e4O6lwBTgktgNKhNEqDXQeNq/pMHIbJvO3eMH8d5dY/jO2AHMW72dy+5/n4kPzeb9lVtoTM2yIkcj2UmiJ7AuZr4wXHYYM7vdzFYCvwHuiFnVx8w+MrOZZnZ2dQWY2S1mlmdmecXFepqZHJ8OrZrznbEn8N5dY7h7/EA+LdrDdX+Zw5f+MItH3lvFtr2lUYcoUqeS3dx0JTDO3W8O568HRrn7pDjbXwdc5O43mFk60Mbdt5rZCOB5YEiVmsdh1Nwkta1yfKgp89ayaP0umqUaYwd15arcLM4ZkElaqob9kIavpuamZN9+uh7IjpnPCpfFMwV4AMDdS4CS8PX8sKZxAqAsIHWmRbPgnoqvju7F0o27eCavkOcXrOeVRUV0aZvOZaf25KoR2XpqnjRaya5JpBF0XF9AkBzmAde5++KYbQa4+4rw9ZeBH7t7rpllAtvcvdzM+gLvAie5+7Z45akmIXWhtKyCt5ZtZur8dbz9aTHlFc7wnA5cNSKbi0/pTrsWzaIOUeSoRHqfhJl9EfgdkAo87O7/ZWb3AHnuPs3Mfg+MBQ4C24FJ7r7YzK4A7gmXVxAkjxdqKktJQura5t0HeP6j9TyTV8iKzXto0SyFcUO6cVVuNqf37UyKhiyXBkA304kkmbvzceFOnslbx7SPN7D7QBk9O7TkihFZXDUii+xOraIOUSQuJQmROnTgYDmvLS5i6vxCZuVvwR1G9+3EV3KzGT+0Oy2ba9hyqV+UJEQisn7Hfp6bX8jUDwtZs3UfbdLTuPjk7lw5IosRvTrqcatSLyhJiETM3Zm7ahvPzC/k5YUb2VdaTt+M1lyZm8UVp2bRtZ1Go5XoKEmI1CN7Ssp4eeFGpuYVMnf1NlIMzjkhk6tGZDN2cBfS09QcJXVLSUKknlq9ZS9T5xfy7IeFbNx5gA6tmnHJKT24KjebIT3aqTlK6oSShEg9V17hvJe/hWfmF/La4iJKyyoY2K0tV+Vmc+mwHnRukx51iNKIKUmINCA79x1k2icbmJq3jo8Ld9Is1RgzsAtXjcjm3BMzaaahQKSWKUmINFCfFu1m6vx1/POj9WzZU0q7FmlcOLgb44d246wBGbRopv4LOX5KEiIN3MHyCt5ZXszLC4t4Y0kRuw6U0SY9jTEDuzB+aDfOO7GL7r+QY6YkIdKIlJZV8EHBVl5ZuJHXl2xi295SWjRL4fwTuzBuaDfGDOxCW40fJUdBSUKkkSorr2Du6m28srCIVxcXUby7hOZpKZwzIINxQ7tz4aCutG+lhCE1U5IQaQIqKpwP127n5YVFvLpoIxt2HiAtxTijfwZfHNqNCwd31VVSUi0lCZEmpnLAwVcWbeTVRUWs2bqPFIPRfTszfmg3LhrSjS66y1tCShIiTZi7s2TjLl5dVMTLCzeysngvZpDbqyPjhnZn3NBu9OzQMuowJUJKEiJyyIpNu3klTBjLinYDcEp2B8YPDS6t7dW5dcQRSl1TkhCRaq3espdXFhXxyqKNfFK4E4DB3dsFCeOk7nosaxOhJCEiR7Ru2z5eW1zEK4uKmL9mOwADurRh/EndGT+0GwO7tdVYUo2UkoSIHJWinQfChLGRuau2UeHQu3OrQwnjpJ7tlTAaESUJETlmW/aU8PriTbyyaCPvr9xKeYXTtV06FwzqythBXTijn4YHaeiUJESkVuzYV8qbSzczfekm3llezN7Sclo0S+Gs/pmMHdSFMYO60KWtLq1taJQkRKTWlZSVM6dgG9OXbuLNpZtZv2M/AKdkteeCQV25YFAXBnfXMzEaAiUJEUkqd2dZ0e5DCePjwh24Q4/2LQ4ljNF9O6tZqp5SkhCROlW8u4S3l23mzaWbeHfFFvYfLKdV81TOHpDBBYO6MmZgFzI0REi9oSQhIpE5cLCcD1Zu5c2lm5i+dDNFuw5gBsOyOzA2rGWc2FWX10ZJSUJE6gV3Z/GGXUxfupnpyzYduoEvq2PLQwljVJ/ONE/T0/fqUqRJwszGAb8HUoGH3P1XVdbfCtwOlAN7gFvcfUm47m7gpnDdHe7+Wk1lKUmINCybdh0IEsbSTczK30JJWQVt0tM454QMLhjYlfMHdqFT6+ZRh9noRZYkzCwVWA5cCBQC84BrK5NAuE07d98Vvp4AfMvdx5nZYOApYCTQA3gTOMHdy+OVpyQh0nDtLy3nvfwtTF8WNEtt3l1CisGpOR0P3ZPRv0sbNUslQU1JIi3JZY8E8t29IAxkCnAJcChJVCaIUGugMmtdAkxx9xJglZnlh8f7IMkxi0gEWjZPZezgrowd3JWKCmfh+p2Hrpb69avL+PWry8jp1IqxYcI4rU8nmqWqWSrZkp0kegLrYuYLgVFVNzKz24HvAc2BMTH7zq6yb89q9r0FuAUgJyenVoIWkWilpBinZHfglOwOfO8LJ7Jhx36mLwuapR6fvYaH31tFm/Q0RvbpxOl9O3N6v84M6t6O1BTVMmpbspNEQtx9MjDZzK4DfgTccBT7Pgg8CEFzU3IiFJEo9ejQkutH9+L60b3YW1LGuyu2MHN5MbMLtvLWss0AtG/Z7FDSOKN/Z07o0pYUJY3jluwksR7IjpnPCpfFMwV44Bj3FZEmoHV6GuOGdmPc0G4AbNy5n9kFW/lg5VY+KNjKG0s2AdCpdXNG9/2sptEvU/0ZxyLZSWIeMMDM+hB8wV8DXBe7gZkNcPcV4eyXgMrX04Anzexego7rAcDcJMcrIg1M9/YtuWx4FpcNzwKCIc8/KNjK7DBpvLywCIDMtumM7tv5UNLo3bmVkkYCkpok3L3MzCYBrxFcAvuwuy82s3uAPHefBkwys7HAQWA7YVNTuN3TBJ3cZcDtNV3ZJCICkN2pFdmdWvGV3GzcnTVbg6RRWdN44eMNAHRr14LT+32WNLI7tYo48vpJN9OJSJPh7qws3nuopjG7YCtb95YCwQ19lQnj9H6d6d6+6Tz3W3dci4hUw91ZvmkPH6zcEiSOgm3s3H8QCB6ydHq/zkETVb/OjXoIdCUJEZEEVFQ4S4t28UFYy5hTsI3dJWUA9Mtszen9OnNGvwxG9+3cqO4EV5IQETkGZeUVLN6w61CfxrzV29hXGnSNDuzW9lAtY3SfzrRv1SziaI+dkoSISC04WF7BJ4U7D11ym7dmGwcOVmAGQ3q0C+7R6JfBaX060Sa9XtyGlhAlCRGRJCgpK+fjdTv5YOVW3l+5hY/W7qC0vILUFOPkrPaHOsJze3WiZfP6+8AlJQkRkTpw4GA589dsP5Q0PincSVmF0yzVGJ7d8dCVU8NzOpCeVn+ShpKEiEgE9paUMW/1tkP3aCxav5MKh/S0FHJ7dwxrGhmcnNU+0sEKlSREROqBnfsPMnfVtkM1jWVFuwFo3TyV02IGKxzSo32dDlaoJCEiUg9t21t62LhT+Zv3ANC2RRqj+nTmjLB56sSuyR2sMMrnSYiISBydWjfniyd154sndQdg864Dhw0h8ubS6gYrzKBfZus6G3dKNQkRkXpq/Y79h5qmZq/cyoadBwDo0jb9sHGncjod32CFam4SEWng3J212/bx/sqtYeLYypY9JQD07NCSy0/tyfe/cOIxHVvNTSIiDZyZ0atza3p1bs21I3PCwQr3HEoY5RXJ+cGvJCEi0gCZGf27tKV/l7Zcf3rvpJWjp4iLiEhcShIiIhKXkoSIiMSlJCEiInEpSYiISFxKEiIiEpeShIiIxKUkISIicTWqYTnMrBhYcxyHyAC21FI4DZ3OxeF0Pj6jc3G4xnA+erl7ZnUrGlWSOF5mlhdv/JKmRuficDofn9G5OFxjPx9qbhIRkbiUJEREJC4licM9GHUA9YjOxeF0Pj6jc3G4Rn0+1CchIiJxqSYhIiJxKUmIiEhcShKAmY0zs0/NLN/M7oo6niiZWbaZvW1mS8xssZndGXVMUTOzVDP7yMxejDqWqJlZBzObambLzGypmZ0edUxRMrPvhp+TRWb2lJm1iDqm2tbkk4SZpQKTgfHAYOBaMxscbVSRKgO+7+6DgdHA7U38fADcCSyNOoh64vfAq+4+EDiFJnxezKwncAeQ6+5DgVTgmmijqn1NPkkAI4F8dy9w91JgCnBJxDFFxt03uvuH4evdBF8CPaONKjpmlgV8CXgo6liiZmbtgXOAvwK4e6m774g2qsilAS3NLA1oBWyIOJ5apyQRfAGui5kvpAl/KcYys97AcGBOtJFE6nfAvwEVUQdSD/QBioFHwua3h8ysddRBRcXd1wO/BdYCG4Gd7v56tFHVPiUJqZaZtQGeBb7j7ruijicKZnYxsNnd50cdSz2RBpwKPODuw4G9QJPtwzOzjgStDn2AHkBrM/tqtFHVPiUJWA9kx8xnhcuaLDNrRpAgnnD356KOJ0JnAhPMbDVBM+QYM3s82pAiVQgUuntlzXIqQdJoqsYCq9y92N0PAs8BZ0QcU61TkoB5wAAz62NmzQk6nqZFHFNkzMwI2pyXuvu9UccTJXe/292z3L03wfviLXdvdL8UE+XuRcA6MzsxXHQBsCTCkKK2FhhtZq3Cz80FNMKO/LSoA4iau5eZ2STgNYKrEx5298URhxWlM4HrgYVmtiBc9kN3fznCmKT++DbwRPiDqgD4esTxRMbd55jZVOBDgqsCP6IRDtGhYTlERCQuNTeJiEhcShIiIhKXkoSIiMSlJCEiInEpSYiISFxKEiIiEpeShDQZZlZuZgtiplobUsLMepvZoqPc5wfhkNsLzGyemX2ttuIJj9/BzL5Vm8eUpqfJ30wnTcp+dx8WdRAAZnYrcCEw0t13mVk74LJaLqYD8C3g/lo+rjQhqklIk2dmq83sN2a20Mzmmln/cHlvM3vLzD4xs+lmlhMu72pm/zSzj8OpcryeVDP7S/gQmtfNrGUNxf4QuK1y8ER33+XufwuPf0E4yupCM3vYzNJj4swIX+ea2Yzw9U/C7WaYWYGZ3RGW8SugX1hT+e/aPWvSVChJSFPSskpz09Ux63a6+0nAfQTDgwP8Efibu58MPAH8IVz+B2Cmu59CMMBd5TAuA4DJ7j4E2AFcUV0QYa2hrbsXVLOuBfAocHUYTxpwWwJ/20DgIoLno/w4HKTxLmCluw9z939N4Bgin6MkIU3J/vALs3L6R8y6p2L+rXwk5+nAk+Hrx4CzwtdjgAcA3L3c3XeGy1e5e+V4V/OB3scQ44nhcZaH838jeNDPkbzk7iXuvgXYDHQ9hrJFPkdJQiTgcV4fjZKY1+XE6fMLm5j2mFnfozx+GZ99Zqs+SzmhskWOlpKESODqmH8/CF+/z2fPLJ4IvBu+nk7YBGRmqeFjPY/WL4HJYdMTZtYmvLrpU6B3Zb8IwYi8M8PXq4ER4etqm7Kq2A20PYbYRA5RkpCmpGqfxK9i1nU0s0+AO4Hvhsu+DXw9XH59uI7w3/PNbCFBs9LgY4jlAeBtYF546ey7QIW7HyAYfvuZ8PgVwJ/CfX4K/N7M8ghqCzVy963Ae2a2SB3Xcqw0VLg0eeGT53LD9nwRiaGahIiIxKWahEgSmdlkgqf9xfq9uz8SRTwiR0tJQkRE4lJzk4iIxKUkISIicSlJiIhIXEoSIiIS1/8H1n2lLPHuRkEAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Im8ABO4vBtZo"
      },
      "source": [
        "### Predict hashtags"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgnxaaF7nJbI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfa009b6-0cfa-4ebb-ddff-fe6ab37dba2c"
      },
      "source": [
        "encoder_pred = Encoder(vocab_inp_size, embedding_dim, units, 1)\n",
        "decoder_pred = Decoder(vocab_tar_size, embedding_dim, units, units,1)\n",
        "\n",
        "encoder_pred.to(device)\n",
        "decoder_pred.to(device)\n"
      ],
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Decoder(\n",
              "  (embedding): Embedding(1716, 64)\n",
              "  (gru): GRU(192, 128, batch_first=True)\n",
              "  (fc): Linear(in_features=128, out_features=1716, bias=True)\n",
              "  (W1): Linear(in_features=128, out_features=128, bias=True)\n",
              "  (W2): Linear(in_features=128, out_features=128, bias=True)\n",
              "  (V): Linear(in_features=128, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 179
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_nGxVj0B0ah"
      },
      "source": [
        "#### Loading weights into another model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uT0NV35qZps",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8079364-50d2-4a04-f31d-6fe0f8dd14e5"
      },
      "source": [
        "encoder_pred.load_state_dict(encoder.state_dict())\n",
        "decoder_pred.load_state_dict(decoder.state_dict())"
      ],
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 180
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O98VDtNptfOB"
      },
      "source": [
        "softmax = nn.Softmax(dim=1)\n",
        "\n",
        "def predict(X_test):\n",
        "  results = []\n",
        "  encoder_pred.eval()\n",
        "  decoder_pred.eval()\n",
        "\n",
        "  for xs in enumerate(X_test):\n",
        "    xs = xs[1]\n",
        "    length = np.sum(1 - np.equal(xs, 0))\n",
        "    xs = torch.tensor(xs).view(-1,1)\n",
        "    length = torch.tensor(length).view(-1,)\n",
        "\n",
        "    enc_output, enc_hidden = encoder_pred(xs.to(device), length, device)\n",
        "    dec_hidden = enc_hidden\n",
        "        \n",
        "    dec_input = torch.tensor([[hashtag_utils.word2idx['$start']]])\n",
        "    curr_hashtags = []\n",
        "    # run code below till we generate \"end$\" tag or 10 hashtags\n",
        "    for t in range(1,10):\n",
        "      predictions, dec_hidden, _ = decoder_pred(dec_input.to(device), \n",
        "                                          dec_hidden.to(device), \n",
        "                                          enc_output.to(device))\n",
        "      #print(predictions.size())\n",
        "      top_val,top_idx  = softmax(predictions).topk(1,dim = 1)\n",
        "      #print(top_idx.item())\n",
        "      if top_idx == hashtag_utils.word2idx[\"end$\"]:\n",
        "        break\n",
        "      pred_hashtag = hashtag_utils.idx2word[top_idx.item()]\n",
        "      curr_hashtags.append(pred_hashtag)\n",
        "      dec_input = torch.tensor([top_idx]).unsqueeze(1)\n",
        "\n",
        "    results.append(curr_hashtags)\n",
        "\n",
        "  return results"
      ],
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDJGPUYGB9zb"
      },
      "source": [
        "#predict hashtags  on test data\n",
        "predicted_hashtags = predict(input_tensor_test)"
      ],
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijat3BwSdErZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de37b775-b889-4cb2-93a8-c50b67fec763"
      },
      "source": [
        "for i in range(10):\n",
        "  print(Y_test.iloc[i],predicted_hashtags[i])"
      ],
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "$start nadiamurad denismukwege nobelprize nobelpeaceprize nobellaureate peace activism activist activists humanrights humanrightsactivist woman women girl girls rape victim victims rapevictim rapesurvivor sexualviolence genderviolence yazidi saveyazidi iraq doctor gynecologist congo congolese end$ ['woman', 'women', 'girl', 'girls', 'woman', 'women', 'girl', 'girls', 'woman']\n",
            "$start giletsjaunes giletjaune 17novembre blocage yellowvests protest demonstration transport transportation traffic gas gasprices petrol diesel car cars tax taxes vehicle road roads driver drivers emmanuelmacron macron manifestation paris france french français end$ []\n",
            "$start whatsyournature wildebeest wildlife kenya magicalkenya africa protectpreserve livenature naturephotography photocontest end$ []\n",
            "$start vacation end$ []\n",
            "$start brussels eucouncil eu2018at justislipsius sunnyday streetphotography rainbow livemusic schuman bruxellesmabelle welovebrussels end$ ['euco', 'eucouncil', 'euco', 'eucouncil', 'euco', 'eucouncil', 'euco', 'eucouncil', 'euco']\n",
            "$start exams exam celebrating oxford fridayfeeling end$ ['repost']\n",
            "$start whatsyournature photocontest beautiful zebra oxpecker wildlife wildplaces protectpreserve livenature nature naturephotography end$ []\n",
            "$start bboy hiphop streetdance streetdancer hiphopculture hiphopdance hiphopstyle hiphopchoreo hiphopdancer hiphopdancers dancechoreography danceroutine dancevideos dancevideo instadance instadancer dancefun letsdance justdance dancelove danceflavors dance dancing tanzen bailar dans danse dancewithtalent end$ ['dance', 'dancer', 'lovedance', 'dancelove', 'lovedancing', 'dancelover', 'bollywood', 'bollywooddance', 'bollywooddanceroutine']\n",
            "$start architecture christmas winter picoftheday end$ []\n",
            "$start euco traineeship internship firstjob bxl stagier eutrainee end$ ['dance', 'dancer', 'lovedance', 'dancelove', 'lovedancing', 'dancelover', 'bollywood', 'bollywooddance', 'bollywooddanceroutine']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZXxn9LKidKN"
      },
      "source": [
        "import csv\n",
        "actual = Y_test.values.tolist()\n",
        "with open(\"file.csv\",\"w\") as f:\n",
        "  writer = csv.writer(f)\n",
        "  writer.writerows(zip(actual,predicted_hashtags))"
      ],
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEZB3YXVtDXe"
      },
      "source": [
        "## Bi-directional Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "id": "8ymDHxFjrFFB",
        "outputId": "239bf222-11b4-4c46-f6af-e8effba8e81f"
      },
      "source": [
        "'''class encoder(nn.Module):\n",
        "\n",
        "  def _init_(self, input_size, embedding_size, hidden_size, layers, bidirectional):\n",
        "    '''\n",
        "    input_size = size of vocab\n",
        "    embedding_size = embedding dim\n",
        "    hidden_size = hidden state size\n",
        "    layer = num of layers of lstms\n",
        "    '''\n",
        "\n",
        "    # The embedding layer is created using nn.Embedding, the LSTM with nn.LSTM\n",
        "    super()._init_()\n",
        "    # input_dim is the size/dimensionality of the one-hot vectors that will be input to the encoder. This is equal to the input (source) vocabulary size.\n",
        "    # emb_dim is the dimensionality of the embedding layer. This layer converts the one-hot vectors into dense vectors with emb_dim dimensions.\n",
        "    # hidden_size is the dimensionality of the hidden and cell states.\n",
        "    # layers is the number of layers in the RNN.\n",
        "    self.embed = nn.Embedding(num_embeddings=input_size, embedding_dim=embedding_size) # output size = (*,embedding_size)\n",
        "    self.lstm = nn.LSTM(input_size=embedding_size, hidden_size= hidden_size, num_layers=layers, batch_first = True, bidirectional = bidirectional)\n",
        "    self.bidirectional = bidirectional\n",
        "    #in order to convert bidirectional hidden state to unidirectional if LSTM is bidirectional\n",
        "    # We can just think of $c_t$ as another type of hidden state. \n",
        "    # Similar to $h_0^l$, $c_0^l$ will be initialized to a tensor of all zeros. \n",
        "    # Also, our context vector will now be both the final hidden state and the final cell state, i.e. $z^l = (h_T^l, c_T^l)$. \n",
        "    self.fc_hidden = nn.Linear(hidden_size*2, hidden_size)\n",
        "    self.fc_cell = nn.Linear(hidden_size*2, hidden_size)\n",
        "\n",
        "  def forward(self,x):\n",
        "    '''\n",
        "    x shape = [batch_size, sentence]\n",
        "    one complete sentence represents a \"sequence\"\n",
        "    '''\n",
        "    x = self.embed(x) # shape [batch_size,  sentence, embed_size]\n",
        "    output, (hidden_state, cell_state) = self.lstm(x) #shape [batch_size, seq_len, num_directions(2)*hidden_size]\n",
        "\n",
        "    if self.bidirectional:  #since we have 2 directions so add(concat) hidden of both directions into one\n",
        "      hidden = torch.cat((hidden_state[0:1], hidden_state[1:2]), dim=2)\n",
        "      cell = torch.cat((cell_state[0:1], cell_state[1:2]), dim = 2) #output [1(layer), batch, hidden_size*2]\n",
        "      hidden_state = self.fc_hidden(hidden)\n",
        "      cell_state = self.fc_cell(cell)\n",
        "\n",
        "    # print(output.shape, x.shape)\n",
        "    #output shape = [batch_size, seq_len, 2*hidden_size] \n",
        "    #hidden shape =[1(layers), batch_size, hidden_size]\n",
        "    #  RNN returns: outputs (the top-layer hidden state for each time-step), \n",
        "    # hidden (the final hidden state for each layer, $h_T$, stacked on top of each other) \n",
        "    # and cell (the final cell state for each layer, $c_T$, stacked on top of each other).\n",
        "    return output, hidden_state, cell_state'''"
      ],
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-185-3b2c71b54bbc>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    input_size = size of vocab\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ]
    }
  ]
}